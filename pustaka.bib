
@article{pengReproducibleResearchComputational2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  volume = {334},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible},
  language = {en},
  number = {6060},
  journal = {Science},
  author = {Peng, Roger D.},
  month = dec,
  year = {2011},
  pages = {1226-1227},
  file = {/Users/rkkautsar/Zotero/storage/YW3DSQ68/Peng - 2011 - Reproducible Research in Computational Science.pdf}
}

@article{vitekR3RepeatabilityReproducibility2012,
  title = {R3: Repeatability, Reproducibility and Rigor},
  volume = {47},
  issn = {03621340},
  shorttitle = {R3},
  doi = {10.1145/2442776.2442781},
  abstract = {Computer systems research spans sub-disciplines that include embedded systems, programming languages and compilers, networking, and operating systems. Our contention is that a number of structural factors inhibit quality systems research. We highlight some of the factors we have encountered in our own work and observed in published papers and propose solutions that could both increase the productivity of researchers and the quality of their output.},
  language = {en},
  number = {4a},
  journal = {ACM SIGPLAN Notices},
  author = {Vitek, Jan and Kalibera, Tomas},
  month = mar,
  year = {2012},
  pages = {30},
  file = {/Users/rkkautsar/Zotero/storage/UAAWS4DU/Vitek and Kalibera - 2012 - R3 repeatability, reproducibility and rigor.pdf}
}

@article{gentRecomputationManifesto2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1304.3674},
  primaryClass = {cs},
  title = {The {{Recomputation Manifesto}}},
  abstract = {Replication of scientific experiments is critical to the advance of science. Unfortunately, the discipline of Computer Science has never treated replication seriously, even though computers are very good at doing the same thing over and over again. Not only are experiments rarely replicated, they are rarely even replicable in a meaningful way. Scientists are being encouraged to make their source code available, but this is only a small step. Even in the happy event that source code can be built and run successfully, running code is a long way away from being able to replicate the experiment that code was used for. I propose that the discipline of Computer Science must embrace replication of experiments as standard practice. I propose that the only credible technique to make experiments truly replicable is to provide copies of virtual machines in which the experiments are validated to run. I propose that tools and repositories should be made available to make this happen. I propose to be one of those who makes it happen.},
  journal = {arXiv:1304.3674 [cs]},
  author = {Gent, Ian P.},
  month = apr,
  year = {2013},
  keywords = {Computer Science - Digital Libraries,Computer Science - General Literature},
  file = {/Users/rkkautsar/Zotero/storage/FEHXU6QD/Gent - 2013 - The Recomputation Manifesto.pdf;/Users/rkkautsar/Zotero/storage/TFWTNRF3/1304.html}
}

@article{goodmanWhatDoesResearch2016,
  title = {What Does Research Reproducibility Mean?},
  volume = {8},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.aaf5027},
  abstract = {The language and conceptual framework of ``research reproducibility'' are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for ``truth.''},
  language = {en},
  number = {341},
  journal = {Science Translational Medicine},
  author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
  month = jun,
  year = {2016},
  pages = {341ps12-341ps12},
  file = {/Users/rkkautsar/Zotero/storage/NCDHTLBP/Goodman et al. - 2016 - What does research reproducibility mean.pdf}
}

@article{johnsonTheoreticiansGuideExperimental2002,
  series = {{{DIMACS Series}} in {{Discrete Mathematics}} and                         {{Theoretical Computer Science}}},
  title = {A {{Theoreticians Guide}} to the {{Experimental Analysis}} of {{Algorithms}}},
  volume = {59},
  shorttitle = {Data {{Structures}}, {{Near Neighbor Searches}}, and {{Methodology}}},
  abstract = {This paper presents an informal discussion of issues that arise when one attempts to analyze algorithms experimentally. It is based on lessons learned by the author over the course of more than a decade of experimentation, survey paper writing, refereeing, and lively discussions with other experimentalists. Although written from the perspective of a theoretical computer scientist, it is intended to be of use to researchers from all elds who want to study algorithms experimentally. It has two goals: rst, to provide a useful guide to new experimentalists about how such work can best be performed and written up, and second, to challenge current researchers to think about whether their own work might be improved from a scienti c point of view. With the latter purpose in mind, the author hopes that at least a few of his recommendations will be considered controversial.},
  language = {en},
  journal = {Data structures, near neighbor searches, and methodology: fifth and sixth DIMACS implementation challenges},
  editor = {Johnson, David S.},
  month = dec,
  year = {2002},
  pages = {215-250},
  file = {/Users/rkkautsar/Zotero/storage/E24N83ZM/Johnson - 2002 - A Theoreticians Guide to the Experimental Analysis.pdf},
  doi = {10.1090/dimacs/059}
}

@article{boettigerIntroductionDockerReproducible2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1410.0846},
  title = {An Introduction to {{Docker}} for Reproducible Research, with Examples from the {{R}} Environment},
  volume = {49},
  issn = {01635980},
  doi = {10.1145/2723872.2723882},
  abstract = {As computational work becomes more and more integral to many aspects of scientific research, computational reproducibility has become an issue of increasing importance to computer systems researchers and domain scientists alike. Though computational reproducibility seems more straight forward than replicating physical experiments, the complex and rapidly changing nature of computer environments makes being able to reproduce and extend such work a serious challenge. In this paper, I explore common reasons that code developed for one research project cannot be successfully executed or extended by subsequent researchers. I review current approaches to these issues, including virtual machines and workflow systems, and their limitations. I then examine how the popular emerging technology Docker combines several areas from systems research - such as operating system virtualization, cross-platform portability, modular re-usable elements, versioning, and a `DevOps' philosophy, to address these challenges. I illustrate this with several examples of Docker use with a focus on the R statistical environment.},
  number = {1},
  journal = {ACM SIGOPS Operating Systems Review},
  author = {Boettiger, Carl},
  month = jan,
  year = {2015},
  keywords = {Computer Science - Software Engineering},
  pages = {71-79},
  file = {/Users/rkkautsar/Zotero/storage/YGYYCI7Z/Boettiger - 2015 - An introduction to Docker for reproducible researc.pdf;/Users/rkkautsar/Zotero/storage/G2ZIUNE7/1410.html}
}

@article{gundersenStateArtReproducibility2018,
  title = {State of the {{Art}}: {{Reproducibility}} in {{Artificial Intelligence}}},
  abstract = {Background: Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between 20\% and 30\% of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with increased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported.},
  language = {en},
  author = {Gundersen, Odd Erik and Kjensmo, Sigbjorn},
  year = {2018},
  file = {/Users/rkkautsar/Zotero/storage/VI6HWTS4/Gundersen and Kjensmo - State of the Art Reproducibility in Artificial In.pdf}
}

@article{sandveTenSimpleRules2013,
  title = {Ten {{Simple Rules}} for {{Reproducible Computational Research}}},
  volume = {9},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003285},
  language = {en},
  number = {10},
  journal = {PLoS Computational Biology},
  author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
  editor = {Bourne, Philip E.},
  month = oct,
  year = {2013},
  pages = {e1003285},
  file = {/Users/rkkautsar/Zotero/storage/7P9M9L6R/Sandve et al. - 2013 - Ten Simple Rules for Reproducible Computational Re.pdf}
}

@article{buckSolvingReproducibility2015,
  title = {Solving Reproducibility},
  volume = {348},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac8041},
  language = {en},
  number = {6242},
  journal = {Science},
  author = {Buck, S.},
  month = jun,
  year = {2015},
  pages = {1403-1403},
  file = {/Users/rkkautsar/Zotero/storage/HN3NRTHR/Buck - 2015 - Solving reproducibility.pdf}
}

@article{drummondReplicabilityNotReproducibility2009,
  title = {Replicability Is Not {{Reproducibility}}: {{Nor}} Is It {{Good Science}}},
  abstract = {At various machine learning conferences, at various times, there have been discussions arising from the inability to replicate the experimental results published in a paper. There seems to be a wide spread view that we need to do something to address this problem, as it is essential to the advancement of our field. The most compelling argument would seem to be that reproducibility of experimental results is the hallmark of science. Therefore, given that most of us regard machine learning as a scientific discipline, being able to replicate experiments is paramount. I want to challenge this view by separating the notion of reproducibility, a generally desirable property, from replicability, its poor cousin. I claim there are important differences between the two. Reproducibility requires changes; replicability avoids them. Although reproducibility is desirable, I contend that the impoverished version, replicability, is one not worth having.},
  language = {en},
  author = {Drummond, Chris},
  year = {2009},
  file = {/Users/rkkautsar/Zotero/storage/K3SNM7C6/Drummond - Replicability is not Reproducibility Nor is it Go.pdf}
}

@inproceedings{chirigatiReproZipComputationalReproducibility2016,
  address = {San Francisco, California, USA},
  title = {{{ReproZip}}: {{Computational Reproducibility With Ease}}},
  isbn = {978-1-4503-3531-7},
  shorttitle = {{{ReproZip}}},
  doi = {10.1145/2882903.2899401},
  abstract = {We present ReproZip, the recommended packaging tool for the SIGMOD Reproducibility Review. ReproZip was designed to simplify the process of making an existing computational experiment reproducible across platforms, even when the experiment was put together without reproducibility in mind. The tool creates a self-contained package for an experiment by automatically tracking and identifying all its required dependencies. The researcher can share the package with others, who can then use ReproZip to unpack the experiment, reproduce the findings on their favorite operating system, as well as modify the original experiment for reuse in new research, all with little effort. The demo will consist of examples of non-trivial experiments, showing how these can be packed in a Linux machine and reproduced on different machines and operating systems. Demo visitors will also be able to pack and reproduce their own experiments.},
  language = {en},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}} - {{SIGMOD}} '16},
  publisher = {{ACM Press}},
  author = {Chirigati, Fernando and Rampin, R\'emi and Shasha, Dennis and Freire, Juliana},
  year = {2016},
  pages = {2085-2088},
  file = {/Users/rkkautsar/Zotero/storage/R7P5LDT3/Chirigati et al. - 2016 - ReproZip Computational Reproducibility With Ease.pdf}
}

@article{collbergRepeatabilityComputerSystems2016,
  title = {Repeatability in Computer Systems Research},
  volume = {59},
  issn = {00010782},
  doi = {10.1145/2812803},
  language = {en},
  number = {3},
  journal = {Communications of the ACM},
  author = {Collberg, Christian and Proebsting, Todd A.},
  month = feb,
  year = {2016},
  pages = {62-69},
  file = {/Users/rkkautsar/Zotero/storage/QR439S4M/Collberg and Proebsting - 2016 - Repeatability in computer systems research.pdf}
}

@inproceedings{greffSacredInfrastructureComputational2017,
  address = {Austin, Texas},
  title = {The {{Sacred Infrastructure}} for {{Computational Research}}},
  doi = {10.25080/shinma-7f4c6e7-008},
  language = {en},
  booktitle = {Proceedings of the 16th {{Python}} in {{Science Conference}}},
  publisher = {{SciPy}},
  author = {Greff, Klaus and Klein, Aaron and Chovanec, Martin and Hutter, Frank and Schmidhuber, J\"urgen},
  year = {2017},
  pages = {49-56},
  file = {/Users/rkkautsar/Zotero/storage/GJ45YZZT/Greff et al. - 2017 - The Sacred Infrastructure for Computational Resear.pdf}
}

@article{beyerReliableBenchmarkingRequirements2019,
  title = {Reliable Benchmarking: Requirements and Solutions},
  volume = {21},
  issn = {1433-2779, 1433-2787},
  shorttitle = {Reliable Benchmarking},
  doi = {10.1007/s10009-017-0469-y},
  abstract = {Benchmarking is a widely used method in experimental computer science, in particular, for the comparative evaluation of tools and algorithms. As a consequence, a number of questions need to be answered in order to ensure proper benchmarking, resource measurement, and presentation of results, all of which is essential for researchers, tool developers, and users, as well as for tool competitions. We identify a set of requirements that are indispensable for reliable benchmarking and resource measurement of time and memory usage of automatic solvers, verifiers, and similar tools, and discuss limitations of existing methods and benchmarking tools. Fulfilling these requirements in a benchmarking framework can (on Linux systems) currently only be done by using the cgroup and namespace features of the kernel. We developed BenchExec, a ready-to-use, tool-independent, and open-source implementation of a benchmarking framework that fulfills all presented requirements, making reliable benchmarking and resource measurement easy. Our framework is able to work with a wide range of different tools, has proven its reliability and usefulness in the International Competition on Software Verification, and is used by several research groups worldwide to ensure reliable benchmarking. Finally, we present guidelines on how to present measurement results in a scientifically valid and comprehensible way.},
  language = {en},
  number = {1},
  journal = {International Journal on Software Tools for Technology Transfer},
  author = {Beyer, Dirk and L\"owe, Stefan and Wendler, Philipp},
  month = feb,
  year = {2019},
  pages = {1-29},
  file = {/Users/rkkautsar/Zotero/storage/XSNBLHZR/Beyer et al. - 2019 - Reliable benchmarking requirements and solutions.pdf}
}

@inproceedings{juvePracticalResourceMonitoring2015,
  address = {Chicago, IL, USA},
  title = {Practical {{Resource Monitoring}} for {{Robust High Throughput Computing}}},
  isbn = {978-1-4673-6598-7},
  doi = {10.1109/CLUSTER.2015.115},
  abstract = {Robust high throughput computing requires effective monitoring and enforcement of a variety of resources including CPU cores, memory, disk, and network traffic. Without effective monitoring and enforcement, it is easy to overload machines, causing failures and slowdowns, or underload machines, which results in wasted opportunities. This paper explores how to describe, measure, and enforce resources used by computational tasks. We focus on tasks running in distributed execution systems, in which a task requests the resources it needs, and the execution system ensures the availability of such resources. This presents two non-trivial problems: how to measure the resources consumed by a task, and how to monitor and report resource exhaustion in a robust and timely manner. For both of these tasks, operating systems have a variety of mechanisms with different degrees of availability, accuracy, overhead, and intrusiveness. We develop a model to describe various forms of monitoring and map the available mechanisms in contemporary operating systems to that model. Based on this analysis, we present two specific monitoring tools that choose different tradeoffs in overhead and accuracy, and evaluate them on a selection of benchmarks. We conclude by describing our experience in collecting large quantities of monitoring data for complex workflows.},
  language = {en},
  booktitle = {2015 {{IEEE International Conference}} on {{Cluster Computing}}},
  publisher = {{IEEE}},
  author = {Juve, Gideon and Tovar, Benjamin and da Silva, Rafael Ferreira and Krol, Dariusz and Thain, Douglas and Deelman, Ewa and Allcock, William and Livny, Miron},
  month = sep,
  year = {2015},
  pages = {650-657},
  file = {/Users/rkkautsar/Zotero/storage/U4TG2ZWD/Juve et al. - 2015 - Practical Resource Monitoring for Robust High Thro.pdf}
}

@inproceedings{hoeflerScientificBenchmarkingParallel2015,
  address = {Austin, Texas},
  title = {Scientific Benchmarking of Parallel Computing Systems: Twelve Ways to Tell the Masses When Reporting Performance Results},
  isbn = {978-1-4503-3723-6},
  shorttitle = {Scientific Benchmarking of Parallel Computing Systems},
  doi = {10.1145/2807591.2807644},
  abstract = {Measuring and reporting performance of parallel computers constitutes the basis for scientific advancement of high-performance computing (HPC). Most scientific reports show performance improvements of new techniques and are thus obliged to ensure reproducibility or at least interpretability. Our investigation of a stratified sample of 120 papers across three top conferences in the field shows that the state of the practice is lacking. For example, it is often unclear if reported improvements are deterministic or observed by chance. In addition to distilling best practices from existing work, we propose statistically sound analysis and reporting techniques and simple guidelines for experimental design in parallel computing and codify them in a portable benchmarking library. We aim to improve the standards of reporting research results and initiate a discussion in the HPC field. A wide adoption of our minimal set of rules will lead to better interpretability of performance results and improve the scientific culture in HPC.},
  language = {en},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}} on - {{SC}} '15},
  publisher = {{ACM Press}},
  author = {Hoefler, Torsten and Belli, Roberto},
  year = {2015},
  pages = {1-12},
  file = {/Users/rkkautsar/Zotero/storage/QHPNUW3S/hoefler-scientific-benchmarking.pdf}
}

@article{crowderReportingComputationalExperiments1978,
  title = {Reporting Computational Experiments in Mathematical Programming},
  volume = {15},
  issn = {0025-5610, 1436-4646},
  doi = {10.1007/BF01609036},
  language = {en},
  number = {1},
  journal = {Mathematical Programming},
  author = {Crowder, Harlan P. and Dembo, Ron S. and Mulvey, John M.},
  month = dec,
  year = {1978},
  pages = {316-329},
  file = {/Users/rkkautsar/Zotero/storage/M3V9IGD8/Crowder et al. - 1978 - Reporting computational experiments in mathematica.pdf}
}

@book{hockneyScienceComputerBenchmarking1996,
  address = {Philadelphia},
  series = {Software, Environments, Tools},
  title = {The Science of Computer Benchmarking},
  isbn = {978-0-89871-363-3},
  lccn = {QA76.9.E94 H63 1996},
  language = {en},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Hockney, Roger W.},
  year = {1996},
  keywords = {Electronic digital computers,Evaluation},
  file = {/Users/rkkautsar/Zotero/storage/R2HWCJB4/Hockney - 1996 - The science of computer benchmarking.pdf}
}

@article{coffinStatisticalAnalysisComputational2000,
  title = {Statistical {{Analysis}} of {{Computational Tests}} of {{Algorithms}} and {{Heuristics}}},
  volume = {12},
  issn = {1091-9856, 1526-5528},
  doi = {10.1287/ijoc.12.1.24.11899},
  language = {en},
  number = {1},
  journal = {INFORMS Journal on Computing},
  author = {Coffin, Marie and Saltzman, Matthew J.},
  month = feb,
  year = {2000},
  pages = {24-44},
  file = {/Users/rkkautsar/Zotero/storage/GD8JJFLA/Coffin and Saltzman - 2000 - Statistical Analysis of Computational Tests of Alg.pdf}
}

@incollection{stumpStarExecCrossCommunityInfrastructure2014,
  address = {Cham},
  title = {{{StarExec}}: {{A Cross}}-{{Community Infrastructure}} for {{Logic Solving}}},
  volume = {8562},
  isbn = {978-3-319-08586-9 978-3-319-08587-6},
  shorttitle = {{{StarExec}}},
  abstract = {We introduce StarExec, a public web-based service built to facilitate the experimental evaluation of logic solvers, broadly understood as automated tools based on formal reasoning. Examples of such tools include theorem provers, SAT and SMT solvers, constraint solvers, model checkers, and software verifiers. The service, running on a compute cluster with 380 processors and 23 terabytes of disk space, is designed to provide a single piece of storage and computing infrastructure to logic solving communities and their members. It aims at reducing duplication of effort and resources as well as enabling individual researchers or groups with no access to comparable infrastructure. StarExec allows community organizers to store, manage and make available benchmark libraries; competition organizers to run logic solver competitions; and community members to do comparative evaluations of logic solvers on public or private benchmark problems.},
  language = {en},
  booktitle = {Automated {{Reasoning}}},
  publisher = {{Springer International Publishing}},
  author = {Stump, Aaron and Sutcliffe, Geoff and Tinelli, Cesare},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Demri, St\'ephane and Kapur, Deepak and Weidenbach, Christoph},
  year = {2014},
  pages = {367-373},
  file = {/Users/rkkautsar/Zotero/storage/3SPQ4463/Stump et al. - 2014 - StarExec A Cross-Community Infrastructure for Log.pdf},
  doi = {10.1007/978-3-319-08587-6_28}
}

@article{tichyShouldComputerScientists1998,
  title = {Should Computer Scientists Experiment More?},
  volume = {31},
  issn = {0018-9162},
  doi = {10.1109/2.675631},
  abstract = {Computer scientists and practitioners defend their lack of experimentation with a wide range of arguments. Some arguments suggest that experimentation is inappropriate, too difficult, useless, and even harmful. This article discusses several such arguments to illustrate the importance of experimentation for computer science. It considers how the software industry is beginning to value experiments, because results may give a company a three- to five-year lead over the competition.},
  number = {5},
  journal = {Computer},
  author = {Tichy, W. F.},
  month = may,
  year = {1998},
  keywords = {Brain modeling,company,competition,Computer aided manufacturing,computer science,Computer science,computer scientists,DP industry,experimentation,Humans,Immune system,Laser modes,Laser theory,Nervous system,Probability,software industry,Testing},
  pages = {32-40},
  file = {/Users/rkkautsar/Zotero/storage/UAET4SSP/Tichy - 1998 - Should computer scientists experiment more.pdf;/Users/rkkautsar/Zotero/storage/6JQ6DAZZ/675631.html}
}

@article{piccoloToolsTechniquesComputational2016,
  title = {Tools and Techniques for Computational Reproducibility},
  volume = {5},
  issn = {2047-217X},
  doi = {10.1186/s13742-016-0135-4},
  abstract = {When reporting research findings, scientists document the steps they followed so that others can verify and build upon the research. When those steps have been described in sufficient detail that others can retrace the steps and obtain similar results, the research is said to be reproducible. Computers play a vital role in many research disciplines and present both opportunities and challenges for reproducibility. Computers can be programmed to execute analysis tasks, and those programs can be repeated and shared with others. The deterministic nature of most computer programs means that the same analysis tasks, applied to the same data, will often produce the same outputs. However, in practice, computational findings often cannot be reproduced because of complexities in how software is packaged, installed, and executed\textemdash{}and because of limitations associated with how scientists document analysis steps. Many tools and techniques are available to help overcome these challenges; here we describe seven such strategies. With a broad scientific audience in mind, we describe the strengths and limitations of each approach, as well as the circumstances under which each might be applied. No single strategy is sufficient for every scenario; thus we emphasize that it is often useful to combine approaches.},
  language = {en},
  number = {1},
  journal = {GigaScience},
  author = {Piccolo, Stephen R. and Frampton, Michael B.},
  month = dec,
  year = {2016},
  file = {/Users/rkkautsar/Zotero/storage/C5JXXN3W/Piccolo and Frampton - 2016 - Tools and techniques for computational reproducibi.pdf}
}

@incollection{charwatVCWCVersioningCompetition2013,
  address = {Berlin, Heidelberg},
  title = {{{VCWC}}: {{A Versioning Competition Workflow Compiler}}},
  volume = {8148},
  isbn = {978-3-642-40563-1 978-3-642-40564-8},
  shorttitle = {{{VCWC}}},
  language = {en},
  booktitle = {Logic {{Programming}} and {{Nonmonotonic Reasoning}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Charwat, G\"unther and Ianni, Giovambattista and Krennwallner, Thomas and Kronegger, Martin and Pfandler, Andreas and Redl, Christoph and Schwengerer, Martin and Spendier, Lara Katharina and Wallner, Johannes Peter and Xiao, Guohui},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Cabalar, Pedro and Son, Tran Cao},
  year = {2013},
  pages = {233-238},
  file = {/Users/rkkautsar/Zotero/storage/XW34YN6A/Charwat et al. - 2013 - VCWC A Versioning Competition Workflow Compiler.pdf},
  doi = {10.1007/978-3-642-40564-8_23}
}

@article{lukowiczExperimentalEvaluationComputer1994,
  title = {Experimental {{Evaluation}} in {{Computer Science}}: {{A Quantitative Study}}},
  volume = {28},
  shorttitle = {Experimental {{Evaluation}} in {{Computer Science}}},
  abstract = {A survey of over 400 recent research articles suggests that computer scientists publish relatively few papers with experimentally validated results. The survey includes complete volumes of several refereed computer science journals, a conference, and 50 titles drawn at random from all articles published by ACM in 1993. The journals Optical Engineering (OE) and Neural Computation (NC) were used for comparison. Of the papers in the random sample that would require experimental validation, 40\% have none at all. In journals related to software engineering, this fraction is over 50\%. In comparison, the fraction of papers lacking quantitative evaluation in OE and NC is only 15\% and 12\%, respectively. Conversely, the fraction of papers that devote one fifth or more of their space to experimental validation is almost 70\% for OE and NC, while it is a mere 30\% for the CS random sample and 20\% for software engineering. The low ratio of validated results appears to be a serious weakness in compute...},
  journal = {Journal of Systems and Software},
  author = {Lukowicz, Paul and Heinz, Ernst A. and Prechelt, Lutz and Tichy, Walter F.},
  year = {1994},
  pages = {9--18},
  file = {/Users/rkkautsar/Zotero/storage/IEBH4459/Lukowicz et al. - 1994 - Experimental Evaluation in Computer Science A Qua.pdf;/Users/rkkautsar/Zotero/storage/N7VQ2GCX/summary.html}
}

@article{dodig-crnkovicScientificMethodsComputer,
  title = {Scientific {{Methods}} in {{Computer Science}}},
  abstract = {This paper analyzes scientific aspects of Computer Science. First it defines science and scientific method in general. It gives a discussion of relations between science, research, development and technology.},
  language = {en},
  author = {{Dodig-Crnkovic}, Gordana},
  pages = {7},
  file = {/Users/rkkautsar/Zotero/storage/XL2JDCJP/Dodig-Crnkovic - Scientific Methods in Computer Science.pdf}
}

@inproceedings{simUsingBenchmarkingAdvance2003,
  title = {Using Benchmarking to Advance Research: A Challenge to Software Engineering},
  shorttitle = {Using Benchmarking to Advance Research},
  doi = {10.1109/ICSE.2003.1201189},
  abstract = {Benchmarks have been used in computer science to compare the performance of computer systems, information retrieval algorithms, databases, and many other technologies. The creation and widespread use of a benchmark within a research area is frequently accompanied by rapid technical progress and community building. These observations have led us to formulate a theory of benchmarking within scientific disciplines. Based on this theory, we challenge software engineering research to become more scientific and cohesive by working as a community to define benchmarks. In support of this challenge, we present a case study of the reverse engineering community, where we have successfully used benchmarks to advance the state of research.},
  booktitle = {25th {{International Conference}} on {{Software Engineering}}, 2003. {{Proceedings}}.},
  author = {Sim, S. E. and Easterbrook, S. and Holt, R. C.},
  month = may,
  year = {2003},
  keywords = {Computer science,benchmark testing,benchmarking,Books,Collaboration,community building,Computer languages,computer system performance,Databases,Guidelines,Information retrieval,information retrieval algorithms,reverse engineering,Reverse engineering,reverse engineering community,Software algorithms,software engineering,Software engineering,software engineering research,technical progress},
  pages = {74-83},
  file = {/Users/rkkautsar/Zotero/storage/U7P57WT7/Sim et al. - 2003 - Using benchmarking to advance research a challeng.pdf;/Users/rkkautsar/Zotero/storage/XPSFZ5YU/1201189.html}
}

@article{kitchenhamPreliminaryGuidelinesEmpirical2002,
  title = {Preliminary Guidelines for Empirical Research in Software Engineering},
  volume = {28},
  issn = {0098-5589},
  doi = {10.1109/TSE.2002.1027796},
  abstract = {Empirical software engineering research needs research guidelines to improve the research and reporting processes. We propose a preliminary set of research guidelines aimed at stimulating discussion among software researchers. They are based on a review of research guidelines developed for medical researchers and on our own experience in doing and reviewing software engineering research. The guidelines are intended to assist researchers, reviewers, and meta-analysts in designing, conducting, and evaluating empirical studies. Editorial boards of software engineering journals may wish to use our recommendations as a basis for developing guidelines for reviewers and for framing policies for dealing with the design, data collection, and analysis and reporting of empirical studies.},
  number = {8},
  journal = {IEEE Transactions on Software Engineering},
  author = {Kitchenham, B. A. and Pfleeger, S. L. and Pickard, L. M. and Jones, P. W. and Hoaglin, D. C. and Emam, K. El and Rosenberg, J.},
  month = aug,
  year = {2002},
  keywords = {Computer science,Guidelines,software engineering,Software engineering,Computer Society,Data analysis,Laboratories,Software performance,software researchers,Software standards,Statistics,Surgery},
  pages = {721-734},
  file = {/Users/rkkautsar/Zotero/storage/M3CM9X8T/Kitchenham et al. - 2002 - Preliminary guidelines for empirical research in s.pdf;/Users/rkkautsar/Zotero/storage/RNJ3V46P/1027796.html}
}

@book{makSolvingEverydayProblems2009,
  title = {Solving {{Everyday Problems}} with the {{Scientific Method}} - {{Thinking Like}} a {{Scientist}}},
  isbn = {978-981-283-510-9},
  language = {en},
  publisher = {{World Scientific Publishing Co. Pte. Ltd.}},
  author = {Mak, Don K. and Mak, Angela T. and Mak, Anthony B.},
  year = {2009},
  file = {/Users/rkkautsar/Zotero/storage/MF9PTYRT/Mak et al. - 2009 - Solving Everyday Problems with the Scientific Meth.pdf},
  doi = {10.1142/9789812835109}
}

@book{kossoSummaryScientificMethod2011,
  title = {A {{Summary}} of {{Scientific Method}}},
  language = {en},
  publisher = {{Springer Netherlands}},
  author = {Kosso, Peter},
  year = {2011},
  file = {/Users/rkkautsar/Zotero/storage/DEZ5G7IT/Kosso - 2011 - A Summary of Scientific Method.pdf},
  note = {OCLC: 943713117}
}

@book{careyBeginnerGuideScientific2012,
  address = {Boston, MA},
  edition = {4th ed},
  title = {A Beginner's Guide to Scientific Method},
  isbn = {978-1-111-30555-0 978-1-111-72601-0},
  lccn = {Q175 .C27 2012},
  language = {en},
  publisher = {{Wadsworth Cengage Learning}},
  author = {Carey, Stephen S.},
  year = {2012},
  keywords = {Forschungsmethode,Méthodologie,Methodology,Recherche,Research,Science,Sciences,Wissenschaftliches Arbeiten},
  file = {/Users/rkkautsar/Zotero/storage/7ZIQJEUL/Carey - 2012 - A beginner's guide to scientific method.pdf;/Users/rkkautsar/Zotero/storage/TL96R8S6/Carey - 2012 - A beginner's guide to scientific method.pdf},
  note = {OCLC: ocn698377925}
}

@book{gowerScientificMethodHistorical2012,
  edition = {1},
  title = {Scientific {{Method}}: {{A Historical}} and {{Philosophical Introduction}}},
  isbn = {978-0-203-04612-8},
  shorttitle = {Scientific {{Method}}},
  language = {en},
  publisher = {{Routledge}},
  author = {Gower, Barry},
  month = oct,
  year = {2012},
  file = {/Users/rkkautsar/Zotero/storage/YUXYZGVR/Gower - 2012 - Scientific Method A Historical and Philosophical .pdf},
  doi = {10.4324/9780203046128}
}

@book{wallimanResearchMethodsBasics2010a,
  title = {Research {{Methods}}: {{The Basics}}},
  isbn = {978-0-203-83607-1},
  shorttitle = {Research {{Methods}}},
  language = {en},
  publisher = {{Routledge}},
  author = {Walliman, Nicholas},
  month = nov,
  year = {2010},
  file = {/Users/rkkautsar/Zotero/storage/VJIIFXZW/Walliman - 2010 - Research Methods The Basics.pdf},
  doi = {10.4324/9780203836071}
}

@article{doelemanImagingEventHorizon2009,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0906.3899},
  primaryClass = {astro-ph},
  title = {Imaging an {{Event Horizon}}: Submm-{{VLBI}} of a {{Super Massive Black Hole}}},
  shorttitle = {Imaging an {{Event Horizon}}},
  abstract = {A long standing goal in astrophysics is to directly observe the immediate environment of a black hole with angular resolution comparable to the event horizon. Realizing this goal would open a new window on the study of General Relativity in the strong field regime, accretion and outflow processes at the edge of a black hole, the existence of an event horizon, and fundamental black hole physics (e.g., spin). Steady long-term progress on improving the capability of Very Long Baseline Interferometry (VLBI) at short wavelengths has now made it extremely likely that this goal will be achieved within the next decade. The most compelling evidence for this is the recent observation by 1.3mm VLBI of Schwarzschild radius scale structure in SgrA*, the compact source of radio, submm, NIR and xrays at the center of the Milky Way. SgrA* is thought to mark the position of a \textasciitilde{}4 million solar mass black hole, and because of its proximity and estimated mass presents the largest apparent event horizon size of any black hole candidate in the Universe. Over the next decade, existing and planned mm/submm facilities will be combined into a high sensitivity, high angular resolution "Event Horizon Telescope" that will bring us as close to the edge of black hole as we will come for decades. This white paper describes the science case for mm/submm VLBI observations of both SgrA* and M87 (a radio loud AGN of a much more luminous class that SgrA*). We emphasize that while there is development and procurement involved, the technical path forward is clear, and the recent successful observations have removed much of the risk that would normally be associated with such an ambitious project.},
  journal = {arXiv:0906.3899 [astro-ph]},
  author = {Doeleman, Sheperd and Agol, Eric and Backer, Don and Baganoff, Fred and Bower, Geoffrey C. and Broderick, Avery and Fabian, Andrew and Fish, Vincent and Gammie, Charles and Ho, Paul and Honma, Mareki and Krichbaum, Thomas and Loeb, Avi and Marrone, Dan and Reid, Mark and Rogers, Alan E. E. and Shapiro, Irwin and Strittmatter, Peter and Tilanus, Remo and Weintroub, Jonathan and Whitney, Alan and Wright, Melvyn and Ziurys, Lucy},
  month = jun,
  year = {2009},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/Users/rkkautsar/Zotero/storage/JD5YMEQC/Doeleman et al. - 2009 - Imaging an Event Horizon submm-VLBI of a Super Ma.pdf;/Users/rkkautsar/Zotero/storage/9LYF3Z46/0906.html}
}

@book{montgomeryDesignAnalysisExperiments2013,
  address = {Hoboken, NJ},
  edition = {Eighth},
  title = {Design and Analysis of Experiments},
  isbn = {978-1-118-14692-7},
  lccn = {QA279 .M66 2013},
  abstract = {"The eighth edition of Design and Analysis of Experiments continues to provide extensive and in-depth information on engineering, business, and statistics-as well as informative ways to help readers design and analyze experiments for improving the quality, efficiency and performance of working systems. Furthermore, the text maintains its comprehensive coverage by including: new examples, exercises, and problems (including in the areas of biochemistry and biotechnology); new topics and problems in the area of response surface; new topics in nested and split-plot design; and the residual maximum likelihood method is now emphasized throughout the book"--},
  language = {en},
  publisher = {{John Wiley \& Sons, Inc}},
  author = {Montgomery, Douglas C.},
  year = {2013},
  keywords = {Experimental design,TECHNOLOGY \& ENGINEERING / Industrial Engineering},
  file = {/Users/rkkautsar/Zotero/storage/CXSFPHA2/Montgomery - 2013 - Design and analysis of experiments.pdf}
}

@book{ryanModernExperimentalDesign2007a,
  title = {Modern {{Experimental Design}}},
  publisher = {{Wiley}},
  author = {Ryan, Thomas P.},
  year = {2007},
  file = {/Users/rkkautsar/Zotero/storage/RZ94SEXK/Modern Experimental Design.pdf}
}

@misc{philipp_wendler_2019_2561835,
  author       = {Philipp Wendler and
                  Dirk Beyer and
                  Thomas Lemberger and
                  Karlheinz Friedberger and
                  Daniel Dietsch and
                  Marek Chalupa and
                  Williame Rocha and
                  George Karpenkov and
                  Peter Schrammel and
                  Michael Tautschnig and
                  Herbert and
                  Thomas Bunk and
                  altmattr and
                  Zvonimir and
                  IljaZakharov and
                  Vladimír Štill and
                  Truc Nguyen Lam and
                  MartinNowack and
                  Animesh Basak Chowdhury and
                  yinliangze and
                  Guang Chen and
                  avritichauhan and
                  PRITOM RAJKHOWA and
                  Thomas Stieglmaier and
                  Jan Mrázek and
                  Stephan Lukasczyk and
                  Florian Heck and
                  Sebastian Ott and
                  Mikhail Ramalho and
                  LE, Ton Chanh},
  title        = {sosy-lab/benchexec: Release 1.18},
  month        = feb,
  year         = 2019,
  doi          = {10.5281/zenodo.2561835}
}

@misc{benchkit:2013,
  Author = {F. Kordon and
  N. Gibelin and
  F. Hulin-Hubard and
  F. Pommereau},
  url = {{http://benchkit.cosyverif.org}},
  Lastchecked = {2013},
  Title = {{BenchKit User Manual}},
  Urldate = {2013},
  year = {2013}
}

@inproceedings{kordonBenchKitToolMassive2014,
  title = {{{BenchKit}}, a {{Tool}} for {{Massive Concurrent Benchmarking}}},
  doi = {10.1109/ACSD.2014.12},
  abstract = {Benchmarking numerous programs in a reasonable time requires the use of several (potentially multicore) computers. We experimented such a situation in the context of the MCC (Model Checking Contest @ Petri net) where we had to operate more than 52000 runs for the 2013 edition. This paper presents BenchKit, a tool to operate programs on sets of potentially parallel machines and to gather monitoring information like CPU or memory usage. It also samples such data over the execution time. BenchKit has been elaborated in the context of the MCC and will be used for the 2014 edition.},
  booktitle = {2014 14th {{International Conference}} on {{Application}} of {{Concurrency}} to {{System Design}}},
  author = {Kordon, F. and {Hulin-Hubard}, F.},
  month = jun,
  year = {2014},
  keywords = {BenchKit,benchmark testing,Benchmark testing,Computers,concurrency (computers),Context,Distributed Computing,Evaluation of programs,formal verification,Kernel,Magnetic heads,massive concurrent benchmarking,MCC,model checking contest,Monitoring,multicore computers,multiprocessing programs,Petri net,Petri nets,Virtual machines,Virtualization},
  pages = {159-165},
  file = {/Users/rkkautsar/Zotero/storage/2KHSLSA8/Kordon and Hulin-Hubard - 2014 - BenchKit, a Tool for Massive Concurrent Benchmarki.pdf;/Users/rkkautsar/Zotero/storage/HH5FKZA9/7016339.html}
}


@article{frings2010flexible,
  title = {A Flexible, Application-and Platform-Independent Environment for Benchmarking},
  volume = {19},
  journal = {Parallel Computing: From Multicores and GPU's to Petascale},
  author = {Frings, Wolfgang and Schnurpfeil, Alexander and Meier, Stefanie and Janetzko, Florian and Arnold, Lukas},
  year = {2010},
  pages = {423},
  file = {/Users/rkkautsar/Zotero/storage/LK3VZ8C9/Frings et al. - 2010 - A flexible, application-and platform-independent e.pdf},
  publisher = {{IOS Press}}
}


@inproceedings{kimPracticalEffectiveSandboxing2013,
  title = {Practical and Effective Sandboxing for Non-Root Users},
  abstract = {MBOX is a lightweight sandboxing mechanism for nonroot users in commodity OSes. MBOX's sandbox usage model executes a program in the sandbox and prevents the program from modifying the host filesystem by layering the sandbox filesystem on top of the host filesystem. At the end of program execution, the user can examine changes in the sandbox filesystem and selectively commit them back to the host filesystem. MBOX implements this by interposing on system calls and provides a variety of useful applications: installing system packages as a non-root user, running unknown binaries safely without network accesses, checkpointing the host filesystem instantly, and setting up a virtual development environment without special tools. Our performance evaluation shows that MBOX imposes CPU overheads of 0.1\textendash{}45.2\% for various workloads. In this paper, we present MBOX's design, efficient techniques for interposing on system calls, our experience avoiding common system call interposition pitfalls, and MBOX's performance evaluation.},
  language = {en},
  booktitle = {Proceedings of {{USENIX Annual Technical Conference}}},
  author = {Kim, Taesoo and Zeldovich, Nickolai},
  year = {2013},
  pages = {139-144},
  file = {/Users/rkkautsar/Zotero/storage/E9QQU2B5/Kim and Zeldovich - Practical and effective sandboxing for non-root us.pdf}
}


@inproceedings{scheepersVirtualizationContainerizationApplication2014,
  title = {Virtualization and {{Containerization}} of {{Application Infrastructure}}: {{A Comparison}}},
  abstract = {Modern cloud infrastructure uses virtualization to isolate applications, optimize the utilization of hardware resources and provide operational flexibility. However, conventional virtualization comes at the cost of resource overhead.},
  language = {en},
  booktitle = {Proceedings of the 21st {{Twente Student Conference}} on {{IT}}},
  author = {Scheepers, Mathijs Jeroen},
  month = jun,
  year = {2014},
  pages = {7},
  file = {/Users/rkkautsar/Zotero/storage/2YUZF55I/Scheepers - Virtualization and Containerization of Application.pdf}
}


@article{zhangComparativeStudyContainers2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.01842},
  primaryClass = {cs},
  title = {A {{Comparative Study}} of {{Containers}} and {{Virtual Machines}} in {{Big Data Environment}}},
  abstract = {Container technique is gaining increasing attention in recent years and has become an alternative to traditional virtual machines. Some of the primary motivations for the enterprise to adopt the container technology include its convenience to encapsulate and deploy applications, lightweight operations, as well as efficiency and flexibility in resources sharing. However, there still lacks an in-depth and systematic comparison study on how big data applications, such as Spark jobs, perform between a container environment and a virtual machine environment. In this paper, by running various Spark applications with different configurations, we evaluate the two environments from many interesting aspects, such as how convenient the execution environment can be set up, what are makespans of different workloads running in each setup, how efficient the hardware resources, such as CPU and memory, are utilized, and how well each environment can scale. The results show that compared with virtual machines, containers provide a more easy-to-deploy and scalable environment for big data workloads. The research work in this paper can help practitioners and researchers to make more informed decisions on tuning their cloud environment and configuring the big data applications, so as to achieve better performance and higher resources utilization.},
  journal = {arXiv:1807.01842 [cs]},
  author = {Zhang, Qi and Liu, Ling and Pu, Calton and Dou, Qiwei and Wu, Liren and Zhou, Wei},
  month = jul,
  year = {2018},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Performance},
  file = {/Users/rkkautsar/Zotero/storage/855FGS7W/Zhang et al. - 2018 - A Comparative Study of Containers and Virtual Mach.pdf;/Users/rkkautsar/Zotero/storage/GV4U4WE3/1807.html}
}

@article{rousselControllingSolverExecution2011,
  title = {Controlling a {{Solver Execution}} with the Runsolver {{Tool}}},
  volume = {7},
  abstract = {The runsolver tool was designed for the 2005 edition of the pseudo-Boolean competition in order to solve the problem of correctly measuring the resources used by solvers, especially solvers with multiple processes. Since then, it has been improved in several directions and adopted by several other competitions or frameworks. This paper presents the inner working of this tool and the technical problems that it addresses.},
  language = {en},
  journal = {Journal on Satisfiability, Boolean Modeling and Computation},
  author = {Roussel, Olivier},
  year = {2011},
  pages = {139-144},
  file = {/Users/rkkautsar/Zotero/storage/2TKWZZZK/Roussel - Controlling a Solver Execution with the runsolver .pdf}
}

@article{marevs2012new,
  title = {A {{New Contest Sandbox}}.},
  volume = {6},
  journal = {Olympiads in Informatics},
  author = {Mare{\v s}, Martin and Blackham, Bernard},
  year = {2012},
  file = {/Users/rkkautsar/Zotero/storage/96PB6PXC/isolate.pdf}
}



@book{hintjens2013zeromq,
  title = {{{ZeroMQ}}: Messaging for Many Applications},
  publisher = {{O'Reilly Media, Inc.}},
  author = {Hintjens, Pieter},
  year = {2013},
  file = {/Users/rkkautsar/Zotero/storage/EHLPD72H/Pieter Hintjens - ZeroMQ_ Messaging for Many Applications (2013, O'Reilly Media).pdf}
}



@article{ben2005yaml,
  title={{{YAML}} ain't markup language ({{YAML™}}) version 1.1},
  author={Ben-Kiki, Oren and Evans, Clark and Ingerson, Brian},
  journal={yaml. org, Tech. Rep},
  pages={23},
  year={2005}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  volume = {521},
  issn = {0028-0836, 1476-4687},
  language = {en},
  number = {7553},
  journal = {Nature},
  doi = {10.1038/nature14539},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  month = may,
  year = {2015},
  pages = {436-444},
  file = {/Users/rkkautsar/Zotero/storage/YV7YBXS4/LeCun et al. - 2015 - Deep learning.pdf}
}


@article{miikkulainenEvolvingDeepNeural2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.00548},
  primaryClass = {cs},
  title = {Evolving {{Deep Neural Networks}}},
  abstract = {The success of deep learning depends on finding an architecture to fit the task. As deep learning has scaled up to more challenging tasks, the architectures have become difficult to design by hand. This paper proposes an automated method, CoDeepNEAT, for optimizing deep learning architectures through evolution. By extending existing neuroevolution methods to topology, components, and hyperparameters, this method achieves results comparable to best human designs in standard benchmarks in object recognition and language modeling. It also supports building a real-world application of automated image captioning on a magazine website. Given the anticipated increases in available computing power, evolution of deep networks is promising approach to constructing deep learning applications in the future.},
  journal = {arXiv:1703.00548 [cs]},
  author = {Miikkulainen, Risto and Liang, Jason and Meyerson, Elliot and Rawal, Aditya and Fink, Dan and Francon, Olivier and Raju, Bala and Shahrzad, Hormoz and Navruzyan, Arshak and Duffy, Nigel and Hodjat, Babak},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/rkkautsar/Zotero/storage/J2QMWAL3/Miikkulainen et al. - 2017 - Evolving Deep Neural Networks.pdf;/Users/rkkautsar/Zotero/storage/5SIEX6D7/1703.html}
}

@inproceedings{cook1971complexity,
  title={The complexity of theorem-proving procedures},
  author={Cook, Stephen A},
  booktitle={Proceedings of the third annual ACM symposium on Theory of computing},
  pages={151--158},
  year={1971},
  organization={ACM}
}

@article{levin1973universal,
  title={Universal sequential search problems},
  author={Levin, Leonid Anatolevich},
  journal={Problemy Peredachi Informatsii},
  volume={9},
  number={3},
  pages={115--116},
  year={1973},
  publisher={Russian Academy of Sciences, Branch of Informatics, Computer Equipment and~…}
}

@article{nordstromPebbleGamesProof2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1307.3913},
  title = {Pebble {{Games}}, {{Proof Complexity}}, and {{Time}}-{{Space Trade}}-Offs},
  volume = {9},
  issn = {18605974},
  abstract = {Pebble games were extensively studied in the 1970s and 1980s in a number of different contexts. The last decade has seen a revival of interest in pebble games coming from the field of proof complexity. Pebbling has proven to be a useful tool for studying resolution-based proof systems when comparing the strength of different subsystems, showing bounds on proof space, and establishing size-space trade-offs. This is a survey of research in proof complexity drawing on results and tools from pebbling, with a focus on proof space lower bounds and trade-offs between proof size and proof space.},
  number = {3},
  journal = {Logical Methods in Computer Science},
  doi = {10.2168/LMCS-9(3:15)2013},
  author = {Nordstr\"o{}m, Jakob},
  month = sep,
  year = {2013},
  keywords = {Computer Science - Computational Complexity,Computer Science - Discrete Mathematics,Computer Science - Logic in Computer Science,Mathematics - Combinatorics,Mathematics - Logic},
  pages = {15},
  file = {/Users/rkkautsar/Zotero/storage/KRPIPQS5/Nordstrom - 2013 - Pebble Games, Proof Complexity, and Time-Space Tra.pdf;/Users/rkkautsar/Zotero/storage/8Z9Y87B7/1307.html}
}

@article{heule2018proceedings,
  title={{Proceedings of SAT Competition 2018}},
  author={Heule, Marijn JH and J{\"a}rvisalo, Matti Juhani and Suda, Martin and others},
  year={2018},
  publisher={Department of Computer Science, University of Helsinki}
}


@incollection{elffersTradeoffsTimeMemory2016,
  address = {Cham},
  title = {Trade-Offs {{Between Time}} and {{Memory}} in a {{Tighter Model}} of {{CDCL SAT Solvers}}},
  volume = {9710},
  isbn = {978-3-319-40969-6 978-3-319-40970-2},
  abstract = {A long line of research has studied the power of conflictdriven clause learning (CDCL) and how it compares to the resolution proof system in which it searches for proofs. It has been shown that CDCL can polynomially simulate resolution even with an adversarially chosen learning scheme as long as it is asserting. However, the simulation only works under the assumption that no learned clauses are ever forgotten, and the polynomial blow-up is significant. Moreover, the simulation requires very frequent restarts, whereas the power of CDCL with less frequent or entirely without restarts remains poorly understood. With a view towards obtaining results with tighter relations between CDCL and resolution, we introduce a more fine-grained model of CDCL that captures not only time but also memory usage and number of restarts. We show how previously established strong size-space trade-offs for resolution can be transformed into equally strong trade-offs between time and memory usage for CDCL, where the upper bounds hold for CDCL without any restarts using the standard 1UIP clause learning scheme, and the (in some cases tightly matching) lower bounds hold for arbitrarily frequent restarts and arbitrary clause learning schemes.},
  language = {en},
  booktitle = {Theory and {{Applications}} of {{Satisfiability Testing}} \textendash{} {{SAT}} 2016},
  publisher = {{Springer International Publishing}},
  author = {Elffers, Jan and Johannsen, Jan and Lauria, Massimo and Magnard, Thomas and Nordstr{\"o}m, Jakob and Vinyals, Marc},
  editor = {Creignou, Nadia and Le Berre, Daniel},
  year = {2016},
  pages = {160-176},
  file = {/Users/rkkautsar/Zotero/storage/GDKDVZTH/Elffers et al. - 2016 - Trade-offs Between Time and Memory in a Tighter Mo.pdf},
  doi = {10.1007/978-3-319-40970-2_11}
}

@article{akiyama2019first,
  title={{First M87 Event Horizon Telescope results. IV. Imaging the central supermassive black hole}},
  author={Akiyama, Kazunori and Alberdi, Antxon and Alef, Walter and Asada, Keiichi and Azulay, Rebecca and Baczko, Anne-Kathrin and Ball, David and Balokovi{\'c}, Mislav and Barrett, John and Bintley, Dan and others},
  journal={The Astrophysical Journal Letters},
  volume={875},
  number={1},
  pages={L4},
  year={2019},
  publisher={IOP Publishing}
}

@inproceedings{nielsenTenUsabilityHeuristics2006,
  title = {Ten {{Usability Heuristics}}},
  abstract = {Visibility of system status The system should always keep users informed about what is going on, through appropriate feedback within reasonable time. Match between system and the real world The system should speak the users' language, with words, phrases and concepts familiar to the user, rather than system-oriented terms. Follow realworld conventions, making information appear in a natural and logical order. User control and freedom Users often choose system functions by mistake and will need a clearly marked "emergency exit" to leave the unwanted state without having to go through an extended dialogue. Support undo and redo. Consistency and standards Users should not have to wonder whether different words, situations, or actions mean the same thing. Follow platform conventions. Error prevention Even better than good error messages is a careful design which prevents a problem from occurring in the first place. Either eliminate error-prone conditions or check for them and present users with a confirmation option before they commit to the action. Recognition rather than recall Minimize the user's memory load by making objects, actions, and options visible. The user should not have to remember information from one part of the dialogue to another. Instructions for use of the system should be visible or easily retrievable whenever appropriate. Flexibility and efficiency of use Accelerators -unseen by the novice user -may often speed up the interaction for the expert user such that the system can cater to both inexperienced and experienced users. Allow users to tailor frequent actions. Aesthetic and minimalist design Dialogues should not contain information which is irrelevant or rarely needed. Every extra unit of information in a dialogue competes with the relevant units of information and diminishes their relative visibility. Help users recognize, diagnose, and recover from errors Error messages should be expressed in plain language (no codes), precisely useit.com Papers and Essays Heuristic Evaluation List of Heuristics Search Page 1 of 2 Heuristics for User Interface Design},
  author = {Nielsen, Jakob},
  year = {2006},
  keywords = {Code,Cognitive dimensions of notations,Conferences,confirmation - ResponseLevel,Error message,Essays,Experience,Feedback,Heuristic evaluation,Heuristics,Paper,Phrases,Relevance,standards characteristics,Undo,Units of information,Usability,User interface design},
  file = {/Users/rkkautsar/Zotero/storage/DLMKUVCI/Nielsen - 2006 - Ten Usability Heuristics.pdf}
}


@incollection{beyerBenchmarkingResourceMeasurement2015,
  address = {Cham},
  title = {Benchmarking and {{Resource Measurement}}},
  volume = {9232},
  isbn = {978-3-319-23403-8 978-3-319-23404-5},
  abstract = {Proper benchmarking and resource measurement is an important topic, because benchmarking is a widely-used method for the comparative evaluation of tools for automatic verification, needed by researchers, tool developers, and users, as well as in competitions. We formulate a set of requirements that are indispensable for reliable benchmarking and accurate resource measurements, and discuss the limitations of commonly-used methods and benchmarking tools. Fulfilling these requirements in a benchmarking framework is complex and can (on Linux) currently only be done by using the cgroups feature of the kernel. We provide BenchExec, a ready-to-use, tool-independent, and free implementation of a benchmarking framework that fulfills all presented requirements, making accurate, reliable, and reproducible benchmarking easy. Our framework has proven useful and able to work with a wide range of different tools by its successful use in the International Competition on Software Verification.},
  language = {en},
  booktitle = {Model {{Checking Software}}},
  publisher = {{Springer International Publishing}},
  author = {Beyer, Dirk and L{\"o}we, Stefan and Wendler, Philipp},
  editor = {Fischer, Bernd and Geldenhuys, Jaco},
  year = {2015},
  pages = {160-178},
  file = {/Users/rkkautsar/Zotero/storage/NQRW8UXE/Beyer et al. - 2015 - Benchmarking and Resource Measurement.pdf},
  doi = {10.1007/978-3-319-23404-5_12}
}

@manual{manpages,
  author = {{Free Software Foundation}},
  year = {2019},
  title = {{Linux User's Manual}},
  Howpublished = {Copy available at \url{https://www.kernel.org/doc/man-pages/}},
  Note = {{Accessed on 28 May 2019}}
}

@article{aceto2013cloud,
  title = {Cloud Monitoring: {{A}} Survey},
  volume = {57},
  number = {9},
  journal = {Computer Networks},
  author = {Aceto, Giuseppe and Botta, Alessio and De Donato, Walter and Pescap{\`e}, Antonio},
  year = {2013},
  pages = {2093-2115},
  file = {/Users/rkkautsar/Zotero/storage/FQVEJ42X/Aceto et al. - 2013 - Cloud monitoring A survey.pdf},
  publisher = {{Elsevier}}
}

@article{tovarJobSizingStrategy2018,
  title = {A {{Job Sizing Strategy}} for {{High}}-{{Throughput Scientific Workflows}}},
  volume = {29},
  issn = {1045-9219},
  abstract = {The user of a computing facility must make a critical decision when submitting jobs for execution: how many resources (such as cores, memory, and disk) should be requested for each job? If the request is too small, the job may fail due to resource exhaustion; if the request is too large, the job may succeed, but resources will be wasted. This decision is especially important when running hundreds of thousands of jobs in a high throughput workflow, which may exhibit complex, long tailed distributions of resource consumption. In this paper, we present a strategy for solving the job sizing problem: (1) applications are monitored and measured in user-space as they run; (2) the resource usage is collected into an online archive; and (3) jobs are automatically sized according to historical data in order to maximize throughput or minimize waste. We evaluate the solution analytically, and present case studies of applying the technique to high throughput physics and bioinformatics workflows consisting of hundreds of thousands of jobs, demonstrating an increase in throughput of 10-400 percent compared to naive approaches.},
  number = {2},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  doi = {10.1109/TPDS.2017.2762310},
  author = {Tovar, B. and da Silva, R. F. and Juve, G. and Deelman, E. and Allcock, W. and Thain, D. and Livny, M.},
  month = feb,
  year = {2018},
  keywords = {automatic job sizing,automatic provision of resources,bioinformatics,bioinformatics workflows,Feedback loop,High throughput computing (HTC),high throughput workflow,high-throughput scientific workflows,Internet,job sizing problem,job sizing strategy,Monitoring,online archive,Physics,Random access memory,records management,resource consumption,resource consumption distributions,Resource management,resource monitoring and enforcement,resource usage,Throughput,throughput and waste optimization,throughput physics,Tools,workflow management software},
  pages = {240-253},
  file = {/Users/rkkautsar/Zotero/storage/PB6CA5XM/Tovar et al. - 2018 - A Job Sizing Strategy for High-Throughput Scientif.pdf}
}


@misc{rodolaCrossplatformLibProcess2019,
  title = {Cross-Platform Lib for Process and System Monitoring in {{Python}}: Giampaolo/Psutil},
  copyright = {BSD},
  shorttitle = {Cross-Platform Lib for Process and System Monitoring in {{Python}}},
  author = {Rodola', Giampaolo},
  url={https://github.com/giampaolo/psutil},
  year = {2019}
}

@misc{WatchesFilesRecords2019,
  title = {Watches Files and Records, or Triggers Actions, When They Change: Facebook/Watchman},
  copyright = {Apache-2.0},
  shorttitle = {Watches Files and Records, or Triggers Actions, When They Change.},
  author = {Facebook},
  url={https://github.com/facebook/watchman},
  year = {2019}
}

@misc{UbuntuManpageForkstat,
  title = {Ubuntu {{Manpage}}: forkstat - a tool to show process fork/exec/exit activity},
  author = {King, Colin},
  month=december,
  year = {2018},
  url = {http://manpages.ubuntu.com/manpages/disco/en/man8/forkstat.8.html},
  file = {/Users/rkkautsar/Zotero/storage/A7YTA6UY/forkstat.8.html}
}


@misc{netblue30LinuxNamespacesSeccompbpf2019,
  title = {Linux Namespaces and Seccomp-Bpf Sandbox},
  copyright = {GPL-2.0},
  url = {https://github.com/netblue30/firejail},
  author = {{netblue30}},
  year = {2019}
}

@misc{LinuxContainers,
  title = {Linux {{Containers}}},
  url = {https://linuxcontainers.org/},
  file = {/Users/rkkautsar/Zotero/storage/F5H8GTHR/linuxcontainers.org.html}
}


@misc{kautsarPsmonMonitorsLimits2019,
  title = {Psmon - {{Monitors}} and Limits Process Resource.},
  copyright = {MIT},
  author = {Kautsar, Rakha Kanz},
  url = {https://github.com/rkkautsar/psmon},
  year = {2019}
}



@misc{OracleVMVirtualBox,
  title = {Oracle {{VM VirtualBox}}},
  url = {https://www.virtualbox.org/},
  file = {/Users/rkkautsar/Zotero/storage/298G9VCL/www.virtualbox.org.html},
  Note = {{Accessed on 29 May 2019}}
}

@misc{XenProject,
  title = {Xen {{Project}}},
  language = {en-US},
  journal = {Xen Project},
  url = {https://xenproject.org/},
  file = {/Users/rkkautsar/Zotero/storage/9HKKVYBS/xenproject.org.html},
  Note = {{Accessed on 29 May 2019}}
}

@misc{scooleyIntroductionHyperVWindows,
  title = {Introduction to {{Hyper}}-{{V}} on {{Windows}} 10},
  abstract = {Introduction to Hyper-V, virtualization, and related technologies.},
  language = {en-us},
  url = {https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/about/},
  author = {Cooley, Sarah},
  file = {/Users/rkkautsar/Zotero/storage/N45W6XGS/about.html},
  Note = {{Accessed on 29 May 2019}}
}


@misc{PegasusWorkflowManagement2019,
  title = {Pegasus {{Workflow Management System}}: {{Automate}}, Recover, and Debug Scientific Computations. - Pegasus-Isi/Pegasus},
  copyright = {Apache-2.0},
  shorttitle = {Pegasus {{Workflow Management System}}},
  author = {{Pegasus Project}},
  url = {https://github.com/pegasus-isi/pegasus},
  year = {2019}
}

@misc{shvedScriptMeasureLimit2019,
  title = {A Script to Measure and Limit {{CPU}} Time and Memory Consumption of Black-Box Processes in {{Linux}}: Pshved/Timeout},
  shorttitle = {A Script to Measure and Limit {{CPU}} Time and Memory Consumption of Black-Box Processes in {{Linux}}},
  author = {Shved, Paul},
  url = {https://github.com/pshved/timeout},
  year = {2019}
}


@misc{LightweightProcessIsolation2019,
  title = {A Light-Weight Process Isolation Tool, Making Use of {{Linux}} Namespaces and Seccomp-Bpf Syscall Filters (with Help of the Kafel Bpf Language): Google/Nsjail},
  copyright = {Apache-2.0},
  shorttitle = {A Light-Weight Process Isolation Tool, Making Use of {{Linux}} Namespaces and Seccomp-Bpf Syscall Filters (with Help of the Kafel Bpf Language)},
  url = {https://github.com/google/nsjail},
  author = {Google},
  year = {2019}
}


@misc{LanguageLibrarySpecifying2019,
  title = {A Language and Library for Specifying Syscall Filtering Policies.: Google/Kafel},
  copyright = {Apache-2.0},
  shorttitle = {A Language and Library for Specifying Syscall Filtering Policies.},
  url = {https://github.com/google/kafel},
  author = {Google},
  year = {2019}
}


@misc{SandboxSecurelyExecuting2019,
  title = {Sandbox for Securely Executing Untrusted Programs. {{Contribute}} to Ioi/Isolate Development by Creating an Account on {{GitHub}}},
  copyright = {GNU GPL},
  url = {https://github.com/ioi/isolate},
  author = {{International Olympiad in Informatics}},
  year = {2019}
}


@inproceedings{maggioloCMSGrowingGrading2014,
  title = {{{CMS}}: A {{Growing Grading System}}},
  volume = {8},
  abstract = {Computer science at high school often focuses on programming, but a broader view of other areas of computer science has key benefits for both writing programs that are more efficient and making more theoretical concepts more accessible to those who do not find programming intrinsically interesting. With the introduction of computer science at high schools, a lack of coherent resources for teachers and students prompted the development of the NZ Computer Science Field Guide, an open-source, on-line textbook.},
  language = {en},
  booktitle = {Olympiads in {{Informatics}}},
  author = {Maggiolo, Stefano and Mascellani, Giovanni and Wehrestedt, Luca},
  year = {2014},
  pages = {123-131},
  file = {/Users/rkkautsar/Zotero/storage/BMF3NT73/Dagienė - 2014 - OLYMPIADS IN INFORMATICS.pdf}
}


@misc{RunLim,
  title = {{{RunLim}}},
  journal = {Institute for Formal Models and Verification},
  url = {http://fmv.jku.at/runlim/},
  file = {/Users/rkkautsar/Zotero/storage/4Y35TLCV/runlim.html},
  note = {Accessed on 29 May 2019}
}


@misc{GitmirrorBenchmarktoolContribute2018,
  title = {Git-Mirror of the Benchmark-Tool.},
  author = {Potassco},
  url = {https://github.com/potassco/benchmark-tool},
  month = jun,
  year = {2018}
}

@misc{ResourceMonitorCooperative,
  title = {Resource {{Monitor}} - {{Cooperative Computing Lab}}},
  author = {{The Cooperative Computing Lab}},
  url = {http://ccl.cse.nd.edu/software/resource\_monitor/},
  file = {/Users/rkkautsar/Zotero/storage/IRG2BFJI/resource_monitor.html}
}

@misc{sebastienPerformanceMonitoringTools2019,
  title = {Performance Monitoring Tools for {{Linux}}.},
  copyright = {GPL-2.0},
  author = {Sebastien, GODARD},
  url = {https://github.com/sysstat/sysstat},
  month = may,
  year = {2019}
}



@misc{StarExecCrossCommunity2019,
  title = {{{StarExec}} Is a Cross Community Logic Solving Service: {{StarExec}}/{{StarExec}}},
  copyright = {MIT},
  shorttitle = {{{StarExec}} Is a Cross Community Logic Solving Service},
  author = {StarExec},
  url = {https://github.com/StarExec/StarExec},
  month = may,
  year = {2019}
}

@inproceedings{beyer2017software,
  title={Software verification with validation of results},
  author={Beyer, Dirk},
  booktitle={International Conference on Tools and Algorithms for the Construction and Analysis of Systems},
  pages={331--349},
  year={2017},
  organization={Springer}
}

@inproceedings{yoo2003slurm,
  title = {Slurm: {{Simple}} Linux Utility for Resource Management},
  booktitle = {Workshop on {{Job Scheduling Strategies}} for {{Parallel Processing}}},
  author = {Yoo, Andy B and Jette, Morris A and Grondona, Mark},
  year = {2003},
  pages = {44-60},
  file = {/Users/rkkautsar/Zotero/storage/EW69TBZ8/241220.pdf},
  organization = {{Springer}}
}

@article{condor-practice, 
  author    = "Douglas Thain and Todd Tannenbaum and Miron Livny",
  title     = "Distributed computing in practice: the Condor experience.",
  journal   = "Concurrency - Practice and Experience",
  volume    = "17",
  number    = "2-4",
  year      = "2005",
  pages     = "323-356",
}