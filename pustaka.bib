
@article{pengReproducibleResearchComputational2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  volume = {334},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible},
  language = {en},
  number = {6060},
  journal = {Science},
  author = {Peng, Roger D.},
  month = dec,
  year = {2011},
  pages = {1226-1227},
  file = {/Users/rkkautsar/Zotero/storage/YW3DSQ68/Peng - 2011 - Reproducible Research in Computational Science.pdf}
}

@article{vitekR3RepeatabilityReproducibility2012,
  title = {R3: Repeatability, Reproducibility and Rigor},
  volume = {47},
  issn = {03621340},
  shorttitle = {R3},
  doi = {10.1145/2442776.2442781},
  abstract = {Computer systems research spans sub-disciplines that include embedded systems, programming languages and compilers, networking, and operating systems. Our contention is that a number of structural factors inhibit quality systems research. We highlight some of the factors we have encountered in our own work and observed in published papers and propose solutions that could both increase the productivity of researchers and the quality of their output.},
  language = {en},
  number = {4a},
  journal = {ACM SIGPLAN Notices},
  author = {Vitek, Jan and Kalibera, Tomas},
  month = mar,
  year = {2012},
  pages = {30},
  file = {/Users/rkkautsar/Zotero/storage/UAAWS4DU/Vitek and Kalibera - 2012 - R3 repeatability, reproducibility and rigor.pdf}
}

@article{gentRecomputationManifesto2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1304.3674},
  primaryClass = {cs},
  title = {The {{Recomputation Manifesto}}},
  abstract = {Replication of scientific experiments is critical to the advance of science. Unfortunately, the discipline of Computer Science has never treated replication seriously, even though computers are very good at doing the same thing over and over again. Not only are experiments rarely replicated, they are rarely even replicable in a meaningful way. Scientists are being encouraged to make their source code available, but this is only a small step. Even in the happy event that source code can be built and run successfully, running code is a long way away from being able to replicate the experiment that code was used for. I propose that the discipline of Computer Science must embrace replication of experiments as standard practice. I propose that the only credible technique to make experiments truly replicable is to provide copies of virtual machines in which the experiments are validated to run. I propose that tools and repositories should be made available to make this happen. I propose to be one of those who makes it happen.},
  journal = {arXiv:1304.3674 [cs]},
  author = {Gent, Ian P.},
  month = apr,
  year = {2013},
  keywords = {Computer Science - Digital Libraries,Computer Science - General Literature},
  file = {/Users/rkkautsar/Zotero/storage/FEHXU6QD/Gent - 2013 - The Recomputation Manifesto.pdf;/Users/rkkautsar/Zotero/storage/TFWTNRF3/1304.html}
}

@article{goodmanWhatDoesResearch2016,
  title = {What Does Research Reproducibility Mean?},
  volume = {8},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.aaf5027},
  abstract = {The language and conceptual framework of ``research reproducibility'' are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for ``truth.''},
  language = {en},
  number = {341},
  journal = {Science Translational Medicine},
  author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
  month = jun,
  year = {2016},
  pages = {341ps12-341ps12},
  file = {/Users/rkkautsar/Zotero/storage/NCDHTLBP/Goodman et al. - 2016 - What does research reproducibility mean.pdf}
}

@article{johnsonTheoreticiansGuideExperimental2002,
  series = {{{DIMACS Series}} in {{Discrete Mathematics}} and                         {{Theoretical Computer Science}}},
  title = {A {{Theoreticians Guide}} to the {{Experimental Analysis}} of {{Algorithms}}},
  volume = {59},
  shorttitle = {Data {{Structures}}, {{Near Neighbor Searches}}, and {{Methodology}}},
  abstract = {This paper presents an informal discussion of issues that arise when one attempts to analyze algorithms experimentally. It is based on lessons learned by the author over the course of more than a decade of experimentation, survey paper writing, refereeing, and lively discussions with other experimentalists. Although written from the perspective of a theoretical computer scientist, it is intended to be of use to researchers from all elds who want to study algorithms experimentally. It has two goals: rst, to provide a useful guide to new experimentalists about how such work can best be performed and written up, and second, to challenge current researchers to think about whether their own work might be improved from a scienti c point of view. With the latter purpose in mind, the author hopes that at least a few of his recommendations will be considered controversial.},
  language = {en},
  journal = {Data structures, near neighbor searches, and methodology: fifth and sixth DIMACS implementation challenges},
  editor = {Johnson, David S.},
  month = dec,
  year = {2002},
  pages = {215-250},
  file = {/Users/rkkautsar/Zotero/storage/E24N83ZM/Johnson - 2002 - A Theoreticians Guide to the Experimental Analysis.pdf},
  doi = {10.1090/dimacs/059}
}

@article{boettigerIntroductionDockerReproducible2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1410.0846},
  title = {An Introduction to {{Docker}} for Reproducible Research, with Examples from the {{R}} Environment},
  volume = {49},
  issn = {01635980},
  doi = {10.1145/2723872.2723882},
  abstract = {As computational work becomes more and more integral to many aspects of scientific research, computational reproducibility has become an issue of increasing importance to computer systems researchers and domain scientists alike. Though computational reproducibility seems more straight forward than replicating physical experiments, the complex and rapidly changing nature of computer environments makes being able to reproduce and extend such work a serious challenge. In this paper, I explore common reasons that code developed for one research project cannot be successfully executed or extended by subsequent researchers. I review current approaches to these issues, including virtual machines and workflow systems, and their limitations. I then examine how the popular emerging technology Docker combines several areas from systems research - such as operating system virtualization, cross-platform portability, modular re-usable elements, versioning, and a `DevOps' philosophy, to address these challenges. I illustrate this with several examples of Docker use with a focus on the R statistical environment.},
  number = {1},
  journal = {ACM SIGOPS Operating Systems Review},
  author = {Boettiger, Carl},
  month = jan,
  year = {2015},
  keywords = {Computer Science - Software Engineering},
  pages = {71-79},
  file = {/Users/rkkautsar/Zotero/storage/YGYYCI7Z/Boettiger - 2015 - An introduction to Docker for reproducible researc.pdf;/Users/rkkautsar/Zotero/storage/G2ZIUNE7/1410.html}
}

@article{gundersenStateArtReproducibility,
  title = {State of the {{Art}}: {{Reproducibility}} in {{Artificial Intelligence}}},
  abstract = {Background: Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between 20\% and 30\% of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with increased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported.},
  language = {en},
  author = {Gundersen, Odd Erik and Kjensmo, Sigbjorn},
  pages = {8},
  file = {/Users/rkkautsar/Zotero/storage/VI6HWTS4/Gundersen and Kjensmo - State of the Art Reproducibility in Artificial In.pdf}
}

@article{sandveTenSimpleRules2013,
  title = {Ten {{Simple Rules}} for {{Reproducible Computational Research}}},
  volume = {9},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003285},
  language = {en},
  number = {10},
  journal = {PLoS Computational Biology},
  author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
  editor = {Bourne, Philip E.},
  month = oct,
  year = {2013},
  pages = {e1003285},
  file = {/Users/rkkautsar/Zotero/storage/7P9M9L6R/Sandve et al. - 2013 - Ten Simple Rules for Reproducible Computational Re.pdf}
}

@article{buckSolvingReproducibility2015,
  title = {Solving Reproducibility},
  volume = {348},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac8041},
  language = {en},
  number = {6242},
  journal = {Science},
  author = {Buck, S.},
  month = jun,
  year = {2015},
  pages = {1403-1403},
  file = {/Users/rkkautsar/Zotero/storage/HN3NRTHR/Buck - 2015 - Solving reproducibility.pdf}
}

@article{drummondReplicabilityNotReproducibility,
  title = {Replicability Is Not {{Reproducibility}}: {{Nor}} Is It {{Good Science}}},
  abstract = {At various machine learning conferences, at various times, there have been discussions arising from the inability to replicate the experimental results published in a paper. There seems to be a wide spread view that we need to do something to address this problem, as it is essential to the advancement of our field. The most compelling argument would seem to be that reproducibility of experimental results is the hallmark of science. Therefore, given that most of us regard machine learning as a scientific discipline, being able to replicate experiments is paramount. I want to challenge this view by separating the notion of reproducibility, a generally desirable property, from replicability, its poor cousin. I claim there are important differences between the two. Reproducibility requires changes; replicability avoids them. Although reproducibility is desirable, I contend that the impoverished version, replicability, is one not worth having.},
  language = {en},
  author = {Drummond, Chris},
  pages = {4},
  file = {/Users/rkkautsar/Zotero/storage/K3SNM7C6/Drummond - Replicability is not Reproducibility Nor is it Go.pdf}
}

@inproceedings{chirigatiReproZipComputationalReproducibility2016,
  address = {San Francisco, California, USA},
  title = {{{ReproZip}}: {{Computational Reproducibility With Ease}}},
  isbn = {978-1-4503-3531-7},
  shorttitle = {{{ReproZip}}},
  doi = {10.1145/2882903.2899401},
  abstract = {We present ReproZip, the recommended packaging tool for the SIGMOD Reproducibility Review. ReproZip was designed to simplify the process of making an existing computational experiment reproducible across platforms, even when the experiment was put together without reproducibility in mind. The tool creates a self-contained package for an experiment by automatically tracking and identifying all its required dependencies. The researcher can share the package with others, who can then use ReproZip to unpack the experiment, reproduce the findings on their favorite operating system, as well as modify the original experiment for reuse in new research, all with little effort. The demo will consist of examples of non-trivial experiments, showing how these can be packed in a Linux machine and reproduced on different machines and operating systems. Demo visitors will also be able to pack and reproduce their own experiments.},
  language = {en},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}} - {{SIGMOD}} '16},
  publisher = {{ACM Press}},
  author = {Chirigati, Fernando and Rampin, R\'emi and Shasha, Dennis and Freire, Juliana},
  year = {2016},
  pages = {2085-2088},
  file = {/Users/rkkautsar/Zotero/storage/R7P5LDT3/Chirigati et al. - 2016 - ReproZip Computational Reproducibility With Ease.pdf}
}

@article{collbergRepeatabilityComputerSystems2016,
  title = {Repeatability in Computer Systems Research},
  volume = {59},
  issn = {00010782},
  doi = {10.1145/2812803},
  language = {en},
  number = {3},
  journal = {Communications of the ACM},
  author = {Collberg, Christian and Proebsting, Todd A.},
  month = feb,
  year = {2016},
  pages = {62-69},
  file = {/Users/rkkautsar/Zotero/storage/QR439S4M/Collberg and Proebsting - 2016 - Repeatability in computer systems research.pdf}
}

@inproceedings{greffSacredInfrastructureComputational2017,
  address = {Austin, Texas},
  title = {The {{Sacred Infrastructure}} for {{Computational Research}}},
  doi = {10.25080/shinma-7f4c6e7-008},
  language = {en},
  booktitle = {Proceedings of the 16th {{Python}} in {{Science Conference}}},
  publisher = {{SciPy}},
  author = {Greff, Klaus and Klein, Aaron and Chovanec, Martin and Hutter, Frank and Schmidhuber, J\"urgen},
  year = {2017},
  pages = {49-56},
  file = {/Users/rkkautsar/Zotero/storage/GJ45YZZT/Greff et al. - 2017 - The Sacred Infrastructure for Computational Resear.pdf}
}

@article{beyerReliableBenchmarkingRequirements2019,
  title = {Reliable Benchmarking: Requirements and Solutions},
  volume = {21},
  issn = {1433-2779, 1433-2787},
  shorttitle = {Reliable Benchmarking},
  doi = {10.1007/s10009-017-0469-y},
  abstract = {Benchmarking is a widely used method in experimental computer science, in particular, for the comparative evaluation of tools and algorithms. As a consequence, a number of questions need to be answered in order to ensure proper benchmarking, resource measurement, and presentation of results, all of which is essential for researchers, tool developers, and users, as well as for tool competitions. We identify a set of requirements that are indispensable for reliable benchmarking and resource measurement of time and memory usage of automatic solvers, verifiers, and similar tools, and discuss limitations of existing methods and benchmarking tools. Fulfilling these requirements in a benchmarking framework can (on Linux systems) currently only be done by using the cgroup and namespace features of the kernel. We developed BenchExec, a ready-to-use, tool-independent, and open-source implementation of a benchmarking framework that fulfills all presented requirements, making reliable benchmarking and resource measurement easy. Our framework is able to work with a wide range of different tools, has proven its reliability and usefulness in the International Competition on Software Verification, and is used by several research groups worldwide to ensure reliable benchmarking. Finally, we present guidelines on how to present measurement results in a scientifically valid and comprehensible way.},
  language = {en},
  number = {1},
  journal = {International Journal on Software Tools for Technology Transfer},
  author = {Beyer, Dirk and L\"owe, Stefan and Wendler, Philipp},
  month = feb,
  year = {2019},
  pages = {1-29},
  file = {/Users/rkkautsar/Zotero/storage/XSNBLHZR/Beyer et al. - 2019 - Reliable benchmarking requirements and solutions.pdf}
}


