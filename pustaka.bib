
@article{pengReproducibleResearchComputational2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  volume = {334},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible},
  language = {en},
  number = {6060},
  journal = {Science},
  author = {Peng, Roger D.},
  month = dec,
  year = {2011},
  pages = {1226-1227},
  file = {/Users/rkkautsar/Zotero/storage/YW3DSQ68/Peng - 2011 - Reproducible Research in Computational Science.pdf}
}

@article{vitekR3RepeatabilityReproducibility2012,
  title = {R3: Repeatability, Reproducibility and Rigor},
  volume = {47},
  issn = {03621340},
  shorttitle = {R3},
  doi = {10.1145/2442776.2442781},
  abstract = {Computer systems research spans sub-disciplines that include embedded systems, programming languages and compilers, networking, and operating systems. Our contention is that a number of structural factors inhibit quality systems research. We highlight some of the factors we have encountered in our own work and observed in published papers and propose solutions that could both increase the productivity of researchers and the quality of their output.},
  language = {en},
  number = {4a},
  journal = {ACM SIGPLAN Notices},
  author = {Vitek, Jan and Kalibera, Tomas},
  month = mar,
  year = {2012},
  pages = {30},
  file = {/Users/rkkautsar/Zotero/storage/UAAWS4DU/Vitek and Kalibera - 2012 - R3 repeatability, reproducibility and rigor.pdf}
}

@article{gentRecomputationManifesto2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1304.3674},
  primaryClass = {cs},
  title = {The {{Recomputation Manifesto}}},
  abstract = {Replication of scientific experiments is critical to the advance of science. Unfortunately, the discipline of Computer Science has never treated replication seriously, even though computers are very good at doing the same thing over and over again. Not only are experiments rarely replicated, they are rarely even replicable in a meaningful way. Scientists are being encouraged to make their source code available, but this is only a small step. Even in the happy event that source code can be built and run successfully, running code is a long way away from being able to replicate the experiment that code was used for. I propose that the discipline of Computer Science must embrace replication of experiments as standard practice. I propose that the only credible technique to make experiments truly replicable is to provide copies of virtual machines in which the experiments are validated to run. I propose that tools and repositories should be made available to make this happen. I propose to be one of those who makes it happen.},
  journal = {arXiv:1304.3674 [cs]},
  author = {Gent, Ian P.},
  month = apr,
  year = {2013},
  keywords = {Computer Science - Digital Libraries,Computer Science - General Literature},
  file = {/Users/rkkautsar/Zotero/storage/FEHXU6QD/Gent - 2013 - The Recomputation Manifesto.pdf;/Users/rkkautsar/Zotero/storage/TFWTNRF3/1304.html}
}

@article{goodmanWhatDoesResearch2016,
  title = {What Does Research Reproducibility Mean?},
  volume = {8},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.aaf5027},
  abstract = {The language and conceptual framework of ``research reproducibility'' are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for ``truth.''},
  language = {en},
  number = {341},
  journal = {Science Translational Medicine},
  author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
  month = jun,
  year = {2016},
  pages = {341ps12-341ps12},
  file = {/Users/rkkautsar/Zotero/storage/NCDHTLBP/Goodman et al. - 2016 - What does research reproducibility mean.pdf}
}

@article{johnsonTheoreticiansGuideExperimental2002,
  series = {{{DIMACS Series}} in {{Discrete Mathematics}} and                         {{Theoretical Computer Science}}},
  title = {A {{Theoreticians Guide}} to the {{Experimental Analysis}} of {{Algorithms}}},
  volume = {59},
  shorttitle = {Data {{Structures}}, {{Near Neighbor Searches}}, and {{Methodology}}},
  abstract = {This paper presents an informal discussion of issues that arise when one attempts to analyze algorithms experimentally. It is based on lessons learned by the author over the course of more than a decade of experimentation, survey paper writing, refereeing, and lively discussions with other experimentalists. Although written from the perspective of a theoretical computer scientist, it is intended to be of use to researchers from all elds who want to study algorithms experimentally. It has two goals: rst, to provide a useful guide to new experimentalists about how such work can best be performed and written up, and second, to challenge current researchers to think about whether their own work might be improved from a scienti c point of view. With the latter purpose in mind, the author hopes that at least a few of his recommendations will be considered controversial.},
  language = {en},
  journal = {Data structures, near neighbor searches, and methodology: fifth and sixth DIMACS implementation challenges},
  editor = {Johnson, David S.},
  month = dec,
  year = {2002},
  pages = {215-250},
  file = {/Users/rkkautsar/Zotero/storage/E24N83ZM/Johnson - 2002 - A Theoreticians Guide to the Experimental Analysis.pdf},
  doi = {10.1090/dimacs/059}
}

@article{boettigerIntroductionDockerReproducible2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1410.0846},
  title = {An Introduction to {{Docker}} for Reproducible Research, with Examples from the {{R}} Environment},
  volume = {49},
  issn = {01635980},
  doi = {10.1145/2723872.2723882},
  abstract = {As computational work becomes more and more integral to many aspects of scientific research, computational reproducibility has become an issue of increasing importance to computer systems researchers and domain scientists alike. Though computational reproducibility seems more straight forward than replicating physical experiments, the complex and rapidly changing nature of computer environments makes being able to reproduce and extend such work a serious challenge. In this paper, I explore common reasons that code developed for one research project cannot be successfully executed or extended by subsequent researchers. I review current approaches to these issues, including virtual machines and workflow systems, and their limitations. I then examine how the popular emerging technology Docker combines several areas from systems research - such as operating system virtualization, cross-platform portability, modular re-usable elements, versioning, and a `DevOps' philosophy, to address these challenges. I illustrate this with several examples of Docker use with a focus on the R statistical environment.},
  number = {1},
  journal = {ACM SIGOPS Operating Systems Review},
  author = {Boettiger, Carl},
  month = jan,
  year = {2015},
  keywords = {Computer Science - Software Engineering},
  pages = {71-79},
  file = {/Users/rkkautsar/Zotero/storage/YGYYCI7Z/Boettiger - 2015 - An introduction to Docker for reproducible researc.pdf;/Users/rkkautsar/Zotero/storage/G2ZIUNE7/1410.html}
}

@article{gundersenStateArtReproducibility2018,
  title = {State of the {{Art}}: {{Reproducibility}} in {{Artificial Intelligence}}},
  abstract = {Background: Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between 20\% and 30\% of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with increased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported.},
  language = {en},
  author = {Gundersen, Odd Erik and Kjensmo, Sigbjorn},
  year = {2018},
  file = {/Users/rkkautsar/Zotero/storage/VI6HWTS4/Gundersen and Kjensmo - State of the Art Reproducibility in Artificial In.pdf}
}

@article{sandveTenSimpleRules2013,
  title = {Ten {{Simple Rules}} for {{Reproducible Computational Research}}},
  volume = {9},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003285},
  language = {en},
  number = {10},
  journal = {PLoS Computational Biology},
  author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
  editor = {Bourne, Philip E.},
  month = oct,
  year = {2013},
  pages = {e1003285},
  file = {/Users/rkkautsar/Zotero/storage/7P9M9L6R/Sandve et al. - 2013 - Ten Simple Rules for Reproducible Computational Re.pdf}
}

@article{buckSolvingReproducibility2015,
  title = {Solving Reproducibility},
  volume = {348},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac8041},
  language = {en},
  number = {6242},
  journal = {Science},
  author = {Buck, S.},
  month = jun,
  year = {2015},
  pages = {1403-1403},
  file = {/Users/rkkautsar/Zotero/storage/HN3NRTHR/Buck - 2015 - Solving reproducibility.pdf}
}

@article{drummondReplicabilityNotReproducibility2009,
  title = {Replicability Is Not {{Reproducibility}}: {{Nor}} Is It {{Good Science}}},
  abstract = {At various machine learning conferences, at various times, there have been discussions arising from the inability to replicate the experimental results published in a paper. There seems to be a wide spread view that we need to do something to address this problem, as it is essential to the advancement of our field. The most compelling argument would seem to be that reproducibility of experimental results is the hallmark of science. Therefore, given that most of us regard machine learning as a scientific discipline, being able to replicate experiments is paramount. I want to challenge this view by separating the notion of reproducibility, a generally desirable property, from replicability, its poor cousin. I claim there are important differences between the two. Reproducibility requires changes; replicability avoids them. Although reproducibility is desirable, I contend that the impoverished version, replicability, is one not worth having.},
  language = {en},
  author = {Drummond, Chris},
  year = {2009},
  file = {/Users/rkkautsar/Zotero/storage/K3SNM7C6/Drummond - Replicability is not Reproducibility Nor is it Go.pdf}
}

@inproceedings{chirigatiReproZipComputationalReproducibility2016,
  address = {San Francisco, California, USA},
  title = {{{ReproZip}}: {{Computational Reproducibility With Ease}}},
  isbn = {978-1-4503-3531-7},
  shorttitle = {{{ReproZip}}},
  doi = {10.1145/2882903.2899401},
  abstract = {We present ReproZip, the recommended packaging tool for the SIGMOD Reproducibility Review. ReproZip was designed to simplify the process of making an existing computational experiment reproducible across platforms, even when the experiment was put together without reproducibility in mind. The tool creates a self-contained package for an experiment by automatically tracking and identifying all its required dependencies. The researcher can share the package with others, who can then use ReproZip to unpack the experiment, reproduce the findings on their favorite operating system, as well as modify the original experiment for reuse in new research, all with little effort. The demo will consist of examples of non-trivial experiments, showing how these can be packed in a Linux machine and reproduced on different machines and operating systems. Demo visitors will also be able to pack and reproduce their own experiments.},
  language = {en},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}} - {{SIGMOD}} '16},
  publisher = {{ACM Press}},
  author = {Chirigati, Fernando and Rampin, R\'emi and Shasha, Dennis and Freire, Juliana},
  year = {2016},
  pages = {2085-2088},
  file = {/Users/rkkautsar/Zotero/storage/R7P5LDT3/Chirigati et al. - 2016 - ReproZip Computational Reproducibility With Ease.pdf}
}

@article{collbergRepeatabilityComputerSystems2016,
  title = {Repeatability in Computer Systems Research},
  volume = {59},
  issn = {00010782},
  doi = {10.1145/2812803},
  language = {en},
  number = {3},
  journal = {Communications of the ACM},
  author = {Collberg, Christian and Proebsting, Todd A.},
  month = feb,
  year = {2016},
  pages = {62-69},
  file = {/Users/rkkautsar/Zotero/storage/QR439S4M/Collberg and Proebsting - 2016 - Repeatability in computer systems research.pdf}
}

@inproceedings{greffSacredInfrastructureComputational2017,
  address = {Austin, Texas},
  title = {The {{Sacred Infrastructure}} for {{Computational Research}}},
  doi = {10.25080/shinma-7f4c6e7-008},
  language = {en},
  booktitle = {Proceedings of the 16th {{Python}} in {{Science Conference}}},
  publisher = {{SciPy}},
  author = {Greff, Klaus and Klein, Aaron and Chovanec, Martin and Hutter, Frank and Schmidhuber, J\"urgen},
  year = {2017},
  pages = {49-56},
  file = {/Users/rkkautsar/Zotero/storage/GJ45YZZT/Greff et al. - 2017 - The Sacred Infrastructure for Computational Resear.pdf}
}

@article{beyerReliableBenchmarkingRequirements2019,
  title = {Reliable Benchmarking: Requirements and Solutions},
  volume = {21},
  issn = {1433-2779, 1433-2787},
  shorttitle = {Reliable Benchmarking},
  doi = {10.1007/s10009-017-0469-y},
  abstract = {Benchmarking is a widely used method in experimental computer science, in particular, for the comparative evaluation of tools and algorithms. As a consequence, a number of questions need to be answered in order to ensure proper benchmarking, resource measurement, and presentation of results, all of which is essential for researchers, tool developers, and users, as well as for tool competitions. We identify a set of requirements that are indispensable for reliable benchmarking and resource measurement of time and memory usage of automatic solvers, verifiers, and similar tools, and discuss limitations of existing methods and benchmarking tools. Fulfilling these requirements in a benchmarking framework can (on Linux systems) currently only be done by using the cgroup and namespace features of the kernel. We developed BenchExec, a ready-to-use, tool-independent, and open-source implementation of a benchmarking framework that fulfills all presented requirements, making reliable benchmarking and resource measurement easy. Our framework is able to work with a wide range of different tools, has proven its reliability and usefulness in the International Competition on Software Verification, and is used by several research groups worldwide to ensure reliable benchmarking. Finally, we present guidelines on how to present measurement results in a scientifically valid and comprehensible way.},
  language = {en},
  number = {1},
  journal = {International Journal on Software Tools for Technology Transfer},
  author = {Beyer, Dirk and L\"owe, Stefan and Wendler, Philipp},
  month = feb,
  year = {2019},
  pages = {1-29},
  file = {/Users/rkkautsar/Zotero/storage/XSNBLHZR/Beyer et al. - 2019 - Reliable benchmarking requirements and solutions.pdf}
}

@inproceedings{juvePracticalResourceMonitoring2015,
  address = {Chicago, IL, USA},
  title = {Practical {{Resource Monitoring}} for {{Robust High Throughput Computing}}},
  isbn = {978-1-4673-6598-7},
  doi = {10.1109/CLUSTER.2015.115},
  abstract = {Robust high throughput computing requires effective monitoring and enforcement of a variety of resources including CPU cores, memory, disk, and network traffic. Without effective monitoring and enforcement, it is easy to overload machines, causing failures and slowdowns, or underload machines, which results in wasted opportunities. This paper explores how to describe, measure, and enforce resources used by computational tasks. We focus on tasks running in distributed execution systems, in which a task requests the resources it needs, and the execution system ensures the availability of such resources. This presents two non-trivial problems: how to measure the resources consumed by a task, and how to monitor and report resource exhaustion in a robust and timely manner. For both of these tasks, operating systems have a variety of mechanisms with different degrees of availability, accuracy, overhead, and intrusiveness. We develop a model to describe various forms of monitoring and map the available mechanisms in contemporary operating systems to that model. Based on this analysis, we present two specific monitoring tools that choose different tradeoffs in overhead and accuracy, and evaluate them on a selection of benchmarks. We conclude by describing our experience in collecting large quantities of monitoring data for complex workflows.},
  language = {en},
  booktitle = {2015 {{IEEE International Conference}} on {{Cluster Computing}}},
  publisher = {{IEEE}},
  author = {Juve, Gideon and Tovar, Benjamin and da Silva, Rafael Ferreira and Krol, Dariusz and Thain, Douglas and Deelman, Ewa and Allcock, William and Livny, Miron},
  month = sep,
  year = {2015},
  pages = {650-657},
  file = {/Users/rkkautsar/Zotero/storage/U4TG2ZWD/Juve et al. - 2015 - Practical Resource Monitoring for Robust High Thro.pdf}
}

@inproceedings{hoeflerScientificBenchmarkingParallel2015,
  address = {Austin, Texas},
  title = {Scientific Benchmarking of Parallel Computing Systems: Twelve Ways to Tell the Masses When Reporting Performance Results},
  isbn = {978-1-4503-3723-6},
  shorttitle = {Scientific Benchmarking of Parallel Computing Systems},
  doi = {10.1145/2807591.2807644},
  abstract = {Measuring and reporting performance of parallel computers constitutes the basis for scientific advancement of high-performance computing (HPC). Most scientific reports show performance improvements of new techniques and are thus obliged to ensure reproducibility or at least interpretability. Our investigation of a stratified sample of 120 papers across three top conferences in the field shows that the state of the practice is lacking. For example, it is often unclear if reported improvements are deterministic or observed by chance. In addition to distilling best practices from existing work, we propose statistically sound analysis and reporting techniques and simple guidelines for experimental design in parallel computing and codify them in a portable benchmarking library. We aim to improve the standards of reporting research results and initiate a discussion in the HPC field. A wide adoption of our minimal set of rules will lead to better interpretability of performance results and improve the scientific culture in HPC.},
  language = {en},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}} on - {{SC}} '15},
  publisher = {{ACM Press}},
  author = {Hoefler, Torsten and Belli, Roberto},
  year = {2015},
  pages = {1-12},
  file = {/Users/rkkautsar/Zotero/storage/QHPNUW3S/hoefler-scientific-benchmarking.pdf}
}

@article{crowderReportingComputationalExperiments1978,
  title = {Reporting Computational Experiments in Mathematical Programming},
  volume = {15},
  issn = {0025-5610, 1436-4646},
  doi = {10.1007/BF01609036},
  language = {en},
  number = {1},
  journal = {Mathematical Programming},
  author = {Crowder, Harlan P. and Dembo, Ron S. and Mulvey, John M.},
  month = dec,
  year = {1978},
  pages = {316-329},
  file = {/Users/rkkautsar/Zotero/storage/M3V9IGD8/Crowder et al. - 1978 - Reporting computational experiments in mathematica.pdf}
}

@book{hockneyScienceComputerBenchmarking1996,
  address = {Philadelphia},
  series = {Software, Environments, Tools},
  title = {The Science of Computer Benchmarking},
  isbn = {978-0-89871-363-3},
  lccn = {QA76.9.E94 H63 1996},
  language = {en},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Hockney, Roger W.},
  year = {1996},
  keywords = {Electronic digital computers,Evaluation},
  file = {/Users/rkkautsar/Zotero/storage/R2HWCJB4/Hockney - 1996 - The science of computer benchmarking.pdf}
}

@article{coffinStatisticalAnalysisComputational2000,
  title = {Statistical {{Analysis}} of {{Computational Tests}} of {{Algorithms}} and {{Heuristics}}},
  volume = {12},
  issn = {1091-9856, 1526-5528},
  doi = {10.1287/ijoc.12.1.24.11899},
  language = {en},
  number = {1},
  journal = {INFORMS Journal on Computing},
  author = {Coffin, Marie and Saltzman, Matthew J.},
  month = feb,
  year = {2000},
  pages = {24-44},
  file = {/Users/rkkautsar/Zotero/storage/GD8JJFLA/Coffin and Saltzman - 2000 - Statistical Analysis of Computational Tests of Alg.pdf}
}

@incollection{stumpStarExecCrossCommunityInfrastructure2014,
  address = {Cham},
  title = {{{StarExec}}: {{A Cross}}-{{Community Infrastructure}} for {{Logic Solving}}},
  volume = {8562},
  isbn = {978-3-319-08586-9 978-3-319-08587-6},
  shorttitle = {{{StarExec}}},
  abstract = {We introduce StarExec, a public web-based service built to facilitate the experimental evaluation of logic solvers, broadly understood as automated tools based on formal reasoning. Examples of such tools include theorem provers, SAT and SMT solvers, constraint solvers, model checkers, and software verifiers. The service, running on a compute cluster with 380 processors and 23 terabytes of disk space, is designed to provide a single piece of storage and computing infrastructure to logic solving communities and their members. It aims at reducing duplication of effort and resources as well as enabling individual researchers or groups with no access to comparable infrastructure. StarExec allows community organizers to store, manage and make available benchmark libraries; competition organizers to run logic solver competitions; and community members to do comparative evaluations of logic solvers on public or private benchmark problems.},
  language = {en},
  booktitle = {Automated {{Reasoning}}},
  publisher = {{Springer International Publishing}},
  author = {Stump, Aaron and Sutcliffe, Geoff and Tinelli, Cesare},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Demri, St\'ephane and Kapur, Deepak and Weidenbach, Christoph},
  year = {2014},
  pages = {367-373},
  file = {/Users/rkkautsar/Zotero/storage/3SPQ4463/Stump et al. - 2014 - StarExec A Cross-Community Infrastructure for Log.pdf},
  doi = {10.1007/978-3-319-08587-6_28}
}

@article{tichyShouldComputerScientists1998,
  title = {Should Computer Scientists Experiment More?},
  volume = {31},
  issn = {0018-9162},
  doi = {10.1109/2.675631},
  abstract = {Computer scientists and practitioners defend their lack of experimentation with a wide range of arguments. Some arguments suggest that experimentation is inappropriate, too difficult, useless, and even harmful. This article discusses several such arguments to illustrate the importance of experimentation for computer science. It considers how the software industry is beginning to value experiments, because results may give a company a three- to five-year lead over the competition.},
  number = {5},
  journal = {Computer},
  author = {Tichy, W. F.},
  month = may,
  year = {1998},
  keywords = {Brain modeling,company,competition,Computer aided manufacturing,computer science,Computer science,computer scientists,DP industry,experimentation,Humans,Immune system,Laser modes,Laser theory,Nervous system,Probability,software industry,Testing},
  pages = {32-40},
  file = {/Users/rkkautsar/Zotero/storage/UAET4SSP/Tichy - 1998 - Should computer scientists experiment more.pdf;/Users/rkkautsar/Zotero/storage/6JQ6DAZZ/675631.html}
}

@article{piccoloToolsTechniquesComputational2016,
  title = {Tools and Techniques for Computational Reproducibility},
  volume = {5},
  issn = {2047-217X},
  doi = {10.1186/s13742-016-0135-4},
  abstract = {When reporting research findings, scientists document the steps they followed so that others can verify and build upon the research. When those steps have been described in sufficient detail that others can retrace the steps and obtain similar results, the research is said to be reproducible. Computers play a vital role in many research disciplines and present both opportunities and challenges for reproducibility. Computers can be programmed to execute analysis tasks, and those programs can be repeated and shared with others. The deterministic nature of most computer programs means that the same analysis tasks, applied to the same data, will often produce the same outputs. However, in practice, computational findings often cannot be reproduced because of complexities in how software is packaged, installed, and executed\textemdash{}and because of limitations associated with how scientists document analysis steps. Many tools and techniques are available to help overcome these challenges; here we describe seven such strategies. With a broad scientific audience in mind, we describe the strengths and limitations of each approach, as well as the circumstances under which each might be applied. No single strategy is sufficient for every scenario; thus we emphasize that it is often useful to combine approaches.},
  language = {en},
  number = {1},
  journal = {GigaScience},
  author = {Piccolo, Stephen R. and Frampton, Michael B.},
  month = dec,
  year = {2016},
  file = {/Users/rkkautsar/Zotero/storage/C5JXXN3W/Piccolo and Frampton - 2016 - Tools and techniques for computational reproducibi.pdf}
}

@incollection{charwatVCWCVersioningCompetition2013,
  address = {Berlin, Heidelberg},
  title = {{{VCWC}}: {{A Versioning Competition Workflow Compiler}}},
  volume = {8148},
  isbn = {978-3-642-40563-1 978-3-642-40564-8},
  shorttitle = {{{VCWC}}},
  language = {en},
  booktitle = {Logic {{Programming}} and {{Nonmonotonic Reasoning}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Charwat, G\"unther and Ianni, Giovambattista and Krennwallner, Thomas and Kronegger, Martin and Pfandler, Andreas and Redl, Christoph and Schwengerer, Martin and Spendier, Lara Katharina and Wallner, Johannes Peter and Xiao, Guohui},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Cabalar, Pedro and Son, Tran Cao},
  year = {2013},
  pages = {233-238},
  file = {/Users/rkkautsar/Zotero/storage/XW34YN6A/Charwat et al. - 2013 - VCWC A Versioning Competition Workflow Compiler.pdf},
  doi = {10.1007/978-3-642-40564-8_23}
}

@article{lukowiczExperimentalEvaluationComputer1994,
  title = {Experimental {{Evaluation}} in {{Computer Science}}: {{A Quantitative Study}}},
  volume = {28},
  shorttitle = {Experimental {{Evaluation}} in {{Computer Science}}},
  abstract = {A survey of over 400 recent research articles suggests that computer scientists publish relatively few papers with experimentally validated results. The survey includes complete volumes of several refereed computer science journals, a conference, and 50 titles drawn at random from all articles published by ACM in 1993. The journals Optical Engineering (OE) and Neural Computation (NC) were used for comparison. Of the papers in the random sample that would require experimental validation, 40\% have none at all. In journals related to software engineering, this fraction is over 50\%. In comparison, the fraction of papers lacking quantitative evaluation in OE and NC is only 15\% and 12\%, respectively. Conversely, the fraction of papers that devote one fifth or more of their space to experimental validation is almost 70\% for OE and NC, while it is a mere 30\% for the CS random sample and 20\% for software engineering. The low ratio of validated results appears to be a serious weakness in compute...},
  journal = {Journal of Systems and Software},
  author = {Lukowicz, Paul and Heinz, Ernst A. and Prechelt, Lutz and Tichy, Walter F.},
  year = {1994},
  pages = {9--18},
  file = {/Users/rkkautsar/Zotero/storage/IEBH4459/Lukowicz et al. - 1994 - Experimental Evaluation in Computer Science A Qua.pdf;/Users/rkkautsar/Zotero/storage/N7VQ2GCX/summary.html}
}

@article{dodig-crnkovicScientificMethodsComputer,
  title = {Scientific {{Methods}} in {{Computer Science}}},
  abstract = {This paper analyzes scientific aspects of Computer Science. First it defines science and scientific method in general. It gives a discussion of relations between science, research, development and technology.},
  language = {en},
  author = {{Dodig-Crnkovic}, Gordana},
  pages = {7},
  file = {/Users/rkkautsar/Zotero/storage/XL2JDCJP/Dodig-Crnkovic - Scientific Methods in Computer Science.pdf}
}

@inproceedings{simUsingBenchmarkingAdvance2003,
  title = {Using Benchmarking to Advance Research: A Challenge to Software Engineering},
  shorttitle = {Using Benchmarking to Advance Research},
  doi = {10.1109/ICSE.2003.1201189},
  abstract = {Benchmarks have been used in computer science to compare the performance of computer systems, information retrieval algorithms, databases, and many other technologies. The creation and widespread use of a benchmark within a research area is frequently accompanied by rapid technical progress and community building. These observations have led us to formulate a theory of benchmarking within scientific disciplines. Based on this theory, we challenge software engineering research to become more scientific and cohesive by working as a community to define benchmarks. In support of this challenge, we present a case study of the reverse engineering community, where we have successfully used benchmarks to advance the state of research.},
  booktitle = {25th {{International Conference}} on {{Software Engineering}}, 2003. {{Proceedings}}.},
  author = {Sim, S. E. and Easterbrook, S. and Holt, R. C.},
  month = may,
  year = {2003},
  keywords = {Computer science,benchmark testing,benchmarking,Books,Collaboration,community building,Computer languages,computer system performance,Databases,Guidelines,Information retrieval,information retrieval algorithms,reverse engineering,Reverse engineering,reverse engineering community,Software algorithms,software engineering,Software engineering,software engineering research,technical progress},
  pages = {74-83},
  file = {/Users/rkkautsar/Zotero/storage/U7P57WT7/Sim et al. - 2003 - Using benchmarking to advance research a challeng.pdf;/Users/rkkautsar/Zotero/storage/XPSFZ5YU/1201189.html}
}

@article{kitchenhamPreliminaryGuidelinesEmpirical2002,
  title = {Preliminary Guidelines for Empirical Research in Software Engineering},
  volume = {28},
  issn = {0098-5589},
  doi = {10.1109/TSE.2002.1027796},
  abstract = {Empirical software engineering research needs research guidelines to improve the research and reporting processes. We propose a preliminary set of research guidelines aimed at stimulating discussion among software researchers. They are based on a review of research guidelines developed for medical researchers and on our own experience in doing and reviewing software engineering research. The guidelines are intended to assist researchers, reviewers, and meta-analysts in designing, conducting, and evaluating empirical studies. Editorial boards of software engineering journals may wish to use our recommendations as a basis for developing guidelines for reviewers and for framing policies for dealing with the design, data collection, and analysis and reporting of empirical studies.},
  number = {8},
  journal = {IEEE Transactions on Software Engineering},
  author = {Kitchenham, B. A. and Pfleeger, S. L. and Pickard, L. M. and Jones, P. W. and Hoaglin, D. C. and Emam, K. El and Rosenberg, J.},
  month = aug,
  year = {2002},
  keywords = {Computer science,Guidelines,software engineering,Software engineering,Computer Society,Data analysis,Laboratories,Software performance,software researchers,Software standards,Statistics,Surgery},
  pages = {721-734},
  file = {/Users/rkkautsar/Zotero/storage/M3CM9X8T/Kitchenham et al. - 2002 - Preliminary guidelines for empirical research in s.pdf;/Users/rkkautsar/Zotero/storage/RNJ3V46P/1027796.html}
}

@book{makSolvingEverydayProblems2009,
  title = {Solving {{Everyday Problems}} with the {{Scientific Method}} - {{Thinking Like}} a {{Scientist}}},
  isbn = {978-981-283-510-9},
  language = {en},
  publisher = {{World Scientific Publishing Co. Pte. Ltd.}},
  author = {Mak, Don K. and Mak, Angela T. and Mak, Anthony B.},
  year = {2009},
  file = {/Users/rkkautsar/Zotero/storage/MF9PTYRT/Mak et al. - 2009 - Solving Everyday Problems with the Scientific Meth.pdf},
  doi = {10.1142/9789812835109}
}

@book{kossoSummaryScientificMethod2011,
  title = {A {{Summary}} of {{Scientific Method}}},
  language = {en},
  publisher = {{Springer Netherlands}},
  author = {Kosso, Peter},
  year = {2011},
  file = {/Users/rkkautsar/Zotero/storage/DEZ5G7IT/Kosso - 2011 - A Summary of Scientific Method.pdf},
  note = {OCLC: 943713117}
}

@book{careyBeginnerGuideScientific2012,
  address = {Boston, MA},
  edition = {4th ed},
  title = {A Beginner's Guide to Scientific Method},
  isbn = {978-1-111-30555-0 978-1-111-72601-0},
  lccn = {Q175 .C27 2012},
  language = {en},
  publisher = {{Wadsworth Cengage Learning}},
  author = {Carey, Stephen S.},
  year = {2012},
  keywords = {Forschungsmethode,Méthodologie,Methodology,Recherche,Research,Science,Sciences,Wissenschaftliches Arbeiten},
  file = {/Users/rkkautsar/Zotero/storage/7ZIQJEUL/Carey - 2012 - A beginner's guide to scientific method.pdf;/Users/rkkautsar/Zotero/storage/TL96R8S6/Carey - 2012 - A beginner's guide to scientific method.pdf},
  note = {OCLC: ocn698377925}
}

@book{gowerScientificMethodHistorical2012,
  edition = {1},
  title = {Scientific {{Method}}: {{A Historical}} and {{Philosophical Introduction}}},
  isbn = {978-0-203-04612-8},
  shorttitle = {Scientific {{Method}}},
  language = {en},
  publisher = {{Routledge}},
  author = {Gower, Barry},
  month = oct,
  year = {2012},
  file = {/Users/rkkautsar/Zotero/storage/YUXYZGVR/Gower - 2012 - Scientific Method A Historical and Philosophical .pdf},
  doi = {10.4324/9780203046128}
}

@book{wallimanResearchMethodsBasics2010a,
  title = {Research {{Methods}}: {{The Basics}}},
  isbn = {978-0-203-83607-1},
  shorttitle = {Research {{Methods}}},
  language = {en},
  publisher = {{Routledge}},
  author = {Walliman, Nicholas},
  month = nov,
  year = {2010},
  file = {/Users/rkkautsar/Zotero/storage/VJIIFXZW/Walliman - 2010 - Research Methods The Basics.pdf},
  doi = {10.4324/9780203836071}
}

@article{doelemanImagingEventHorizon2009,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0906.3899},
  primaryClass = {astro-ph},
  title = {Imaging an {{Event Horizon}}: Submm-{{VLBI}} of a {{Super Massive Black Hole}}},
  shorttitle = {Imaging an {{Event Horizon}}},
  abstract = {A long standing goal in astrophysics is to directly observe the immediate environment of a black hole with angular resolution comparable to the event horizon. Realizing this goal would open a new window on the study of General Relativity in the strong field regime, accretion and outflow processes at the edge of a black hole, the existence of an event horizon, and fundamental black hole physics (e.g., spin). Steady long-term progress on improving the capability of Very Long Baseline Interferometry (VLBI) at short wavelengths has now made it extremely likely that this goal will be achieved within the next decade. The most compelling evidence for this is the recent observation by 1.3mm VLBI of Schwarzschild radius scale structure in SgrA*, the compact source of radio, submm, NIR and xrays at the center of the Milky Way. SgrA* is thought to mark the position of a \textasciitilde{}4 million solar mass black hole, and because of its proximity and estimated mass presents the largest apparent event horizon size of any black hole candidate in the Universe. Over the next decade, existing and planned mm/submm facilities will be combined into a high sensitivity, high angular resolution "Event Horizon Telescope" that will bring us as close to the edge of black hole as we will come for decades. This white paper describes the science case for mm/submm VLBI observations of both SgrA* and M87 (a radio loud AGN of a much more luminous class that SgrA*). We emphasize that while there is development and procurement involved, the technical path forward is clear, and the recent successful observations have removed much of the risk that would normally be associated with such an ambitious project.},
  journal = {arXiv:0906.3899 [astro-ph]},
  author = {Doeleman, Sheperd and Agol, Eric and Backer, Don and Baganoff, Fred and Bower, Geoffrey C. and Broderick, Avery and Fabian, Andrew and Fish, Vincent and Gammie, Charles and Ho, Paul and Honma, Mareki and Krichbaum, Thomas and Loeb, Avi and Marrone, Dan and Reid, Mark and Rogers, Alan E. E. and Shapiro, Irwin and Strittmatter, Peter and Tilanus, Remo and Weintroub, Jonathan and Whitney, Alan and Wright, Melvyn and Ziurys, Lucy},
  month = jun,
  year = {2009},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/Users/rkkautsar/Zotero/storage/JD5YMEQC/Doeleman et al. - 2009 - Imaging an Event Horizon submm-VLBI of a Super Ma.pdf;/Users/rkkautsar/Zotero/storage/9LYF3Z46/0906.html}
}

@book{montgomeryDesignAnalysisExperiments2013,
  address = {Hoboken, NJ},
  edition = {Eighth},
  title = {Design and Analysis of Experiments},
  isbn = {978-1-118-14692-7},
  lccn = {QA279 .M66 2013},
  abstract = {"The eighth edition of Design and Analysis of Experiments continues to provide extensive and in-depth information on engineering, business, and statistics-as well as informative ways to help readers design and analyze experiments for improving the quality, efficiency and performance of working systems. Furthermore, the text maintains its comprehensive coverage by including: new examples, exercises, and problems (including in the areas of biochemistry and biotechnology); new topics and problems in the area of response surface; new topics in nested and split-plot design; and the residual maximum likelihood method is now emphasized throughout the book"--},
  language = {en},
  publisher = {{John Wiley \& Sons, Inc}},
  author = {Montgomery, Douglas C.},
  year = {2013},
  keywords = {Experimental design,TECHNOLOGY \& ENGINEERING / Industrial Engineering},
  file = {/Users/rkkautsar/Zotero/storage/CXSFPHA2/Montgomery - 2013 - Design and analysis of experiments.pdf}
}

@book{ryanModernExperimentalDesign2007a,
  title = {Modern {{Experimental Design}}},
  publisher = {{Wiley}},
  author = {Ryan, Thomas P.},
  year = {2007},
  file = {/Users/rkkautsar/Zotero/storage/RZ94SEXK/Modern Experimental Design.pdf}
}


@misc{philipp_wendler_2019_2561835,
  author       = {Philipp Wendler and
                  Dirk Beyer and
                  Thomas Lemberger and
                  Karlheinz Friedberger and
                  Daniel Dietsch and
                  Marek Chalupa and
                  Williame Rocha and
                  George Karpenkov and
                  Peter Schrammel and
                  Michael Tautschnig and
                  Herbert and
                  Thomas Bunk and
                  altmattr and
                  Zvonimir and
                  IljaZakharov and
                  Vladimír Štill and
                  Truc Nguyen Lam and
                  MartinNowack and
                  Animesh Basak Chowdhury and
                  yinliangze and
                  Guang Chen and
                  avritichauhan and
                  PRITOM RAJKHOWA and
                  Thomas Stieglmaier and
                  Jan Mrázek and
                  Stephan Lukasczyk and
                  Florian Heck and
                  Sebastian Ott and
                  Mikhail Ramalho and
                  LE, Ton Chanh},
  title        = {sosy-lab/benchexec: Release 1.18},
  month        = feb,
  year         = 2019,
  doi          = {10.5281/zenodo.2561835}
}

@misc{benchkit:2013,
  Author = {F. Kordon and
  N. Gibelin and
  F. Hulin-Hubard and
  F. Pommereau},
  Howpublished = {{http://benchkit.cosyverif.org}},
  Lastchecked = {2013},
  Title = {{BenchKit User Manual}},
  Urldate = {2013},
  year = {2013}
}


@inproceedings{kordonBenchKitToolMassive2014,
  title = {{{BenchKit}}, a {{Tool}} for {{Massive Concurrent Benchmarking}}},
  doi = {10.1109/ACSD.2014.12},
  abstract = {Benchmarking numerous programs in a reasonable time requires the use of several (potentially multicore) computers. We experimented such a situation in the context of the MCC (Model Checking Contest @ Petri net) where we had to operate more than 52000 runs for the 2013 edition. This paper presents BenchKit, a tool to operate programs on sets of potentially parallel machines and to gather monitoring information like CPU or memory usage. It also samples such data over the execution time. BenchKit has been elaborated in the context of the MCC and will be used for the 2014 edition.},
  booktitle = {2014 14th {{International Conference}} on {{Application}} of {{Concurrency}} to {{System Design}}},
  author = {Kordon, F. and {Hulin-Hubard}, F.},
  month = jun,
  year = {2014},
  keywords = {BenchKit,benchmark testing,Benchmark testing,Computers,concurrency (computers),Context,Distributed Computing,Evaluation of programs,formal verification,Kernel,Magnetic heads,massive concurrent benchmarking,MCC,model checking contest,Monitoring,multicore computers,multiprocessing programs,Petri net,Petri nets,Virtual machines,Virtualization},
  pages = {159-165},
  file = {/Users/rkkautsar/Zotero/storage/2KHSLSA8/Kordon and Hulin-Hubard - 2014 - BenchKit, a Tool for Massive Concurrent Benchmarki.pdf;/Users/rkkautsar/Zotero/storage/HH5FKZA9/7016339.html}
}



@article{frings2010flexible,
  title = {A Flexible, Application-and Platform-Independent Environment for Benchmarking},
  volume = {19},
  journal = {Parallel Computing: From Multicores and GPU's to Petascale},
  author = {Frings, Wolfgang and Schnurpfeil, Alexander and Meier, Stefanie and Janetzko, Florian and Arnold, Lukas},
  year = {2010},
  pages = {423},
  file = {/Users/rkkautsar/Zotero/storage/LK3VZ8C9/Frings et al. - 2010 - A flexible, application-and platform-independent e.pdf},
  publisher = {{IOS Press}}
}



@inproceedings{kimPracticalEffectiveSandboxing2013,
  title = {Practical and Effective Sandboxing for Non-Root Users},
  abstract = {MBOX is a lightweight sandboxing mechanism for nonroot users in commodity OSes. MBOX's sandbox usage model executes a program in the sandbox and prevents the program from modifying the host filesystem by layering the sandbox filesystem on top of the host filesystem. At the end of program execution, the user can examine changes in the sandbox filesystem and selectively commit them back to the host filesystem. MBOX implements this by interposing on system calls and provides a variety of useful applications: installing system packages as a non-root user, running unknown binaries safely without network accesses, checkpointing the host filesystem instantly, and setting up a virtual development environment without special tools. Our performance evaluation shows that MBOX imposes CPU overheads of 0.1\textendash{}45.2\% for various workloads. In this paper, we present MBOX's design, efficient techniques for interposing on system calls, our experience avoiding common system call interposition pitfalls, and MBOX's performance evaluation.},
  language = {en},
  booktitle = {Proceedings of {{USENIX Annual Technical Conference}}},
  author = {Kim, Taesoo and Zeldovich, Nickolai},
  year = {2013},
  pages = {139-144},
  file = {/Users/rkkautsar/Zotero/storage/E9QQU2B5/Kim and Zeldovich - Practical and effective sandboxing for non-root us.pdf}
}



@inproceedings{scheepersVirtualizationContainerizationApplication2014,
  title = {Virtualization and {{Containerization}} of {{Application Infrastructure}}: {{A Comparison}}},
  abstract = {Modern cloud infrastructure uses virtualization to isolate applications, optimize the utilization of hardware resources and provide operational flexibility. However, conventional virtualization comes at the cost of resource overhead.},
  language = {en},
  booktitle = {Proceedings of the 21st {{Twente Student Conference}} on {{IT}}},
  author = {Scheepers, Mathijs Jeroen},
  month = jun,
  year = {2014},
  pages = {7},
  file = {/Users/rkkautsar/Zotero/storage/2YUZF55I/Scheepers - Virtualization and Containerization of Application.pdf}
}



@article{zhangComparativeStudyContainers2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.01842},
  primaryClass = {cs},
  title = {A {{Comparative Study}} of {{Containers}} and {{Virtual Machines}} in {{Big Data Environment}}},
  abstract = {Container technique is gaining increasing attention in recent years and has become an alternative to traditional virtual machines. Some of the primary motivations for the enterprise to adopt the container technology include its convenience to encapsulate and deploy applications, lightweight operations, as well as efficiency and flexibility in resources sharing. However, there still lacks an in-depth and systematic comparison study on how big data applications, such as Spark jobs, perform between a container environment and a virtual machine environment. In this paper, by running various Spark applications with different configurations, we evaluate the two environments from many interesting aspects, such as how convenient the execution environment can be set up, what are makespans of different workloads running in each setup, how efficient the hardware resources, such as CPU and memory, are utilized, and how well each environment can scale. The results show that compared with virtual machines, containers provide a more easy-to-deploy and scalable environment for big data workloads. The research work in this paper can help practitioners and researchers to make more informed decisions on tuning their cloud environment and configuring the big data applications, so as to achieve better performance and higher resources utilization.},
  journal = {arXiv:1807.01842 [cs]},
  author = {Zhang, Qi and Liu, Ling and Pu, Calton and Dou, Qiwei and Wu, Liren and Zhou, Wei},
  month = jul,
  year = {2018},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Performance},
  file = {/Users/rkkautsar/Zotero/storage/855FGS7W/Zhang et al. - 2018 - A Comparative Study of Containers and Virtual Mach.pdf;/Users/rkkautsar/Zotero/storage/GV4U4WE3/1807.html}
}


@article{rousselControllingSolverExecution2011,
  title = {Controlling a {{Solver Execution}} with the Runsolver {{Tool}}},
  volume = {7},
  abstract = {The runsolver tool was designed for the 2005 edition of the pseudo-Boolean competition in order to solve the problem of correctly measuring the resources used by solvers, especially solvers with multiple processes. Since then, it has been improved in several directions and adopted by several other competitions or frameworks. This paper presents the inner working of this tool and the technical problems that it addresses.},
  language = {en},
  journal = {Journal on Satisfiability, Boolean Modeling and Computation},
  author = {Roussel, Olivier},
  year = {2011},
  pages = {139-144},
  file = {/Users/rkkautsar/Zotero/storage/2TKWZZZK/Roussel - Controlling a Solver Execution with the runsolver .pdf}
}


@article{marevs2012new,
  title = {A {{New Contest Sandbox}}.},
  volume = {6},
  journal = {Olympiads in Informatics},
  author = {Mare{\v s}, Martin and Blackham, Bernard},
  year = {2012},
  file = {/Users/rkkautsar/Zotero/storage/96PB6PXC/isolate.pdf}
}




@book{hintjens2013zeromq,
  title = {{{ZeroMQ}}: Messaging for Many Applications},
  publisher = {{" O'Reilly Media, Inc."}},
  author = {Hintjens, Pieter},
  year = {2013},
  file = {/Users/rkkautsar/Zotero/storage/EHLPD72H/Pieter Hintjens - ZeroMQ_ Messaging for Many Applications (2013, O'Reilly Media).pdf}
}




