\chapter{\chImplementation}
\label{ch:implementation}

\section{System Design}
\label{sec:impl.design}

This section discusses the design of \OurBenchmarkingTool~and its rationale.

\subsection{Architecture}
\label{sec:impl.architecture}

\begin{figure}
    \centering
    \ifdraft{
        \dummyfig{assets/diagrams/arch.tikz}
    }{
        \input{assets/diagrams/arch.tikz}
    }
    \caption{Architecture of \OurBenchmarkingTool}
    \label{fig:architecture}
\end{figure}

Figure \ref{fig:architecture} shows the architecture of \OurBenchmarkingTool, our attempt on a new benchmarking tool fulfilling most of the defined requirements.
The architecture follows the client-server design pattern, communicating through Transmission Control Protocol (TCP).
The client manages the heavy computation while the server manages the data.
Typically, the client will be in a high performance computing (HPC) cluster system while the server in a separate system, e.g. a virtual private server (VPS).

This decision to use client-server architecture is made to handle concurrent writes to the database, which is difficult to achieve in some database engines.
With a single server process to manage the database, the writes can be queued in the application level.
This provides a clean separation of computation and data.

Furthermore, the server can be separated to another system outside of the cluster system, allowing avoidance of filesystem-related issues such as locking issues on NFS (Network File System).
Long running job is arguably an anti-pattern in job scheduling, because not only it will take a long time before it's scheduled, it will also prevent other jobs from being scheduled.

\emph{Server} manages the data in the database.
It receives events through a TCP socket and optionally send replies.
To achieve extensibility, the server maintains a specified set of \emph{observers}, each listening for related events.
The events are distributed through a publisher-subscriber design pattern.
The observer will then executes its jobs, such as inserting data to the database.

\emph{Bootstrapper} is a component that will read the configuration file from the user, prepare the computing environment, and then tell the server $R \in C \times I$, the set of benchmark runs that will be executed.
Among the things prepared are the tools, benchmark instances, and database schema.
Preparing the specified tools and instances may additionally include downloading and running related setting up steps.
In short, the bootstrapping is also separated to the computation-related and data-related between the client-side and server-side.

\emph{Manager} manages the benchmark workers.
This component acts as an interface to the underlying job submitting system, or even implements its own job queue as in the case of the \emph{local} manager.
The manager is responsible for deploying, assigning tasks, and stopping the benchmark workers, making sure they finishes the assigned job.
This allows the benchmarking tool to be agnostic about the execution environment and can be extended in many ways.

\emph{Workers} do the heavy computing steps, typically in parallel.
Because the benchmark run is embarrassingly parallel\footnote{there is no effort needed to parallelize the problem since there is no dependencies between tasks}, each worker can be run as its own process without interacting with other workers.
Submitted to a HPC cluster, this allows the computation to be executed as fast as possible.
A worker is assigned a run identifier and executes the needed steps for the benchmark run after asking the context (i.e. $r = (c, i) \in R$) from the server.

A worker consists of smaller building blocks called \emph{run steps}.
The run step is executed in sequence.
In most cases, one of these steps, called the \emph{executor}, acts as a resource monitor and executes the tool configuration $c$ with the instance $i$.
The executor then measures and limits the execution of the tool.
Finally, each step can report its result to the server.

The worker only executes one specific benchmark run as opposed to requesting many jobs as its available.
This is also to prevent worker to be a long-running job.
A worker maps nicely to a job in the HPC cluster system.

\emph{Analyzer} aggregates the data from the database and analyze it, outputting a presentable result.
This component is usually used after the benchmarking is finished, but it can also be used to serve a live analysis of a benchmarking in process.
Analyzer also consists of steps that is implemented as a modular, reusable module.
This is to encourage code reuse and minimize the effort of doing the common analysis of research results.


\subsection{Messaging}

\begin{figure}
    \ifdraft{
        \dummyfig{assets/diagrams/zeromq.tikz}
    }{
        \input{assets/diagrams/zeromq.tikz}
    }
    \caption{Messaging architecture of \OurBenchmarkingTool}
    \label{fig:zmq}
\end{figure}

Figure \ref{fig:zmq} gives an overview of the messaging architecture and the overall network of \OurBenchmarkingTool.
Each process is independent to each other and can be separated across virtual nodes.
This means it can be implemented in a single node, in a cluster system, or even across clusters.
Data is transported through TCP for inter-process communication, and through local in-process (inter-thread) transportation for intra-process communication.
Communication follows some basic messaging patterns used by the \O MQ (ZeroMQ)\footnote{\href{http://zeromq.org/}{http://zeromq.org/}} messaging framework, namely \textsc{router-dealer} and \textsc{pub-sub}.

\First~decided to use \O MQ to avoid reinventing the wheel and just focus the core functionalities.
\O MQ provides many messaging patterns for many use cases.
It is also lightweight and easily available in most platform with a lot of supported languages.

The \textsc{router-dealer} patterns allows two-way communication between the party \citep{hintjens2013zeromq}.
This is used in the communication between the worker and the server gateway.
The patterns allows many workers to send events to the server and optionally request for a reply from the server.
The message sent to a \textsc{router} socket is enveloped by a unique identifier, allowing the \textsc{router} to reply to the correct \textsc{dealer}.
This powerful pattern allows reliable many to one communication between the workers and the server gateway.

On the other hand, \textsc{pub-sub} patterns follows the publisher-subscriber pattern as its name suggests \citep{hintjens2013zeromq}.
This is a fan-out pattern, on which the publisher just publish the message without caring if the message is received.
Both the publisher and subscriber does not know each other.
The \textsc{pub} socket just publish the message to the socket, and the \textsc{sub} sockets listen to one or more `topic'.
This allows fast distribution of event message received by the server gateway to the possibly many observers.
The downside is the distribution is not reliable since the message is just thrown without confirmation if the other party is ready.
This can be corrected with proper synchronization using other messaging pattern if needed.


\subsection{Benchmarking Workflow}
\begin{figure}
    \centering
    \ifdraft{
        \dummyfig{assets/pics/workflow-swimlane.png}
    }{
        \includegraphics[width=\textwidth]{assets/pics/workflow-swimlane.png}
    }
    \caption{Benchmarking workflow}
    \label{fig:swimlane}
\end{figure}

Figure \ref{fig:swimlane} presents the steps and interactions taken by each actors, namely the Client, Workers, and Server.
Aside from the already defined Workers and Server in Section \ref{sec:impl.architecture} above, Client represents the actual user interfacing with \OurBenchmarkingTool.
Note that the figure assumes the server is already started.

First, the client uses the bootstrapper to read the configuration and then send a bootstrap event to the server.
This event includes the needed information for the server to also set up the database schema.
Then the server replies its readiness to the client.
After that, the client uses the manager to spawn workers.
This is the end of the interaction needed by the user.

Then, after being scheduled by the job scheduling system used by the manager, each worker request the context for its assigned run identifier to the server.
The server replies with the context.
This includes things such as the tool and its configuration, the input instance, resource limits, etc.

Then come the main computation represented as run steps.
Each steps is executed sequentially.
After execution of the step, typically the worker will report the result to the server which in turn publish that event to observers.
Any observers listening to the event will execute its work, often time this means inserting data to the database. After there is no more run step to execute, the worker sends a finish event to the server and terminates.

As soon as soon as the bootstrapping is done, analysis can be executed from the available data in database.
This allows the flexibility of doing either on-demand analysis or live analysis.


\subsection{Benchmarking Model}

\First~decided to define the data as relational model of SQL, as opposed to nonrelational model of NoSQL.
The reason is because SQL provides an easy and fast interface for querying and aggregating data, tasks that occur often when analyzing benchmark results.
The benchmarking process itself can be defined nicely into structured relations, centering on benchmark runs.

\begin{figure}
    \centering
    \ifdraft{
        \dummyfig{assets/pics/erd.png}
    }{
        \includegraphics[width=\textwidth]{assets/pics/erd.png}
    }
    \caption{Entity-relationship diagram of \OurBenchmarkingTool's model}
    \label{fig:erd}
\end{figure}

The benchmarking process is modeled as in the entity-relationship diagram (ERD) shown in Figure \ref{fig:erd}.
The database stores every configuration defined to support reproducibility.
Among the core entities are \textsc{run}, \textsc{tool}, \textsc{parameter}, \textsc{limit}, \textsc{step}, \textsc{observer}, and \textsc{task}.
Some entities like \textsc{parameter} and \textsc{task} are grouped to another intermediate many-to-many entity, that is \textsc{parameterGroup} and \textsc{taskGroup}.
It can be seen that the relationship centers around the \textsc{run} entity.

Other modules, like the ones used as steps, can then register its own model to store its data, such as \textsc{runstatistic} (by the executor module) or \textsc{runnode} (by the system information collector module).
This supports separation of concern and encapsulation of the module.
User can use community-created module without having to consider its data model.

In fact, it is also possible to move all the data that is normally stored in the filesystem into the SQL table.
Many SQL database engine provides a BLOB data type to store binary data.
For example, \textsc{SQLite} even has an archiving tool called \code{sqlar}\footnote{\href{https://www.sqlite.org/sqlar}{https://www.sqlite.org/sqlar}} that use the database as storage just like a ZIP archive.
This allow the database to store everything needed to reproduce the benchmark.
But \first~decided not to do it to make debugging runs easier, a task which also occur surprisingly often alongside benchmarking.

\section{Implementation}

This section presents details regarding the implementation of \OurBenchmarkingTool.
The implementation is attached in the appendix.
It is also available as Git repository: \href{https://github.com/rkkautsar/reprobench}{\code{https://github.com/rkkautsar/reprobench}} and pypi package: \href{https://pypi.org/project/reprobench/}{\code{https://pypi.org/project/reprobench/}}.

\subsection{Development Environment}

\First~choose Python as the main programming language.
Specifically, Python 3 is used as its predecessor Python 2 will meet its end of the line and not be supported anymore in 2020.
It is a widely available language with huge community.
Python also has good interopability with C and C++, it's relatively easy to write wrappers of C libraries to Python.
That said, there is a huge collection of publicly available python packages in the Python Package Index (pypi)\footnote{\href{https://pypi.org/}{https://pypi.org/}} readily installable through \code{pip} tool, which often distributed alongside Python.
This allows for rapid software development by reducing common implementation.

Being an interpreter and not a compiled language, Python is relatively slow.
For this project, speed can be sacrificed in place of developer experience.
Most of the CPU usage will be used by the benchmarked program anyway, so the development speed is arguably more important.

\First~use a combination of \textsc{pyenv}\footnote{\href{https://github.com/pyenv/pyenv}{https://github.com/pyenv/pyenv}} and \textsc{poetry}\footnote{\href{https://github.com/sdispater/poetry}{https://github.com/sdispater/poetry}} to manage dependencies and package publishing.
With \textsc{pyenv}, it is easy to switch between specific versions of Python and also pin a project to use specific version of Python.
This allow reproducible development environment across machines without influence from other projects using different Python version.
Additionally, testing if the software works under specific version of Python is a trivial task.
\textsc{poetry} on the other hand, manages the dependencies in a virtual environment with version locking.
This allows separate, reproducible dependencies across projects.
\textsc{poetry} also provide simple API to package and publish the project to pypi.
Under a UNIX system, setting up both tools is easy and works without superuser privilege, as seen in \lst~\ref{lst:impl.setup.pyenv.poetry}.

\begin{listing}
    \begin{minted}{bash}
$ curl https://pyenv.run | bash
$ pyenv install "3.7.2"
$ pyenv global "3.7.2"
$ pip install poetry
    \end{minted}
    \caption{Setting up \textsc{pyenv} and \textsc{poetry}}
    \label{lst:impl.setup.pyenv.poetry}
\end{listing}

The source code is organized by the Git version control and made publicly available in GitHub.
This allows others to freely contribute to the project and also encourage them to also make their modules open-source.
Additionally, the source code is licensed under the MIT License to allow permissive usage.

\subsection{Project Structure}

The source code organization and their brief descriptions is listed below:
\begin{itemize}
    \item \emph{examples/}\\
    This directory provides a collection of working usage benchmark examples demonstrating various features of the benchmarking tool.

    \item \emph{pyproject.toml}\\
    Configuration file describing the various dependencies used and the packaging options used for publishing purposes.

    \item \emph{reprobench/console/}\\
    This directory consists of the implementations of the command line interface (CLI), including the \code{reprobench} root command and related setup for the subcommands.

    \item \emph{reprobench/core/}\\
    The core implementation of \OurBenchmarkingTool~is organized in this directory.
    This includes generic classes, database models, and implementation for the worker, server, bootstrapper, and analyzer components.

    \item \emph{reprobench/executors/}\\
    Implementations of the special run step doing the actual execution and monitoring of the tools and its input.
    This includes the accompanying observer, its database model and related events.

    \item \emph{reprobench/managers/}\\
    Implementations of the manager component and its CLI subcommands under the \code{reprobench manage} subcommand, e.g. \code{reprobench manage local} or \code{reprobench manage slurm}.
    In the future, it is preferable for each manager to be separated into its own package.

    \item \emph{reprobench/statistics/}\\
    This directory consists of reference implementations of the analysis step modules.
    It is encouraged to implement analysis steps as separate packages and only put the most general ones in the main project.

    \item \emph{reprobench/task\_sources/}\\
    This directory consists of various implementation for extracting tasks from the configuration.
    Currently, this includes local filesystem sourced tasks, URL-based remote sourced tasks, and Digital Object Identifier (DOI) sourced tasks.

    \item \emph{reprobench/tools/}\\
    This directory implements some base classes to be extended for writing tool interfaces.
    It is also possible in the future to allow tools to be defined only from the configuration, further lowering the barrier for using this benchmarking tool.
    Writing tool interface is discussed in Section \ref{sec:impl.tools}.

    \item \emph{reprobench/utils.py}\\
    This file consists of various small utilities commonly used by the implementation.


\end{itemize}


\subsection{Configuration}

\begin{listing}
    \inputminted{yaml}{assets/listings/reprobench/examples/sat/benchmark.yml}
    \caption{Example of \OurBenchmarkingTool~configuration file}
    \label{lst:impl.config.example}
\end{listing}

Configuration for a benchmark is done using a YAML file, provided to the bootstrapper.
An example configuration file is shown in \lst~\ref{lst:impl.config.example}.
\First~choose YAML file over other format because of it is readable and easy to edit by hand.
It is also widely adopted and implemented in various programming languages.
\First~also considered to use XML or JSON but find both to be harder to edit by hand.

``YAML Ainâ€™t Markup Language'', recursively abbreviated to YAML, is a data serialization language designed to be human-friendly, portable between programming languages, and easy to use for every tasks \citep{ben2005yaml}.
While it's easy to understand at a glance, it has an arguably complex specification allowing for various use cases.
Avoiding its complexity, \first~choose to use \textsc{StrictYAML}\footnote{\href{https://github.com/crdoconnor/strictyaml}{https://github.com/crdoconnor/strictyaml}}, a relatively simple implementation of a small subset of the YAML specification.
\textsc{StrictYAML} also allows definition of a schema to provide type-safe parsing.
The schema for \OurBenchmarkingTool~configuration file is defined in \emph{reprobench/core/schema.py}.


\subsection{Module}

\begin{listing}
    \inputminted[firstline=31, lastline=34]{python}{assets/listings/reprobench/reprobench/utils.py}
    \caption{Importing class module from path string}
    \label{lst:impl.import}
\end{listing}


To enable high extensibility, many components of \OurBenchmarkingTool~is defined in a modular, reusable Python \emph{modules}.
What \first~define as a module is in fact just a Python class that can be imported by path strings.
For example, the path string \mintinline{python}{"my_package.my_folder.my_file.MyClass"} references the class \code{MyClass} in \emph{my\_folder/my\_file.py} (or \emph{my\_folder/my\_file/\_\_init\_\_.py}) file in the Python package \code{my\_package}.
This allows \OurBenchmarkingTool~to flexibly import user defined code, either it's in the working directory or installed through \code{pip}.

The related utility for importing these modules is shown in \lst~\ref{lst:impl.import}.
Firstly, it uses \mintinline{python}{importlib.import_module(...)} from Python's standard library to import the path as a Python module.
Then \mintinline{python}{getattr(...)} is used to extract the class from the last part of the path string.

\subsection{Server}

\begin{listing}
    \inputminted{python}{assets/listings/pseudocodes/server.py}
    \caption{Pseudo-code of the server component}
    \label{lst:impl.server}
    % \captionof{listing}{Pseudo-code of the server component \label{lst:impl.server}}
\end{listing}

Pseudo-code for the server implementation is given in \lst~\ref{lst:impl.server}.
In short, the server is implemented in three main phases.
The first phase is setting up the database and sockets.
Then the server waits for a \textsc{bootstrap} event from the client, as mentioned in Section \ref{sec:impl.design} earlier.
As soon as the database is bootstrapped, the server query the needed observers and register the respective modules.
Finally, the server spawn greenlets of the observers and its own loop method and waited for them to terminate.

Greenlets is

\subsection{Observers}

\subsection{Bootstrapper}

\subsection{Manager}

\subsection{Workers}

\subsection{Steps}

\subsection{Tools}
\label{sec:impl.tools}

\subsection{Task Sources}

\subsection{Command Line Interface}


\section{Usage Scenario}

\subsection{Installation}

\subsection{Benchmark Setup}

\subsection{Running Benchmark}

\subsection{Analysis}

