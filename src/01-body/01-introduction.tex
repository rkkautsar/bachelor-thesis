\chapter{\chIntroduction}
\label{ch:introduction}


% \section{Motivation}


\citet{gowerScientificMethodHistorical2012} describe the famous thought experiment by Galileo Galilei.
The claim was as follows.
Take two objects, one lighter and the other heavier in weight.
Then tie and drop them together from the top of a tower.
If, like many have believed at that time, heavier object fall faster then the lighter one will pull the heavier one and slow down the fall.
But instead this new tied object is heavier than both and should have fallen faster.
This is a contradiction.
Thus, Galileo postulated that this phenomenon was due to air resistance and friction, independent of the object mass.
This example shows that experiment can also take place in the mind and does not have to be physical.

Experiments have also demonstrated that artificial neural network performance is better than \newstyle{what was predicted in theory} \citep{tichyShouldComputerScientists1998}.
\newcontent{Today, artificial neural networks are used in a number of surprising real-world applications.
Its applications include image recognition, speech recognition, natural language understanding, and even self-driving cars \citep{lecunDeepLearning2015}.}

\newcontent{
The Boolean Satisfiability (SAT) problem has been proven to be $\textbf{NP}$-complete by \citet{cook1971complexity} and \citet{levin1973universal} independently.
This means that SAT solving algorithms have exponential worst case running time.
\citet{nordstromPebbleGamesProof2013} remarks that most modern solvers are based on Davis-Putnam-Logemann-Loveland (DPLL) backtracking procedure from the 1960s with augmented clause learning, also known as conflict-driven clause learning (CDCL).
They further remark that it is not yet fully understood why these solvers can be so efficient in practice \citep{elffersTradeoffsTimeMemory2016}.
This can be seen from the majority solvers submitted to the annual international SAT competition being CDCL-based, and successfully solving problems with millions of literals and clauses \citep{heule2018proceedings}.
}

And last but not least, Isaac Eddington conducted an expensive validation in 1919 to prove Einstein's theory that gravity bends light.
This important experiment involves expedition to Principe Island, West Africa and pushed the limits of photographic emulsion technology \citep{tichyShouldComputerScientists1998}.
\newstyle{Today, 100 years after the validation, this is further supported by another great advancement in astronomy.}
In April 10th 2019, the Event Horizon Telescope (EHT) presented to the public the first ever successful image of a black hole, an object predicted by Einstein's general theory of relativity \citep{akiyama2019first}.

All these examples show that experimentation has contributed to several important breakthrough in various disciplines in science.
Computational science specifically has seen a wide use of benchmarking as one way to conduct the experiment as shown in the above neural network and SAT example.
Benchmarking allow automated execution of many repeated experiments.
This automation is convenient to help tackle the recent concern of reproducibility.

\citet{gundersenStateArtReproducibility2018} and \citet{collbergRepeatabilityComputerSystems2016} both report that only a small number of researches are reproducible.
They linked this concern with the effort it takes to make a research work reproducible.
That is why we deemed it necessary to survey if the existing benchmarking tools may help with this concern.
We also tried to improve the state by contributing to the existing or even create a new benchmarking tool altogether.
This is further motivated after we see that the development of some of these benchmarking tools are often requirement-driven and thus made specific to a community need and not general need.

In this chapter, we list the research questions for this thesis in \Cref{sec:intro.questions}.
Then we give pointers on how we answer these questions in \Cref{sec:intro.method}.
Our contributions to this topic are then listed in \Cref{sec:intro.contributions}.
We mention some study related to this thesis in \Cref{sec:intro.related}.
We outline the content for the rest of this thesis in \Cref{sec:intro.outline}.

\section{Research Questions}
\label{sec:intro.questions}

The research questions for this thesis are:
\begin{enumerate}
	\item How does the existing benchmarking tools compare to each other?
	\item Can they help reduce the barrier to embrace reproducibility?
	\item Should we design a better tool to address this concern for reproducibility?
	\item How does our proposed tool compare to the other tools?
\end{enumerate}

\section{Methodology}
\label{sec:intro.method}

To answer the mentioned research questions, we conduct the following:
\begin{enumerate}
	\item Survey the existing tools and compare them.
	\item Formulate requirements on how a benchmarking tool can help researchers embrace reproducibility.
	\item Check if none of the existing tools fulfills our requirements.
	\item Design and implement a better tool to address the reproducibility issue.
	\item Evaluate and compare the proposed tool to the other tools.
\end{enumerate}

\section{Contributions}
\label{sec:intro.contributions}

The main contributions of this thesis are as follows:
\begin{enumerate}
	\item We survey six existing benchmarking tools, discuss their implementation, and compare them with each other.
	\item We introduced an implementation of a cross-platform resource monitoring tool, namely \textsc{psmon}. The source code can be accessed at \url{https://github.com/rkkautsar/psmon}.
	\item We design and implement a new benchmarking tool to address our requirements, namely \textsc{ReproBench}. The source code can be accessed at \url{https://github.com/rkkautsar/reprobench}.
	\item We evaluate our proposed tool and compare it with the existing tools.
\end{enumerate}


\section{Related Works}
\label{sec:intro.related}

\citet{beyerReliableBenchmarkingRequirements2019} formulate requirements for reliable benchmarking and address it with their implementation, namely \textsc{BenchExec}.
They also prove that none of the existing solution for benchmarking is accurate and reliable by their requirements.
Their work focuses around their implementation of a reliable resource monitoring tool for \textsc{BenchExec}, namely \textsc{runexec}.

We extend their requirements and compare their implementation with other existing solutions.
In this work we instead focus on improving the concern of reproducibility in computational science.
For example, by making benchmarking setup and result easier to share among researchers.
We also focus on lessening the effort to produce a reproducible benchmarking setup.


\section{Outline}
\label{sec:intro.outline}

The following chapters of this thesis is structured as follows:

\begin{enumerate}
	\item In \Cref{ch:experimentation}, we explain the concept of experimentation, the concern of reproducibility, and how benchmarking can tackle this concern. We also formally define definitions and requirements that are used throughout the thesis.
	\item In \Cref{ch:resource}, we discuss the process of resource monitoring, the difficulties, how it relates to the benchmarking process and survey some existing implementations.
	\item In \Cref{ch:existing}, we survey some existing benchmarking tools and compare it to the defined requirements.
	\item In \Cref{ch:implementation}, we propose the design and implementation of our proposed benchmarking tool, \textsc{ReproBench}.
	\item In \Cref{ch:evaluation}, we evaluate the proposed tool and compare it to other tools from \Cref{ch:existing}.
	\item In the final chapter we conclude the thesis and give several possibilities for future works.
\end{enumerate}
