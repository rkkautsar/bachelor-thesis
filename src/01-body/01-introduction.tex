\chapter{\chIntroduction}
\label{ch:introduction}


\section{Overview}

% TODO: talk about the need for benchmarking: measurement, assessing result, comparison etc

% In computational science, it is often preferable to compare new algorithm to previous studies and get various measurements.
% But unfortunately most of the time it is not easy to reproduce those studies. \cite{collbergRepeatabilityComputerSystems2016} has shown that out of 402 computer science and engineering paper backed by code that they have examined, only 32.3\% can be built in an under 30 minutes attempt to resolve dependencies and environments needed to run the code, and this number only raises to 48.3\% when the attempt time is not limited.
% Not to mention that it is only 56,22\% out of those 402 paper whose source code is obtainable, even after requesting directly from the authors.

% TODO: more talk about reproducibility here, vs replicability etc

% Few attempts has been made to this computational reproducibility problem.
% Some notable examples are Sacred Infrastructure \citep{greffSacredInfrastructureComputational2017} on reproducible experiment, Reprozip \citep{chirigatiReproZipComputationalReproducibility2016} attempt on packing provenance, and BenchExec \citep{beyerReliableBenchmarkingRequirements2019} attempt on reliable benchmarks.
% Out of those, only BenchExec tackle the problem of benchmarking, but even so it is too domain-specific on software verification and doesn't support running long-running benchmarks such as those with millions of algorithms/parameters/instance combinations in high performance computing (HPC) clusters.

% A new benchmarking tool that is capable of limiting and measuring resource usage, parallel runs, running on HPC clusters, (partially) re-run the benchmarks with new algorithm version, and producing reproducible result that can be shared with others will surely be a huge contribution to computational science in general.
% Authors can benchmark their algorithms with various parameters and compare them to previous algorithms.
% Reviewers can then check the benchmark results claimed, and other researchers can compare their algorithms by extending from this benchmark results in an objective manner.

\section{Contributions}

\section{Research Scope}


\section{Outline}
This thesis is structured like so:
\begin{itemize}
	\item Chapter \ref{ch:introduction} \chIntroduction \\
	\item Chapter \ref{ch:experimentation} \chExperimentation \\
	\item Chapter \ref{ch:resource} \chResource \\
	\item Chapter \ref{ch:existing} \chExisting \\
	\item Chapter \ref{ch:implementation} \chImplementation \\
	\item Chapter \ref{ch:evaluation} \chEvaluation \\
	\item Chapter \ref{ch:conclusion} \chConclusion \\
\end{itemize}
