%-----------------------------------------------------------------------------%
\chapter{\babSatu}
%-----------------------------------------------------------------------------%


%-----------------------------------------------------------------------------%
\section{Overview}
%-----------------------------------------------------------------------------%

In computational science, it is often preferable to compare new algorithm to previous studies and get various measurements. But unfortunately most of the time it is not easy to reproduce those studies. \cite{collbergRepeatabilityComputerSystems2016} has shown that out of 402 computer science and engineering paper backed by code that they have examined, only 32.3\% can be built in an under 30 minutes attempt to resolve dependencies and environments needed to run the code, and this number only raises to 48.3\% when the attempt time is not limited. Not to mention that it is only 56,22\% out of those 402 paper whose source code is obtainable, even after requesting directly from the authors.

Few attempts has been made to this computational reproducibility problem. Some notable examples are Sacred Infrastructure \citep{greffSacredInfrastructureComputational2017} on reproducible experiment, Reprozip \citep{chirigatiReproZipComputationalReproducibility2016} attempt on packing provenance, and BenchExec \citep{beyerReliableBenchmarkingRequirements2019} attempt on reliable benchmarks. Out of those, only BenchExec tackle the problem of benchmarking, but even so it is too domain-specific on software verification and doesn't support running long-running benchmarks such as those with millions of algorithms/parameters/instance combinations in high performance computing (HPC) clusters.

A new benchmarking tool that is capable of  limiting and measuring resource usage, parallel runs, running on HPC clusters, (partially) re-run the benchmarks with new algorithm version, and producing reproducible result that can be shared with others will surely be a huge contribution to computational science in general. Authors can benchmark their algorithms with various parameters and compare them to previous algorithms. Reviewers can then check the benchmark results claimed, and other researchers can compare their algorithms by extending from this benchmark results in an objective manner.

%-----------------------------------------------------------------------------%
\section{Contributions}
%-----------------------------------------------------------------------------%

%-----------------------------------------------------------------------------%
\section{Research Scope}
%-----------------------------------------------------------------------------%


%-----------------------------------------------------------------------------%
\section{Outline}
%-----------------------------------------------------------------------------%
Sistematika penulisan laporan adalah sebagai berikut:
\begin{itemize}
	\item Bab 1 \babSatu \\
	\item Bab 2 \babDua \\
	\item Bab 3 \babTiga \\
	\item Bab 4 \babEmpat \\
	\item Bab 5 \babLima \\
	\item Bab 6 \babEnam \\
	\item Bab 7 \kesimpulan \\
\end{itemize}
