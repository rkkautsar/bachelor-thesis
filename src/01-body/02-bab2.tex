\chapter{\babDua}

This chapter [...]. \hyperref[sec:experimentation]{Section 2.1} discuss [...]

\section{Experimentation}
\label{sec:experimentation}

Science aims to understand why things happen as they do in the natural world \citep{careyBeginnerGuideScientific2012}
Investigating these why questions can be achieved by following what is known as the scientific method, or what Karl Popper has defined as the hypothetico-deductive method, a combination of inductive and deductive reasoning \citep{wallimanResearchMethodsBasics2010a}.
He further defines the scientific method as follows:
\begin{itemize}[noitemsep]
	\item identification or clarification of a problem;
	\item developing a hypothesis (testable theory) inductively from observations;
	\item charting their implications by deduction;
	\item practical or theoretical testing of the hypothesis;
	\item rejecting or refining it in the light of the results.
\end{itemize}

\citet{careyBeginnerGuideScientific2012} provides a simpler definition for the scientific method as three simple steps: observing, explaining, and testing.
These definitions suggest that science advances by the means of trial and error.
The explaining-testing steps are repeated until the result is acceptable.
When a theory is falsified, another one is proposed and tested, until the most fitting theory is accepted \citep{wallimanResearchMethodsBasics2010a}.

Experimentation is one way to test these proposed theories or hypotheses.
Some would argue that as experiments don't proof anything.
But on the other hand, experiments can falsify a theory and corroborate it, but not actually proofing that the theory is true \citep{tichyShouldComputerScientists1998}.
Thus, experiments can be seen as just another way of proofing theories by \textit{reductio ad absurdum}, like what proof by contradiction is commonly used in logic and mathematics.

Experimentation can also be used for induction, deriving generalized conclusion from repeated observations.
A rather well-known example in computer science is the case of artificial neural networks.
Experiments demonstrated that its performance is better than what was predicted after being discarded in theoretical grounds \citep{tichyShouldComputerScientists1998}.

Another example is Boolean Satisfiability (SAT).
It has been proved to be $\mathcal{NP}$-complete by the Cook-Levin theorem.
Many would give up after seeing that the problem they are trying to solve is in $\mathcal{NP}$.
But this is not the case.
State-of-the-art SAT solvers are good enough in solving real world problems with millions of literals and clauses.
It is widely used in many fields despite the fact that the problem itself is seen as bad in theory.

Not only in computer science, experimentation has been used in practice in many fields.
In particular fields like physics and chemistry that involves observing and explaining physical objects often use this method.

An early example would be the famous thought experiment by Galileo Galilei.
The thought experiment was about tying two bodies of object, one lighter and one heavier and dropping if from top of a tower.
If, like many believed in that time, heavier object will fall faster as such the case with a heavy metal ball and a feather, the lighter one will pull the heavier one and slow down the fall.
But these tied objects makes an object of heavier than either and thus should fall faster, this makes it a contradiction \citep{goodmanWhatDoesResearch2016}. This example also points out that external factors which are not taken into account can effect the result of an experiment in a bad way, such as air resistance in this case.

There's also a rather extreme case of measuring the weight of a human soul, conducted by Duncan MacDougall, MD. He suggest that human soul has a measurable mass. To proof this hypothesis, he use six terminal patients and weighed them before, during, and after the process of death \citep{ryanModernExperimentalDesign2007a}.

And last but not least, there's the expensive validation by Isaac Eddington in 1919 to prove Einstein's theory that gravity bends light.
This important experiment involves expedition to Principe Island, West Africa and pushed the limits of photographic emulsion technology \citep{tichyShouldComputerScientists1998}.
And now, 100 years after the validation, it was further supported by another great advancement in astronomy.
In April 10th 2019, the Event Horizon Telescope (EHT) presented to the public the first ever successful image of a black hole, an object predicted by Einstein's general theory of relativity.

Besides for the objectives of validation and induction already mentioned above, there's a lot of reason to run an experiment.
\citet{montgomeryDesignAnalysisExperiments2013} mentioned some of the reasons as follows:
\begin {itemize}
	\item \textbf{Factor screening or characterization}. This is often the case when working with new systems or technologies to minimize wasted resource usage.
	\item \textbf{Optimization}. Finding the desirable settings of factors that result in desirable response.
	\item \textbf{Confirmation}. This is more or less the same as the validation objective already mentioned.
	\item \textbf{Discovery}. This is more or less the same as the induction objective already mentioned.
	\item \textbf{Robustness}. Finding the condition on which the response variables will degrade.
\end{itemize}

\citet{montgomeryDesignAnalysisExperiments2013} also defines a guideline in designing an experiment that maps nicely to the scientific method:

\begin{enumerate}[noitemsep]
	\item Recognition of and statement of the problem
	\item Selection of the response variable
	\item Choice of factors, levels, and ranges
	\item Choice of experimental design
	\item Performing the experiment
	\item Statistical analysis of the data
	\item Conclusions and recommendations
\end{enumerate}

Step 1-3 can be considered as the pre-experimental planning steps.
He also noted that steps 2 and 3 is interchangeable in practice.
Furthermore, the experiment should also be done in an iterative way.

\section{Reproducible Benchmarks}

The term reproducibility is often intertwined with the term replicability.
\citet{drummondReplicabilityNotReproducibility2009} argues that replicability is the impoverished version of reproducibility which albeit the great effort to achieve it, its purpose is only for preventing fraud.

\citet{goodmanWhatDoesResearch2016} suggests to define new terminologies for reproducibility to avoid the inconsistencies between replicability and reproducibility:
\begin{itemize}
	\item \textbf{Methods reproducibility}. Ability to implement, as exactly as possible, the experimental and computational procedures, with the same data and tools, to obtain the same results.

	\item \textbf{Results reproducibility}. The production of corroborating results in a new study, having followed the same experimental methods.

	\item \textbf{Inferential reproducibility}. The making of knowledge claims of similar strength from a study replication or reanalysis.
\end{itemize}

\citet{gundersenStateArtReproducibility2018} remarks that methods reproducibility is what Drummond states by replicability, and the combination of both results and inferential reproducibility is what Drummond states by reproducibility.
They also suggest three degrees of reproducibility, specific to the Artificial Intelligence (AI) field.
While these are defined as specific in the AI field, it can be applied to computational science in general with a slight modification:
\begin{itemize}
	\item \textbf{R1: Experiment Reproducible}. The results of an experiment are experiment reproducible when the execution of the same implementation produces the same results when executed on the same data.
	\item \textbf{R2: Data Reproducible}. The results of an experiment are data reproducible when an experiment is conducted that executes an alternative implementation that produces the same results when executed on the same data.
	\item \textbf{R3: Method Reproducible}. The results of an experiment are method reproducible when the execution of an alternative implementation produces the same results when executed on different data.
\end{itemize}

In short, R1 is the highest degree of reproducibility.
R3 only requires documented method of the experiment, while R2 additionally requires documented data used in the experiment, and finally R1 also requires documented experiment implementation.

They also reported that no papers are fully reproducible by any degree defined above.
The degree of reproducibility of each paper (measured by the number of required variables documented) is only in the 24-26\% range.
\citet{collbergRepeatabilityComputerSystems2016} results further support this issue.
The degree of reproduciblity in computer science is only up to 54\% even after considerable effort in contacting the authors.
They mention some of the reasons as licensing issues, no version tracking, the code is not ready for public, no backup, obsolete dependencies, and so on.

\citet{beyerReliableBenchmarkingRequirements2019} defines benchmarking as a performance evaluation method that is used for comparing different tools of the same domain, evaluating and comparing different features or configurations of a tool, or finding out the performance of a tool under different inputs.
This suggests that benchmarking is a good option for experiments where the design chosen is comparative or variance analysis.

Simplified process of benchmarking maps closely to designing an experiment, except in computational science the perform and collection phase can (and should) be automated:
\begin{enumerate}[noitemsep]
	\item Planning
	\item Performing benchmarks \& collecting results
	\item Analysis
	\item Conclusion (i.e. presentation)
\end{enumerate}

