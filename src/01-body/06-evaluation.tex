\chapter{\chEvaluation}
\label{ch:evaluation}


\section{Example Benchmarking Scenario}

\subsection{Installation}

As noted in Chapter \ref{ch:implementation}, \OurBenchmarkingTool~is distributed through the Python standard pypi.
There are various specific extra dependencies that also need to be installed depending on what the \OurBenchmarkingTool~will be doing in the system.
This is taken care using the `extras' dependencies specification of the package.

\begin{listing}
    \begin{minted}{bash}
client $ pip install reprobench[client,sysinfo,psmon]
server $ pip install reprobench[server,pcs]
personal $ pip install reprobench[analytics]
    \end{minted}
    \caption{Installing \OurBenchmarkingTool~in various environment}
    \label{lst:eval.install}
\end{listing}

For example, one might want to set up to three different environment.
These are the client-side for running the computation, server-side for collecting the data, and their own personal machine for analysis.
\lst~\ref{lst:eval.install} shows the installation done for this scenario.


\subsection{Benchmark Setup}




\subsection{Running Benchmark}

\subsection{Analysis}

\begin{figure}
	\centering
	\ifdraft{
		\dummyfig{assets/plots/cactus.pgf}
	}{
		\resizebox{\textwidth}{!}{\input{assets/plots/cactus.pgf}}
	}
	\caption{Example generated cactus plot on analysis step}
\end{figure}



\section{Stress Testing}

657 out of 5304 tasks submitted concurrently failed.
The cause is the High Water Mark settings in \O MQ.


\section{Messaging Implications}

A typical benchmark run with executor, system information collection, and output validation sends these messages:
1 x worker join
1 x run start
3 x run step
1 x sys info
1 x run statistic
1 x validation
1 x run finish
1 x worker leave

Most of the events are just notifications with little or no payload.
Validation also has small payload (< 20 bytes).
Run step averages on about 60 bytes.
Sys info averages on about 360 bytes.
Run statistics averages on about 100 bytes.
So a typical run will only consume below 1 KB each.

With bandwidth that small, the TCP itself becomes the one contributing most to the latency.

\section{Comparison to Other Benchmarking Tools}
\label{sec:prior_works.method}

The requirements from Section \ref{sec:idealBenchmarkingTool} are broken down to a few key factors.
These key factors are used to measure how much of the requirements are fulfilled.

\newcounter{reqCount}
\newcounter{reqFactorCount}[reqCount]
\newcommand{\reqLabel}[1]{
	\setcounter{reqFactorCount}{0}
	\addtocounter{reqCount}{1}
	\arabic{reqCount}.
	#1
}
\newcommand{\reqFactor}[1]{
	\addtocounter{reqFactorCount}{1}
	(\alph{reqFactorCount}) #1
}

\begin{ThreePartTable}
	\begin{TableNotes}
		\footnotesize
		\item[$\alpha$] Subjective evaluation
	\end{TableNotes}
	\begin{longtable}{ll}

		\textbf{Requirements} & \textbf{Factors} \\
		\toprule
		\endhead

		% \cmidrule{2-2}
		\multicolumn{2}{r}{\textit{continued}}
		\endfoot

		\bottomrule
		\insertTableNotes\\
		\caption{Metrics for evaluating existing benchmarking tools}
		\endlastfoot

		\multirow{5}{*}{\reqLabel{Extensibility}}
			& \reqFactor{Flexible evaluation step} \\*
			& \reqFactor{Flexible analysis step} \\*
			& \reqFactor{Flexible benchmark instance source} \\*
			& \reqFactor{Does not enforce implementation type} \\*
			& \reqFactor{Can support arbitrary task scheduler} \\*
		\midrule

		\multirow{5}{*}{\reqLabel{Configurability}}
			& \reqFactor{Multiple runs} \\*
			& \reqFactor{Multiple tool configurations} \\*
			& \reqFactor{Support parameter space} \\*
			& \reqFactor{Benchmark instance selection} \\*
			& \reqFactor{Set resource limit} \\*
		\midrule

		\multirow{5}{*}{\reqLabel{Documentation}}
			& \reqFactor{Self-documenting configuration} \\*
			& \reqFactor{Installation guide} \\*
			& \reqFactor{Configuration guide} \\*
			& \reqFactor{Main workflow guide} \\*
			& \reqFactor{Comprehensive documentation\tnote{$\alpha$}} \\*
		\midrule

		\multirow{4}{*}{\reqLabel{Setup Effort}}
			& \reqFactor{No superuser privilege} \\*
			& \reqFactor{Installation guide} \\*
			& \reqFactor{Documented requirements} \\*
			& \reqFactor{No cumbersome dependencies\tnote{$\alpha$}} \\*
		\midrule

		\multirow{6}{*}{\reqLabel{Accuracy \& Reliability}}
			& \reqFactor{Measure and Limit Resources Accurately} \\*
			& \reqFactor{Terminate Processes Reliably} \\*
			& \reqFactor{Assign Cores Deliberately} \\*
			& \reqFactor{Respect Nonuniform Memory Access} \\*
			& \reqFactor{Avoid Swapping} \\*
			& \reqFactor{Isolate Individual Runs} \\*
		\midrule

		\multirow{5}{*}{\reqLabel{Reproducibility}}
			& \reqFactor{Stored system information} \\*
			& \reqFactor{Sharable results} \\*
			& \reqFactor{Sharable configuration} \\*
			& \reqFactor{Encourage sharable data\tnote{$\alpha$}} \\*
			& \reqFactor{Encourage sharable implementation\tnote{$\alpha$}} \\*
	\end{longtable}
\end{ThreePartTable}

Each of the evaluated tools are scored according to the degree of fulfillment for each requirements.
That is, if $M_i$ is the scored degree of fulfillment of the $i$-th requirement, and $\mu_{i}$ is a vector of size $n$ such that
\[
	\mu_{ij} =
	\begin{cases}
		1 & \text{if the $j$-th key factor of $i$-th requirement is fulfilled}\\
		0 & \text{otherwise}
	\end{cases}
\]
then the degree of fulfillment is the average of $\mu_i$:
\[
	M_i = \frac{\sum\mu_{i}}{|\mu_i|}
\]

Furthermore, as an overall measure, \first~also denote $M_\sigma$ as the overall overage of all $\mu_{i}$ regardless of its requirement category.


\begin{ThreePartTable}
	\begin{TableNotes}
		\footnotesize
		\item[*] Values are emptied if it's not applicable. The complete evaluation is given in the appendix.
	\end{TableNotes}
	\begin{longtable}{lddddddd}
		& \multicolumn{1}{c}{$M_1$} & \multicolumn{1}{c}{$M_2$} & \multicolumn{1}{c}{$M_3$} & \multicolumn{1}{c}{$M_4$} & \multicolumn{1}{c}{$M_5$} & \multicolumn{1}{c}{$M_6$} & \multicolumn{1}{c}{$M_\sigma$}\\
		\midrule
		\textsc{benchmark-tool} & .60 & .80 & .20 & .50 & .17 & .40 & .43 \\
		\textsc{BenchExec} & .40 & .60 & .60 & .50 & 1.00 & .80 & .66 \\
		\textsc{Benchkit} & .40 & .40 & .60 & .25 & .50 & .40 & .43 \\
		\textsc{JuBE} & .80 & 1.00 & 1.00 & 1.00 & & .40 & .83 \\
		% \textsc{Optil.io} & .40 & .80 & .60 & 1.00 & & .40 & .62 \\
		\textsc{StarExec} & .60 & .60 & .80 & 1.00 & 1.00 & 1.00 & .83 \\
		\bottomrule
		\insertTableNotes\\
		\caption{Requirements score for various existing benchmarking tools}
		\label{tab:reqscoresummary}
	\end{longtable}
\end{ThreePartTable}

Table \ref{tab:reqscoresummary} shows that none of the considered benchmarking tools fulfilled the requirements given.
The closest is \textsc{StarExec} and \textsc{JuBE}.
\textsc{StarExec} can even---arguably---achieves $\bm{R_1}$ by default.
But most of the features available in \textsc{StarExec}, such as managing post-processors and plots, are restricted to the community leaders.
The normal user is forced to use what was already available in the community space.
On the other hand, \textsc{JuBE} is too generic and provides little support for reproducibility.
