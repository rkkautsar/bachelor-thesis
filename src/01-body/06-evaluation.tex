\chapter{\chEvaluation}
\label{ch:evaluation}

This chapter presents several evaluation regarding the implementation of \OurBenchmarkingTool~and its position relative to other benchmarking tools mentioned in \Cref{ch:existing}.
First, an example usage scenario is given in \Cref{sec:eval.scenario} to give an overview of how \OurBenchmarkingTool~can be used from setting up, running the benchmark, until the analysis step.
\Cref{sec:eval.messaging} gives a detailed look to the performance and implications of the implemented messaging mechanism.
A measurement of the resource usage of the implementation is then given in \Cref{sec:eval.resource}.
Last but not least, comparisons to other mentioned benchmarking tools is discussed in \Cref{sec:eval.comparison} based on a breakdown of the requirements from \Cref{sec:idealBenchmarkingTool}.

\section{Example Usage Scenario}
\label{sec:eval.scenario}

This section describe an example benchmarking workflow using \OurBenchmarkingTool.
Each step needed from setting up to analyzing the result is explained.

\subsection{Benchmark Setup}

The configuration for this benchmarking scenario is publicly available as a GitHub repository: \href{https://github.com/rkkautsar/benchmark-demo-sat}{\code{https://github.com/rkkautsar/benchmark-demo-sat}}.
The configuration is the same as in \lst~\ref{lst:impl.config.example}.
This benchmark is \(\bm{R_1}\) reproducible in UNIX machines.
The tools and tasks are downloaded from the internet and set up programmatically.

For this scenario, the benchmarking is done in three environment.
Computation is done in a \textsc{slurm} cluster system of TU Dresden, Taurus.
Taurus sports 2\,179 nodes with a total of 41\,468 cores.
A typical node used for the computation has an Intel(R) Xeon(R) CPU E5-2690 @ 2.90GHz.
Data collection is done in a VPS physically located in Paris, France.
This is a small virtual instance sporting an Intel(R) Atom(TM) CPU C3955 @ 2.10GHz and 1 GB of RAM.
For future reference, \first~refer to the IP bound for this VPS as \code{168.8.8.8}.
Finally, the analysis is done in a personal machine with a Intel Core i5 CPU @ 1.8 GHz.
This is not necessary but in this scenario it is needed to execute an analysis in a Jupyter notebook.

\subsection{Installation}

As noted in Chapter \ref{ch:implementation}, \OurBenchmarkingTool~is distributed through the Python's standard pypi.
There are various specific extra dependencies that also need to be installed depending on what the \OurBenchmarkingTool~will be doing in the system.
This is taken care using the `extras' dependencies specification of the package.

\begin{listing}
	\begin{minted}{bash}
client $ pip install reprobench[client,sysinfo,psmon]
server $ pip install reprobench[server,pcs]
personal $ pip install reprobench[analytics]
    \end{minted}
	\caption{Installing \OurBenchmarkingTool~in various environment}
	\label{lst:eval.install}
\end{listing}

For example, one might want to set up to three different environment.
These are the client-side for running the computation, server-side for collecting the data, and their own personal machine for analysis.
\Cref{lst:eval.install} shows how the installation can be done for each environment.

Additionally, Python's \code{virtualenv} is used in the server-side to contain the dependencies.
In client-side, a Miniconda\footlink{https://docs.conda.io/en/latest/miniconda.html} environment is used instead.
Finally, a Jupyter\footlink{https://jupyter.org/} and IPython kernel is installed for running the analysis.
Also, the repository is cloned to each environment.

\subsection{Running the Benchmark}

\begin{listing}
	\begin{minted}{text}
server $ reprobench server -a tcp://0.0.0.0:31313
client $ reprobench bootstrap -a tcp://168.8.8.8:31313
client $ reprobench manager slurm run -a tcp://168.8.8.8:31313
server $ reprobench status
 74%[#######################        ] 3904/5304 [20:18<21:00,  1.11it/s]
    \end{minted}
	\caption{Running the benchmark}
	\label{lst:eval.running}
\end{listing}

\Cref{lst:eval.running} shows the series of commands executed for running the benchmark.
First, the server process is started and bind a TCP socket at \code{0.0.0.0:31313}.
Note that exposing a port to public like this is not safe per se.
This issue is discussed later in \Cref{ch:conclusion}.

Then the bootstrapping process is started from the client.
Tools and tasks are downloaded first in the client-side before it compiles the information and send it to the server to do its own bootstrapping process.
This whole process may take a while.
In this scenario and the environment listed earlier, it takes about a minute to fully download the tools, tasks, and ready the database.

Last but not least, the \textsc{slurm} manager spawn the workers to do the computation.
This submits a 5\,304-sized job array to the job queue, as much as the number of pending runs, which also happen to be the total number of runs in the beginning.
While the worker interacts with the server and do the computation, a \code{reprobench status} command can be used to monitor the progress in real time.
Besides giving reports of the progress, this command also gives an estimate of the remaining time to completion by analyzing the speed of completion.

\subsection{Analysis}

After the benchmarking is completed, the resulting database from the server is obtained for example by using the \code{scp} command such as \mintinline{bash}{scp server:/tmp/benchmark/output/benchmark.db ./output/benchmark.db}.
This database contains all the configuration for this benchmark and can also be shared for others to analyze, validate or even replicate.
The analysis is then executed by running the \code{reprobench analyze} command with the output directory as an argument.

\begin{figure}
	\centering
	\dummyfig{assets/plots/cactus.pgf}
	% \ifdraft{
	% 	\dummyfig{assets/plots/cactus.pgf}
	% }{
	% 	\resizebox{\textwidth}{!}{\input{assets/plots/cactus.pgf}}
	% }
	\caption{An example cactus plot on analysis step}
	\label{fig:eval.cactus}
\end{figure}

The analysis step will then be executed sequentially.
\Cref{fig:eval.cactus} shows an example of a cactus plot generated in this phase.
This plot is generated by executing a generated Jupyter notebook.
\citet{piccoloToolsTechniquesComputational2016} mentions Jupyter notebooks as a notable example of literate programming tools supporting computational reproducibility.
This format allows one to easily read the code as well as its results in one easy presentation.
Using it as an analysis step allows flexible configuration to iterate the result.

\section{Messaging Performance}
\label{sec:eval.messaging}

A typical benchmark run with executor, system information collection, and output validation sends these 10 events:
1 x worker join;
1 x run start;
3 x run step;
1 x system info;
1 x run statistic;
1 x validation;
1 x run finish; and
1 x worker leave.
Among them, the worker join event is the only one that waits for a reply.

Most of the events are just notifications with little or no payload.
Validation also has small payload ($< 20$ bytes).
Run step averages on about 60 bytes.
System information averages on about 360 bytes.
Run statistic averages on about 100 bytes.
This means a typical run will only consume below 1 KB each.

An experiment is conducted to check the throughput of the client-server communication across server.
As in \Cref{sec:eval.scenario}, the client is located in TU Dresden HPC system while the server is located in a VPS in Paris.
\First~measure \(L_1\), and \(L_2\), respectively the latency between receiving message, and sending then receiving message.
Each event consists of a small 32 byte payload.
The result as an average of 1\,000 measurement is 325,29 ms, and 2\,878,54 ms respectively.
The throughput \(T_1\), and \(T_2\) can then be computed by inverting the latency, resulting in 307,86, and 34,84 messages per second respectively.
As a comparison, raw throughput between the machine as measured with \code{netcat} is 23 MB/s or equivalent to about 576 messages per second.

Furthermore, \first~also stress tested the network.
Since the server only run in one process it might not handle high throughput messaging.
Note that the \O MQ messaging framework also has a limit to its queue, defaults to 1\,000 messages.
Fortunately, the \textsc{dealer-router} pattern used in \textsc{client-server} connection implements a blocking behavior in the case of a full queue.
To validate this, two experiments are conducted.
First, a 100 parallel process is spawned, each sending up to 2\,000 events (totalling to 200\,000 events).
Second, a variable number of process up to 500 parallel process, each sending 25 events (totalling to 12\,500 events).
Each event is replied by the server, like the worker join event.

In both cases, there is no events dropped.
In every trial, all of the events are received by the server.
The highly parallel experiment also has no apparent time increase.
This means there is no blocking since the server works fast enough to forward the event to the \textsc{pub}-\textsc{sub} channel to clear the queue.
But it need to be noted that by using greenlets, this means it only works as fast as the event is processed.


\section{Resource Usage}
\label{sec:eval.resource}

\begin{figure}
	\centering

	\begin{subfigure}{.45\textwidth}
		\centering
		\ifdraft{
			\dummyfig{assets/plots/perf\_cpu.pgf}
		}{
			\resizebox{\textwidth}{!}{\input{assets/plots/perf_cpu.pgf}}
		}
		\caption{CPU Time}
		\label{fig:eval.perf.cpu}
	\end{subfigure}%
	\hfill%
	\begin{subfigure}{.45\textwidth}
		\centering
		\ifdraft{
			\dummyfig{assets/plots/perf\_rss.pgf}
		}{
			\resizebox{\textwidth}{!}{\input{assets/plots/perf_rss.pgf}}
		}
		\caption{Resident Memory}
		\label{fig:eval.perf.rss}
	\end{subfigure}

	\vspace{1cm}

	\begin{subfigure}{.45\textwidth}
		\centering
		\ifdraft{
			\dummyfig{assets/plots/perf\_io\_rw.pgf}
		}{
			\resizebox{\textwidth}{!}{\input{assets/plots/perf_io_rw.pgf}}
		}
		\caption{Disk I/O}
		\label{fig:eval.perf.disk}
	\end{subfigure}%
	\hfill%
	\begin{subfigure}{.45\textwidth}
		\centering
		\ifdraft{
			\dummyfig{assets/plots/perf\_io\_recvsent.pgf}
		}{
			\resizebox{\textwidth}{!}{\input{assets/plots/perf_io_recvsent.pgf}}
		}
		\caption{Network I/O}
		\label{fig:eval.perf.network}
	\end{subfigure}

	\caption{Server resource usage for a sample benchmark}
	\label{fig:eval.perf}
\end{figure}

\textsc{resource\_monitor}\footlink{http://ccl.cse.nd.edu/software/resource\_monitor} is used to monitor the resource usage over time for the server component as shown in \Cref{fig:eval.perf}.
It is another resource monitoring tools mentioned by \citet{juvePracticalResourceMonitoring2015} besides \textsc{kickstart}.
The same VPS instance as in \Cref{sec:eval.scenario,sec:eval.messaging} is used for this experiment.

\Cref{fig:eval.perf.cpu} shows that the CPU Time usage is just below 13 seconds for a 90 seconds session.
This means the server only uses about 15\% of CPU.
\Cref{fig:eval.perf.rss} shows the resident memory usage peaked to only about 62 MB.
Note that resident memory usage means the actual physical memory used by the process.
The actual requested memory peaked to about 366 MB.
\Cref{fig:eval.perf.disk} shows the cumulative amount of data read (resp. written) from (resp. to) the disk.
It shows the server writing to the database steadily while the read only happened in the beginning.
Finally, \Cref{fig:eval.perf.network} shows the cumulative amount of data sent (resp. received) through the network.
It shows a similar trend although the data sent is a little higher than the data received.
This is because the server is sending information regarding the run to the workers while the worker only reports a small amount of result.

As the worker performance is very dependent to the steps it need to execute, we only present the performance measurement of the server.
However, a quick experiment with a single executor step with \code{sleep 10} as the tool results in sub-second CPU Time, and only 1\,724 KB of resident memory.


\section{Comparison to Other Benchmarking Tools}
\label{sec:eval.comparison}

The requirements from \Cref{sec:idealBenchmarkingTool} are broken down to a few key factors in \Cref{tab:reqfactors}.
These key factors are used to measure how much of the requirements are fulfilled.

\newcounter{reqCount}
\newcounter{reqFactorCount}[reqCount]
\newcommand{\reqLabel}[1]{
	\setcounter{reqFactorCount}{0}
	\addtocounter{reqCount}{1}
	\arabic{reqCount}.
	#1
}
\newcommand{\reqFactor}[1]{
	\addtocounter{reqFactorCount}{1}
	(\alph{reqFactorCount}) #1
}

\begin{table}
	\begin{threeparttable}
		\begin{tabular}{ll}

			\textbf{Requirements} & \textbf{Factors}                                              \\

			\toprule
			% \endhead

			% % \cmidrule{2-2}
			% \multicolumn{2}{r}{\textit{continued}}
			% \endfoot

			% \bottomrule
			% \insertTableNotes\\

			% \endlastfoot

			\multirow{5}{*}{\reqLabel{Extensibility}}
			                      & \reqFactor{Flexible evaluation step}                          \\*
			                      & \reqFactor{Flexible analysis step}                            \\*
			                      & \reqFactor{Flexible benchmark instance source}                \\*
			                      & \reqFactor{Does not enforce implementation type}              \\*
			                      & \reqFactor{Can support arbitrary task scheduler}              \\*
			\midrule

			\multirow{5}{*}{\reqLabel{Configurability}}
			                      & \reqFactor{Multiple runs}                                     \\*
			                      & \reqFactor{Multiple tool configurations}                      \\*
			                      & \reqFactor{Support parameter space}                           \\*
			                      & \reqFactor{Benchmark instance selection}                      \\*
			                      & \reqFactor{Set resource limit}                                \\*
			\midrule

			\multirow{5}{*}{\reqLabel{Documentation}}
			                      & \reqFactor{Self-documenting configuration}                    \\*
			                      & \reqFactor{Installation guide}                                \\*
			                      & \reqFactor{Configuration guide}                               \\*
			                      & \reqFactor{Main workflow guide}                               \\*
			                      & \reqFactor{Comprehensive documentation\tnote{$\alpha$}}       \\*
			\midrule

			\multirow{4}{*}{\reqLabel{Setup Effort}}
			                      & \reqFactor{No superuser privilege}                            \\*
			                      & \reqFactor{Installation guide}                                \\*
			                      & \reqFactor{Documented requirements}                           \\*
			                      & \reqFactor{No cumbersome dependencies\tnote{$\alpha$}}        \\*
			\midrule

			\multirow{6}{*}{\reqLabel{Accuracy \& Reliability}}
			                      & \reqFactor{Measure and Limit Resources Accurately}            \\*
			                      & \reqFactor{Terminate Processes Reliably}                      \\*
			                      & \reqFactor{Assign Cores Deliberately}                         \\*
			                      & \reqFactor{Respect Nonuniform Memory Access}                  \\*
			                      & \reqFactor{Avoid Swapping}                                    \\*
			                      & \reqFactor{Isolate Individual Runs}                           \\*
			\midrule

			\multirow{5}{*}{\reqLabel{Reproducibility}}
			                      & \reqFactor{Stored system information}                         \\*
			                      & \reqFactor{Sharable results}                                  \\*
			                      & \reqFactor{Sharable configuration}                            \\*
			                      & \reqFactor{Encourage sharable data\tnote{$\alpha$}}           \\*
			                      & \reqFactor{Encourage sharable implementation\tnote{$\alpha$}} \\*
		\end{tabular}

		\begin{tablenotes}
			\footnotesize
			\item[$\alpha$] Subjective evaluation
		\end{tablenotes}

		\caption{Metrics for evaluating existing benchmarking tools}
		\label{tab:reqfactors}
	\end{threeparttable}
\end{table}

Each of the evaluated tools are scored according to the degree of fulfillment for each requirements.
That is, if $M_i$ is the scored degree of fulfillment of the $i$-th requirement, and $\mu_{i}$ is a vector of size $n$ such that
\[
	\mu_{ij} =
	\begin{cases}
		1 & \text{if the $j$-th key factor of $i$-th requirement is fulfilled} \\
		0 & \text{otherwise}
	\end{cases}
\]
then the degree of fulfillment is the average of $\mu_i$:
\[
	M_i = \frac{\sum\mu_{i}}{|\mu_i|}
\]

Furthermore, as an overall measure, \first~also denote $M_\sigma$ as the overall overage of all $\mu_{i}$ regardless of its requirement category.


\begin{table}
	\begin{threeparttable}
		\begin{adjustbox}{max width=\textwidth}
			\begin{tabular}{lddddddd}
				                              & \multicolumn{1}{c}{$M_1$} & \multicolumn{1}{c}{$M_2$} & \multicolumn{1}{c}{$M_3$} & \multicolumn{1}{c}{$M_4$} & \multicolumn{1}{c}{$M_5$} & \multicolumn{1}{c}{$M_6$} & \multicolumn{1}{c}{$M_\sigma$} \\
				\midrule
				\textsc{benchmark-tool}       & .60                       & .80                       & .20                       & .50                       & .17                       & .40                       & .43                            \\
				\textsc{BenchExec}            & .40                       & .60                       & .60                       & .50                       & 1.00                      & .80                       & .66                            \\
				\textsc{Benchkit}             & .40                       & .40                       & .60                       & .25                       & .50                       & .40                       & .43                            \\
				\textsc{JuBE}                 & .80                       & 1.00                      & 1.00                      & 1.00                      &                           & .40                       & .83                            \\
				\textsc{StarExec}             & .60                       & .60                       & .80                       & 1.00                      & 1.00                      & 1.00                      & .83                            \\
				\textbf{\OurBenchmarkingTool} & 1.00                      & 1.00                      & .80                       & 1.00                      &                           & 1.00                      & .95                            \\
				% \bottomrule
			\end{tabular}
		\end{adjustbox}
		\begin{tablenotes}
			\footnotesize
			\item[*] Values are emptied if it's not applicable. The complete evaluation is given in the appendix.
		\end{tablenotes}
		\caption{Requirements score for various existing benchmarking tools}
		\label{tab:reqscoresummary}
	\end{threeparttable}
\end{table}

Table \ref{tab:reqscoresummary} shows that none of the considered benchmarking tools fulfilled the requirements given.
The closest is \textsc{StarExec} and \textsc{JuBE}.
\textsc{StarExec} can even---arguably---achieves $\bm{R_1}$ by default.
But most of the features available in \textsc{StarExec}, such as managing post-processors and plots, are restricted to the community leaders.
The normal user is forced to use what was already available in the community space.
On the other hand, \textsc{JuBE} is too generic and provides little support for reproducibility.

\OurBenchmarkingTool~fulfills most of the requirements, with exception of $M_3$ and $M_5$, respectively documentation and accuracy and reliability.
Currently, the documentation is not yet comprehensive but is adequate for most use cases.
More comprehensive documentation is already on the back log for future works.
Accuracy and reliability on the other hand is delegated to the resource monitor, or the executor step in this case.

This comparison is not complete and should only serves as a high level comparison between the tools.
\First~tried to make the comparison as objective as possible but some of the factors subjects to non-objective evaluation and might be vague.
There are also many other factors that are not taken into account.
This means advantages and disadvantages between the benchmarking tools is not fully captured from the evaluation.
