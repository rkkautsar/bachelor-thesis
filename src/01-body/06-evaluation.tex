\chapter{\chEvaluation}
\label{ch:evaluation}

This chapter presents several evaluation regarding the implementation of \OurBenchmarkingTool~and its position relative to other benchmarking tools mentioned in \Cref{ch:existing}.
First, an example usage scenario is given in \Cref{sec:eval.scenario} to give an overview of how \OurBenchmarkingTool~can be used from setting up, running the benchmark, until the analysis step.
\Cref{sec:eval.messaging} gives a detailed look to the performance and implications of the implemented messaging mechanism.
A measurement of the resource usage of the implementation is then given in \Cref{sec:eval.resource}.
Last but not least, comparisons to other mentioned benchmarking tools is discussed in \Cref{sec:eval.comparison} based on a breakdown of the requirements from \Cref{sec:idealBenchmarkingTool}.

\section{Example Usage Scenario}
\label{sec:eval.scenario}

This section describe an example benchmarking workflow using \OurBenchmarkingTool.
Each step needed from setting up to analyzing the result is explained.

\subsection{Benchmark Setup}

The configuration for this benchmarking scenario is publicly available as a GitHub repository: \href{https://github.com/rkkautsar/benchmark-demo-sat}{\code{https://github.com/rkkautsar/benchmark-demo-sat}}.
The configuration is the same as in \lst~\ref{lst:impl.config.example}.
This benchmark is \(\bm{R_1}\) reproducible in UNIX machines.
The tools and tasks are downloaded from the internet and set up programmatically.

For this scenario, the benchmarking is done in three environment.
Computation is done in a \textsc{slurm} cluster system of TU Dresden, Taurus.
Taurus sports 2\,179 nodes with a total of 41\,468 cores.
A typical node used for the computation has an Intel(R) Xeon(R) CPU E5-2690 @ 2.90GHz.
Data collection is done in a VPS physically located in Paris, France.
This is a small virtual instance sporting an Intel(R) Atom(TM) CPU C3955 @ 2.10GHz and 1 GB of RAM.
For future reference, \first~refer to the IP bound for this VPS as \code{168.8.8.8}.
Finally, the analysis is done in a personal machine with a Intel Core i5 CPU @ 1.8 GHz.
This is not necessary but in this scenario it is needed to execute an analysis in a Jupyter notebook.

\subsection{Installation}

As noted in Chapter \ref{ch:implementation}, \OurBenchmarkingTool~is distributed through the Python's standard pypi.
There are various specific extra dependencies that also need to be installed depending on what the \OurBenchmarkingTool~will be doing in the system.
This is taken care using the `extras' dependencies specification of the package.

\begin{listing}
	\begin{minted}{bash}
client $ pip install reprobench[client,sysinfo,psmon]
server $ pip install reprobench[server,pcs]
personal $ pip install reprobench[analytics]
    \end{minted}
	\caption{Installing \OurBenchmarkingTool~in various environment}
	\label{lst:eval.install}
\end{listing}

For example, one might want to set up to three different environment.
These are the client-side for running the computation, server-side for collecting the data, and their own personal machine for analysis.
\Cref{lst:eval.install} shows how the installation can be done for each environment.

Additionally, Python's \code{virtualenv} is used in the server-side to contain the dependencies.
In client-side, a Miniconda\footlink{https://docs.conda.io/en/latest/miniconda.html} environment is used instead.
Finally, a Jupyter\footlink{https://jupyter.org/} and IPython kernel is installed for running the analysis.
Also, the repository is cloned to each environment.

\subsection{Running the Benchmark}

\begin{listing}
	\begin{minted}{text}
server $ reprobench server -a tcp://0.0.0.0:31313
client $ reprobench bootstrap -a tcp://168.8.8.8:31313
client $ reprobench manager slurm run -a tcp://168.8.8.8:31313
server $ reprobench status
 74%[#######################        ] 3904/5304 [20:18<21:00,  1.11it/s]
    \end{minted}
	\caption{Running the benchmark}
	\label{lst:eval.running}
\end{listing}

\Cref{lst:eval.running} shows the series of commands executed for running the benchmark.
First, the server process is started and bind a TCP socket at \code{0.0.0.0:31313}.
Note that exposing a port to public like this is not safe per se.
This issue is discussed later in \Cref{ch:conclusion}.

Then the bootstrapping process is started from the client.
Tools and tasks are downloaded first in the client-side before it compiles the information and send it to the server to do its own bootstrapping process.
This whole process may take a while.
In this scenario and the environment listed earlier, it takes about a minute to fully download the tools, tasks, and ready the database.

Last but not least, the \textsc{slurm} manager spawn the workers to do the computation.
This submits a 5\,304-sized job array to the job queue, as much as the number of pending runs, which also happen to be the total number of runs in the beginning.
While the worker interacts with the server and do the computation, a \code{reprobench status} command can be used to monitor the progress in real time.
Besides giving reports of the progress, this command also gives an estimate of the remaining time to completion by analyzing the speed of completion.

\subsection{Analysis}

After the benchmarking is completed, the resulting database from the server is obtained for example by using the \code{scp} command such as \mintinline{bash}{scp server:/tmp/benchmark/output/benchmark.db ./output/benchmark.db}.
This database contains all the configuration for this benchmark and can also be shared for others to analyze, validate or even replicate.
The analysis is then executed by running the \code{reprobench analyze} command with the output directory as an argument.

\begin{figure}
	\centering
	% \dummyfig{assets/plots/cactus.pgf}
	\ifdraft{
		\dummyfig{assets/plots/cactus.pgf}
	}{
		\resizebox{\textwidth}{!}{\input{assets/plots/cactus.pgf}}
	}
	\caption{An example cactus plot on analysis step}
	\label{fig:eval.cactus}
\end{figure}

The analysis step will then be executed sequentially.
\Cref{fig:eval.cactus} shows an example of a cactus plot generated in this phase.
This plot is generated by executing a generated Jupyter notebook.
\citet{piccoloToolsTechniquesComputational2016} mentions Jupyter notebooks as a notable example of literate programming tools supporting computational reproducibility.
This format allows one to easily read the code as well as its results in one easy presentation.
Using it as an analysis step allows flexible configuration to iterate the result.

\section{Messaging Performance}
\label{sec:eval.messaging}

A typical benchmark run with executor, system information collection, and output validation sends these 10 events:
one worker join event;
one run start event;
three run step events;
one system info event;
one run statistic event;
one validation event;
one run finish event;
and one worker leave event.
Among them, the worker join event is the only one that waits for a reply.

Most of the events are just notifications with little or no payload.
Validation also has small payload ($< 20$ bytes).
Run step averages on about 60 bytes.
System information averages on about 360 bytes.
Run statistic averages on about 100 bytes.
This means a typical run will only consume below 1 KB each.

An experiment is conducted to check the throughput of the client-server communication across server.
As in \Cref{sec:eval.scenario}, the client is located in TU Dresden HPC system while the server is located in a VPS in Paris.
\First~measure \(L_1\), and \(L_2\), respectively the latency between receiving message, and sending then receiving message.
Each event consists of a small 32 byte payload.
The result as an average of 1\,000 measurement is 325,29 ms, and 2\,878,54 ms respectively.
The throughput \(T_1\), and \(T_2\) can then be computed by inverting the latency, resulting in 307,86, and 34,84 messages per second respectively.
As a comparison, raw throughput between the machine as measured with \code{netcat} is 23 MB/s or equivalent to about 576 messages per second.

Furthermore, \first~also stress tested the network.
Since the server only run in one process it might not handle high throughput messaging.
Note that the \O MQ messaging framework also has a limit to its queue, defaults to 1\,000 messages.
Fortunately, the \textsc{dealer-router} pattern used in \textsc{client-server} connection implements a blocking behavior in the case of a full queue.
To validate this, two experiments are conducted.
First, a 100 parallel process is spawned, each sending up to 2\,000 events (totalling to 200\,000 events).
Second, a variable number of process up to 500 parallel process, each sending 25 events (totalling to 12\,500 events).
Each event is replied by the server, like the worker join event.

In both cases, there is no events dropped.
In every trial, all of the events are received by the server.
The highly parallel experiment also has no apparent time increase.
This means there is no blocking since the server works fast enough to forward the event to the \textsc{pub}-\textsc{sub} channel to clear the queue.
But it need to be noted that by using greenlets, this means it only works as fast as the event is processed.


\section{Resource Usage}
\label{sec:eval.resource}

\begin{figure}
	\centering

	\begin{subfigure}{.45\textwidth}
		\centering
		\ifdraft{
			\dummyfig{assets/plots/perf\_cpu.pgf}
		}{
			\resizebox{\textwidth}{!}{\input{assets/plots/perf_cpu.pgf}}
		}
		\caption{CPU Time}
		\label{fig:eval.perf.cpu}
	\end{subfigure}%
	\hfill%
	\begin{subfigure}{.45\textwidth}
		\centering
		\ifdraft{
			\dummyfig{assets/plots/perf\_rss.pgf}
		}{
			\resizebox{\textwidth}{!}{\input{assets/plots/perf_rss.pgf}}
		}
		\caption{Resident Memory}
		\label{fig:eval.perf.rss}
	\end{subfigure}

	\vspace{1cm}

	\begin{subfigure}{.45\textwidth}
		\centering
		\ifdraft{
			\dummyfig{assets/plots/perf\_io\_rw.pgf}
		}{
			\resizebox{\textwidth}{!}{\input{assets/plots/perf_io_rw.pgf}}
		}
		\caption{Disk I/O}
		\label{fig:eval.perf.disk}
	\end{subfigure}%
	\hfill%
	\begin{subfigure}{.45\textwidth}
		\centering
		\ifdraft{
			\dummyfig{assets/plots/perf\_io\_recvsent.pgf}
		}{
			\resizebox{\textwidth}{!}{\input{assets/plots/perf_io_recvsent.pgf}}
		}
		\caption{Network I/O}
		\label{fig:eval.perf.network}
	\end{subfigure}

	\caption{Server resource usage for a sample benchmark}
	\label{fig:eval.perf}
\end{figure}

\textsc{resource\_monitor} \citep{ResourceMonitorCooperative} is used to monitor the resource usage over time for the server component as shown in \Cref{fig:eval.perf}.
It is another resource monitoring tools mentioned by \citet{juvePracticalResourceMonitoring2015} besides \textsc{kickstart}.
The same VPS instance as in \Cref{sec:eval.scenario,sec:eval.messaging} is used for this experiment.

\Cref{fig:eval.perf.cpu} shows that the CPU Time usage is just below 13 seconds for a 90 seconds session.
This means the server only uses about 15\% of CPU.
\Cref{fig:eval.perf.rss} shows the resident memory usage peaked to only about 62 MB.
Note that resident memory usage means the actual physical memory used by the process.
The actual requested memory peaked to about 366 MB.
\Cref{fig:eval.perf.disk} shows the cumulative amount of data read (resp. written) from (resp. to) the disk.
It shows the server writing to the database steadily while the read only happened in the beginning.
Finally, \Cref{fig:eval.perf.network} shows the cumulative amount of data sent (resp. received) through the network.
It shows a similar trend although the data sent is a little higher than the data received.
This is because the server is sending information regarding the run to the workers while the worker only reports a small amount of result.

As the worker performance is very dependent to the steps it need to execute, we only present the performance measurement of the server.
However, a quick experiment with a single executor step with \code{sleep 10} as the tool results in sub-second CPU Time, and only 1\,724 KB of resident memory.

\section{Comparison to Existing Tools}
\label{sec:eval.comparison}

% Insert evaluation

\OurBenchmarkingTool~fulfills most of the requirements, with exception of $M_3$ and $M_5$, respectively documentation and accuracy and reliability.
Currently, the documentation is not yet comprehensive but is adequate for most use cases.
More comprehensive documentation is already on the back log for future works.
Accuracy and reliability on the other hand is delegated to the resource monitor, or the executor step in this case.
