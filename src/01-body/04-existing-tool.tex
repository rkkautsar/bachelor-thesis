\chapter{\chExisting}
\label{ch:existing}

This chapter discusses the existing benchmarking tools developed by the community.
Section \ref{sec:prior_works.overview} gives an overview of the benchmarking tool considered.
Then the following sections describes each of the considered benchmarking tools.

\section{Overview}
\label{sec:prior_works.overview}

There are five benchmarking tool that \first~selected for evaluation.
This is by no means an exhaustive collection but it should represent the current state of existing benchmarking tools.
\textsc{StarExec} particularly took a web-based approach, where the user submit the configuration and the run happened in the host system.
The rest of them, \textsc{benchmark-tool}, \textsc{BenchExec}, \textsc{BenchKit}, and \textsc{JuBE}, approaches benchmarking as task to be run in the local machine or submitted to a cluster system.

\section{\textsc{benchmark-tool}}
\textsc{benchmark-tool}\footnote{\href{https://github.com/potassco/benchmark-tool}{https://github.com/potassco/benchmark-tool}} is originally a part of Potassco\footnote{\href{https://potassco.org/}{https://potassco.org/}}, the Postdam Answer Set Solving Collection, developed at the University of Potsdam.
It has been forked and used by some researchers in the computational logic field, such as its use in Technicsche Universität Dresden and Technicsche Universität Wien.

The general workflow is like this:
(a) create a \textit{runscript}, defining the benchmark run configuration;
(b) generate the file necessary for the benchmark by running the \code{bgen} script;
(c) run the benchmark by running the script generated in the output dir;
(d) evaluate the benchmark by running the \code{beval} script, outputting an xml file;
(e) generate a CSV or OpenOffice spreadsheet from the evaluated result by running the \code{bconv} script.

This benchmarking tool is not designed to be reusable.
Instead, the user has to clone the source code, then add their benchmarking config on top of the cloned code.
This makes it possible to achieve some degree of extensibility.
For example, it can be extended to submit the benchmarking job to any cluster job scheduler, like Slurm or HTCondor by writing a specific script generation used in the step (b).
On the other hand, this also makes it hard to share the configuration without actually sharing the whole cloned source code.
Some user even uses a centralized git repository and manages different benchmarks with git branches.

This tool is also lacking in documentation and accuracy.
The only documentation available is minimal and not has not been updated since 9 years ago.
Additionally, it uses \textsc{runsolver} to measure and limit the resources.
According to \citet{beyerReliableBenchmarkingRequirements2019}, \textsc{runsolver} is not accurate and reliable.

Setting up a benchmark with this tool takes a lot effort.
The requirements for the Python interpreter and external requirements is not made clear in the 9 years old documentation.
There is also certain rules for location and name of the tool that will be used.
Furthermore, writing the \textit{result parser} often involves a lot of copying since there is no certain definition of it and most of the time it is specific to each benchmark.

\First~tried to refactor this tool to some degree\footnote{\href{https://github.com/daajoe/benchmark-tool/tree/refactor}{https://github.com/daajoe/benchmark-tool/tree/refactor}}, but decided to create a new tool instead in the middle of refactoring.
This is because to create the ideal tool, we need significant architecture overhaul and decided it's better to start anew.

\section{\textsc{BenchExec}}
\label{sec:benchmarking.impl.benchexec}

\textsc{BenchExec} \citep{philipp_wendler_2019_2561835} is developed by the Software and Computational Systems Lab of Ludwig-Maximilians-Universität München (LMU Munich)\footnote{\href{https://www.sosy-lab.org/}{https://www.sosy-lab.org/}} with the main goal of reliable measurement and limitation of resource usage.
It has been used successfully and proven its reliability and usefulness in the International Competition on Software Verification (SV-COMP) \citep{beyerReliableBenchmarkingRequirements2019}.
It is actively developed with the last release (as of 24th April 2019) in 11th February 2019 and licensed under the Apache License 2.0.

To achieve accuracy and reliability, they developed a new resource usage measurement and limiting tool, \textsc{RunExec}.
This tool uses linux-specific features, Linux Control Groups (CGroup) and Linux Containers optionally.
This allows the tool to contain the underlying process and its descendants reliably to then measure their accumulated resource usage accurately.
But in turn this also makes the installation a bit difficult and requires a minimum requirement for the kernel (full support for these features is available in Linux kernel 3.18 onwards).
Setting up CGroups in particular requires superuser access which often is not directly available in shared cluster systems.

The general workflow for benchmarking with this tool is as follows:
(a) defining an xml configuration file for the benchmark;
(b) defining a tasks set to be run (optionally in a yaml definition);
(c) reusing or defining a new \textit{tool-info module} for the tool to benchmark;
(d) running the \code{benchexec} program with the xml configuration;
(e) generating interactive table and plots with the \code{table-generator} program.

The documentation for this benchmarking framework, although somewhat sufficient, is not comprehensive and feels incomplete.
There is no tutorial or guides for getting started on benchmarking, instead the documentation revolves around the usage of each tools.
This makes following the documentation a bit difficult because the information is scattered across files.

The modular structure of this framework makes it extensible.
Tools are defined in self-contained \textit{tool-info modules} and can be reused.
They encourage user to submit a pull request for their \textit{tool-info modules} to the main repository so other user can reuse it, albeit this does not mean that the tool itself is made available to achieve $\bm{R_1}$.
There is also an executor module that can be written to enable running the benchmark in arbitrary execution environment, such as in cluster system.

\begin{listing}
	\begin{minted}{text}
		CHECK( init(main()), LTL(G valid-free) )
		CHECK( init(main()), LTL(G valid-deref) )
		CHECK( init(main()), LTL(G valid-memtrack) )
	\end{minted}
	\caption{An example property definition for \textsc{BenchExec}}
	\label{lst:benchexec.property}
\end{listing}

The downside of this framework is the evaluation is tightly coupled to the convention used in SV-COMP.
They use a property file like the one listed in Listing \ref{lst:benchexec.property}, specific to the competition.
Combined with the lack of documentation, this makes it impossible, for now, to evaluate and analyze a benchmark outside the scope of software verification with \textsc{BenchExec}.


\section{\textsc{Benchkit}}

\textsc{Benchkit} \citep{benchkit:2013} is a benchmarking tool actively used in the Model Checking Contest (MCC)\footnote{\href{https://mcc.lip6.fr/}{https://mcc.lip6.fr/}}.
The competition participants submit a virtual machine consisting of the (minimal) operating system (OS) to run the program, dependencies, and a small \textsc{BenchKit} head to interface with the \textsc{BenchKit} kernel.

The documentation for this tool\footnote{\href{http://cosyverif.org/wp-content/benchmarks/BenchKit.pdf}{http://cosyverif.org/wp-content/benchmarks/BenchKit.pdf}} has not been updated since version $\beta1$, released in February, 2013.
This documentation is plentiful and guides the user through the general workflow, but the workflow itself is confusing to follow.

This tool allows the user to run the benchmark across remote nodes, which in turn execute the run inside multiple virtual machines (this is not yet the case in the $\beta1$ version of the software described in the documentation).
The result is then compiled manually or sent automatically through e-mails from the remote nodes and then manually analyzed from the generated CSV files.
It also forces certain requirements before running the benchmarks, such as the need for a specific folder structure, and the deployment of virtual machine images to the remote nodes in advance.

It uses \textsc{SysStat}\footnote{\href{https://github.com/sysstat/sysstat}{https://github.com/sysstat/sysstat}}, which measures the whole virtual machine resource usage.
This means that the resource usage measured also includes the OS.
Accordingly, \cite{beyerReliableBenchmarkingRequirements2019} assess this measurement tool as neither accurate nor reliable.

Although the necessity of providing virtual machine makes sure the program is $\bm{R_1}$ reproducible, \citet{kordonBenchKitToolMassive2014} reported an overhead of 40 seconds per run in their experiment due to the boot-up time of virtual machines.
\cite{beyerReliableBenchmarkingRequirements2019} remarks that this 40 seconds overhead would have taken an additional 190 CPU days if used in their competition, definitely a prohibitive overhead.


\section{\textsc{JuBE}}

\textsc{JuBE} \citep{frings2010flexible} is a benchmarking environment developed by Jülich Supercomputing Centre (JSC) of Forschungszentrum Jülich, Germany.
It is actively developed, albeit with no public version control repository, with the last release (as of 24th April 2019) in 4th February 2019.
It is licensed under the GNU General Public License version 3.

Compared to other benchmarking tool, it approaches the benchmarking task in a different, completely generic way.
There is no explicit measurement and limiter tool, no explicit task instances, and no specific evaluation or analysis step.
Instead, all the benchmarking task revolves around steps, parameters, and pattern-based analyzers.

The parameters can be defined as static, a parameter space, or even dynamic parameter space with the help scripting languages such as Python.
The steps are just a shell command, receiving several variables from the parameters and \textsc{JuBE} itself.
Finally, the analyzer just match patterns to the files generated from the benchmarking steps, then produce a CSV file.

The documentation\footnote{\href{https://apps.fz-juelich.de/jsc/jube/jube2/docu/index.html}{https://apps.fz-juelich.de/jsc/jube/jube2/docu/index.html}} is comprehensive and checks all the marks in \firstposs~$M_3$ evaluation.
There is a quick start guide and various advanced usage guides available.
Every command also has all its options documented.

Since it has no preference of resource measurement and limiter tool, \first~can't evaluate $M_5$.
But it definitely has the potential to use any kind of tool to measure and limit the resource usage.
This is also the case for the execution environment.
Submitting the benchmarking task to a job queue is just another benchmarking step.

The flexibility of this tool is surely its strong point.
But this flexibility also burdens the user to do all the repetitive task of benchmarking on their own, although it is possible to create some reusable wrappers for these repetitive tasks.
The analysis step is also restricting, since the user has to output the metrics to a file, then capture it with patterns.
Additionally, the tool itself does not provide much in terms of reproducibility.
It is up to the user how to make their benchmarking reproducible since shell commands is often not reproducible by itself.


\section{\textsc{StarExec}}

\textsc{StarExec} is a web-based service for evaluating logic solvers on user-supplied benchmarks input \citep{stumpStarExecCrossCommunityInfrastructure2014}.
It is officially hosted at \href{https://www.starexec.org/}{\code{https://www.starexec.org/}}.
The source code is publicly available\footnote{\href{https://github.com/StarExec/StarExec}{https://github.com/StarExec/StarExec}} and is actively developed under the MIT license.
\First~will consider the hosted version of this tool for the evaluation.

The service is built around the idea of \emph{spaces} and \emph{primitives}.
A space is a collection of solvers, benchmarks, jobs, and users, collectively defined as \emph{primitives}.
Spaces have a hierarchical structure.
The topmost spaces are called the \emph{community spaces}, while the descendants are called \emph{subspaces} \citep{stumpStarExecCrossCommunityInfrastructure2014}.

The documentation for this service is by no mean complete, but it is detailed enough for a normal user.
Measurement and limitation of resource usage currently can be handled by either \textsc{runsolver} or \textsc{RunExec} from \textsc{BenchExec}.
There is also an active effort to apply containerization feature of \textsc{RunExec}.

To register, a user has to apply to be approved---or in other words, endorsed---by a community leader, the person managing a community space.
Then after accepted, the user can log in and view the public spaces in their community or create new subspaces.
Users can submit their own solvers and benchmarks to a space, which can also be copied across spaces.
A benchmarking job can then be submitted with the existing solver configurations, benchmarks, and predetermined post-processors.
These community-specific post-processors can only be configured by a community leader.

The service encourage shared solvers and benchmarks in the community to reduce duplication of effort.
This also helps the community practice $\bm{R_1}$ reproducibility.
Other users can view or even re-run another user's job to confirm the results in a fully reproducible way since the execution environment is the same.

The limitation that only the community leaders can approve members and configure things like pre- and post-processors is limiting the capability of this service.
Sure, for a specific community the measured metrics is more or less the same, but this also prevent other metrics to be evaluated freely.
This decision might be necessary to prevent abuse as this involves sharing a large computing power.
