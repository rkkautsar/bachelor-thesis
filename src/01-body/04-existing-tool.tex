\fancyhead[LE,RO]{\thepage}

\chapter{\chExisting}
\label{ch:existing}

Resource monitoring tools are useful to produce a comparable measurement, but as per our definition of benchmarking in \Cref{def:benchmarking}, benchmarking involves a set of runs.
Benchmarking tools exist to orchestrate these runs and use the resource monitoring tools to compare the benchmarking runs.
This chapter discusses various existing benchmarking tools developed by the community.
We first describe each of the existing tools we surveyed in \Cref{sec:existing.tools}.
Then we evaluate those tools in \Cref{sec:existing.eval}.


\section{Existing Tools}
\label{sec:existing.tools}

\begin{table}[h]
    \caption{Overview of existing benchmarking tools}
    \label{tab:existing.overview}
    \begin{threeparttable}
        \begin{adjustbox}{max width=\textwidth}
            \begin{tabular}{r c l l m{2.7cm} l l}
                Tool & Online & Language & License & Res. Monitor & Virtualization & Updated \\
                \midrule
                \textsc{benchmark-tool} & & Python & & \textsc{runsolver} & & 2018 \\
                \textsc{BenchExec} & & Python & Apache-2.0 & \textsc{runexec} & Container & 2019 \\
                \textsc{JuBE} & & Python & GPL-3 & & & 2019 \\
                \textsc{compbench} & & Python & MIT & Perf. counter + \code{getrusage()} & & 2018 \\
                \textsc{BenchKit} & & Bash & & \textsc{SysStat} & VM & 2017 \\
                \textsc{StarExec} & \checkmark & Java & MIT & \textsc{runsolver} or \textsc{runexec} & Container\tnote{1} & 2019\\
                \textsc{Optil.io} & \checkmark & ? & Closed source & ? & ? & ? \\
                \bottomrule
            \end{tabular}
        \end{adjustbox}
        \begin{tablenotes}
            \footnotesize
            \item[1] Unmerged implementation
        \end{tablenotes}
    \end{threeparttable}
\end{table}


There are six benchmarking tools that \first~selected to represent the existing tools.
An overview describing these tools is given in \Cref{tab:existing.overview}.
This is by no means an exhaustive collection, but it should represent the current state of existing benchmarking tools.
\textsc{StarExec} particularly took the approach as an internet-based service where user submit the configuration and the run happened in the host system.
The rest of them, \textsc{benchmark-tool}, \textsc{BenchExec}, \textsc{BenchKit}, and \textsc{JuBE}, approaches benchmarking as task to be run in the local machine or submitted to a cluster system.
\textsc{Optil.io} \citep{wasikOptilIoCloud2016a} is another internet-based service for doing experimentation.
It is mainly designed for creating competition.
Alas, because the source code is not publicly available and the publication does not mention its details, \first~decided not to discuss it further.
\First~describe each tools in its own section.

\subsection{\textsc{benchmark-tool}}

\textsc{benchmark-tool} \citep{GitmirrorBenchmarktoolContribute2018} is originally developed from the need to carry out experiments in the Potassco (Potsdam Answer Set Solving Collection, \url{https://potassco.org/}) project at the University of Potsdam.
It has been forked and used by some researchers in the computational logic field, such as its use in Technische Universit\"at Dresden (TU Dresden) and Technische Universit\"at Wien (TU Wien).

% The general workflow is like this:
% (a) create a \textit{runscript}, defining the benchmark run configuration;
% (b) generate the file necessary for the benchmark by running the \code{bgen} script;
% (c) run the benchmark by running the script generated in the output dir;
% (d) evaluate the benchmark by running the \code{beval} script, outputting an xml file;
% (e) generate a CSV or OpenOffice spreadsheet from the evaluated result by running the \code{bconv} script.

This benchmarking tool is not designed for sharing or integrating multiple tools.
The user has to clone the source code, then add their benchmarking config on top of the cloned source code.
This also makes it possible to achieve some degree of extensibility.
For example, it can be extended to submit the benchmarking job to any cluster job scheduler, like \textsc{Slurm} \citep{yoo2003slurm} or \textsc{HTCondor} \citep{condor-practice} by writing a specific script generation used in the step (b).
On the other hand, this also makes it hard to share the configuration without actually sharing the whole cloned source code.
Some user even uses a centralized git repository and manages different benchmarks with git branches.

This tool is also lacking in documentation and reliable measurement.
The only documentation available is minimal and has not been updated since 2010.
It uses \textsc{runsolver} to measure and limit the resources which suffers from unreliable measurement as discussed before in \Cref{ch:resource}.

Setting up a benchmark with this tool takes a lot effort.
Requirements such as the used Python version and external dependencies are not listed clearly in the 9 years old documentation.
There are also certain rules for location and name of the tool that will be used.
Furthermore, writing the \textit{result parser} often involves a lot of copying since there is no certain definition of it and most of the time it is specific to each benchmark.

\subsection{\textsc{BenchExec}}
\label{sec:benchmarking.impl.benchexec}

\textsc{BenchExec} \citep{philipp_wendler_2019_2561835} is developed by the SoSy-Lab (Software and Computational Systems Lab, \url{https://www.sosy-lab.org/}) of Ludwig-Maximilians-Universit\"at München (LMU Munich) with the main goal of reliable measurement and limitation of resource usage.
It has been used successfully and proven its reliability and usefulness in the International Competition on Software Verification (SV-COMP) \citep{beyerReliableBenchmarkingRequirements2019}.
It is actively developed with the last release (as of 24th April 2019) in 11th February 2019 and licensed under the Apache License 2.0.

To achieve accuracy and reliability, they developed a new resource usage measurement and limiting tool, \textsc{RunExec}.
This tool uses linux-specific features, Linux Control Groups (CGroup) and Linux Containers optionally.
This allows the tool to contain the underlying process and its descendants reliably to then measure their accumulated resource usage accurately.
But in turn this also makes the installation a bit difficult and requires a minimum requirement for the kernel (full support for these features is available in Linux kernel 3.18 onwards).
Setting up CGroups in particular requires superuser access which often is not directly available in shared cluster systems.

% The general workflow for benchmarking with this tool is as follows:
% (a) defining an xml configuration file for the benchmark;
% (b) defining a tasks set to be run (optionally in a yaml definition);
% (c) reusing or defining a new \textit{tool-info module} for the tool to benchmark;
% (d) running the \code{benchexec} program with the xml configuration;
% (e) generating interactive table and plots with the \code{table-generator} program.

The documentation for this benchmarking framework is sufficient but not comprehensive.
It only covers basic use case and points out several details to the source code.
There is no tutorial or guides for getting started on benchmarking, instead the documentation revolves around the usage of each tools.
This makes following the documentation a bit difficult because the information is scattered across files.

The modular structure of this framework makes it extensible.
Tools are defined in self-contained \textit{tool-info modules} and can be reused.
They encourage user to submit a pull request for their \textit{tool-info modules} to the main repository so other user can reuse it, albeit this does not mean that the tool itself is made available to achieve \textbf{R1}.
There is also an executor module that can be written to enable running the benchmark in arbitrary execution environment, such as in cluster system.

\begin{listing}
    \begin{minted}{text}
        CHECK( init(main()), LTL(G valid-free) )
        CHECK( init(main()), LTL(G valid-deref) )
        CHECK( init(main()), LTL(G valid-memtrack) )
    \end{minted}
    \caption{An example property definition for \textsc{BenchExec}}
    \label{lst:benchexec.property}
\end{listing}

The downside of this framework is the evaluation is tightly coupled to the convention used in SV-COMP.
They use a property file like the one listed in Listing \ref{lst:benchexec.property}, specific to the competition.
Combined with the lack of documentation, this makes it impossible, for now, to evaluate and analyze a benchmark outside the scope of software verification with \textsc{BenchExec}.


\subsection{\textsc{Benchkit}}

\textsc{BenchKit} \citep{benchkit:2013} is a benchmarking tool actively used in the MCC (Model Checking Contents, \url{https://mcc.lip6.fr/}).
The competition participants submit a virtual machine consisting of the (minimal) operating system (OS) to run the program, dependencies, and a small \textsc{BenchKit} head to interface with the \textsc{BenchKit} kernel.

The documentation for this tool\footlink{http://cosyverif.org/wp-content/benchmarks/BenchKit.pdf} has not been updated since version $\beta1$, released in February, 2013.
The documentation guides the user through the general workflow with comprehensive explanation.
Unfortunately, some of the details of setting up the virtual machine image and interfacing it with \textsc{BenchKit} is left unexplained.
An example image implementation can greatly improve the documentation.


This tool allows the user to run the benchmark across remote nodes, which in turn execute the run inside multiple virtual machines (this is not yet the case in the $\beta1$ version of the software described in the documentation).
The result is then compiled manually or sent automatically through e-mails from the remote nodes and then manually analyzed from the generated CSV files.
It also forces certain requirements before running the benchmarks, such as the need for a specific folder structure, and the deployment of virtual machine images to the remote nodes in advance.

It uses \textsc{SysStat} \citep{sebastienPerformanceMonitoringTools2019}, a general purpose system monitoring tool which measures the whole virtual machine resource usage.
This means that the resource usage measured also includes the one used by the virtualized OS, which may not be reliably comparable with each other since the OS can differ.
\First~ do not put \textsc{SysStat} as the discussed resource monitoring tool in \Cref{ch:resource} because normally \first~want to monitor resource usage of a process, not the whole OS.


Although the necessity of providing virtual machine makes sure the program is \textbf{R1} reproducible, \citet{kordonBenchKitToolMassive2014} reported an overhead of 40 seconds per run in their experiment due to the boot-up time of virtual machines.
\cite{beyerReliableBenchmarkingRequirements2019} remark that this 40 seconds overhead would have taken an additional 190 CPU days if used in their competition.
The total computation time for their competition is 490 days \citep{beyer2017software}.
This means the booting overhead for using virtual machine alone will result in 39\% more computation time.


\subsection{\textsc{JuBE}}

\textsc{JuBE} \citep{frings2010flexible} is a benchmarking environment developed by Jülich Supercomputing Centre (JSC) of Forschungszentrum Jülich, Germany.
It is actively developed internally with the last release (as of 24th April 2019) in 4th February 2019.Its source code is publicly available and is licensed under the GNU General Public License version 3.

Compared to other benchmarking tool, it approaches the benchmarking task in a different, completely generic way.
There is no explicit measurement and limiter tool, no explicit task instances, and no specific evaluation or analysis step.
Instead, all the benchmarking task revolves around steps, parameters, and pattern-based analyzers.

The parameters can be defined as static, a parameter space, or even dynamic parameter space with the help scripting languages such as Python.
The steps are just a shell command, receiving several variables from the parameters and \textsc{JuBE} itself.
Finally, the analyzer just match patterns to the files generated from the benchmarking steps, then produce a CSV file.

The documentation\footlink{https://apps.fz-juelich.de/jsc/jube/jube2/docu/index.html} is comprehensive.
There is a quick start guide and various advanced usage guides available.
Every command also has all its options documented.

It has no preference of resource measurement and limiter tool.
But it definitely has the potential to use any kind of tool to measure and limit the resource usage.
This is also the case for the execution environment.
Submitting the benchmarking task to a job queue is just another benchmarking step.

The flexibility of this tool is surely its strong point.
But this flexibility also burdens the user to do all the repetitive task of benchmarking on their own, although it is possible to create some reusable wrappers for these repetitive tasks.
The analysis step is also restricting, since the user has to output the metrics to a file, then capture it with patterns.
Additionally, the tool itself does not provide much in terms of reproducibility.
It is up to the user how to make their benchmarking process reproducible since the tool only provides a high level structure for benchmarking.


\subsection{\textsc{StarExec}}

\textsc{StarExec} is a web-based service for evaluating logic solvers on user-supplied benchmarks input \citep{stumpStarExecCrossCommunityInfrastructure2014}.
It is officially hosted at \href{https://www.starexec.org/}{\code{https://www.starexec.org/}}.
The source code is publicly available and is actively developed under the MIT license in \citet{StarExecCrossCommunity2019}.
\First~will consider the hosted version of this tool for the evaluation.

The service is built around the idea of \emph{spaces} and \emph{primitives}.
A space is a collection of solvers, benchmarks, jobs, and users, collectively defined as \emph{primitives}.
Spaces have a hierarchical structure.
The topmost spaces are called the \emph{community spaces}, while the descendants are called \emph{subspaces} \citep{stumpStarExecCrossCommunityInfrastructure2014}.

The documentation for this service is not comprehensive but is detailed enough for a normal user.
Some parts, particularly the feature specific for community leader is not documented.
Measurement and limitation of resource usage can currently be handled by either \textsc{runsolver} or \textsc{RunExec}.
There is also an active effort to apply containerization feature of \textsc{RunExec}\footlink{https://github.com/StarExec/StarExec/pull/167}.

To register, a user has to apply to be approved---or in other words, endorsed---by a community leader, the person managing a community space.
Then after accepted, the user can log in and view the public spaces in their community or create new subspaces.
Users can submit their own solvers and benchmarks to a space, which can also be copied across spaces.
A benchmarking job can then be submitted with the existing solver configurations, benchmarks, and predetermined post-processors.
These community-specific post-processors can only be configured by a community leader.

The service encourage shared solvers and benchmarks in the community to reduce duplication of effort.
This also helps the community practice \textbf{R1} reproducibility.
Other users can view or even re-run another user's job to confirm the results in a fully reproducible way since the execution environment is the same.

The limitation that only the community leaders can approve members and configure things like pre- and post-processors is limiting the capability of this service.
For a specific community the measured metrics is more or less the same, but this also prevent other from using different metrics that may only be needed specifically for their benchmark.
This decision might be necessary to prevent abuse as this service involves sharing a large computing power.

\subsection{\textsc{compbench}}

\textsc{compbench} \citep{gochtPythonFrameworkRunning2018} is an experimentation tool developed by Stephan Gocht from KTH Royal Institute of Technology.
It was used to carry out the experiment for their master thesis in Karlsruhe Institute of Technology \citep{gochtIncrementalSATSolving}.
It is implemented as a Python module, although not published to any Python package publisher.
The source code is publicly under the MIT License.
It was last updated at October 2018.

There is no documentation available, although a minimal example is given with the source code.
\textsc{compbench} is designed to be extensible by using modular tools as building blocks.
The tools are listed in the configuration file, allowing the user to fully customize how the benchmarking process will run.
Analysis is then done separately with the resulting result stored in the database.


\section{Evaluation of Existing Tools}
\label{sec:existing.eval}

\First~breakdown the requirements defined in \Cref{sec:idealBenchmarkingTool} to a few key factors or indicators.
These key factors are used to measure how much of the requirements are fulfilled.
The breakdown is as following:

\begin{enumerate}[noitemsep]
    \item Extensible/configurable
    \begin{enumerate}[noitemsep]
        \item Evaluation is not enforced.
        \item Analysis is not enforced.
        \item Able to source benchmark instances from remote sources.
        \item Able to submit jobs to arbitrary job scheduler such as \textsc{Slurm} and \textsc{HTCondor} cluster system.
        \item Does not enforce tool to be implemented in specific language.
        \item Support repeating runs.
        \item Support multiple tool configurations.
        \item Support defining tool configurations as parameter spaces.
        \item Support resource limiting.
        \item Support arbitrary resource limiting tool.
    \end{enumerate}

    \item Minimal effort to use
    \begin{enumerate}[noitemsep]
        \item Configuration is easy to edit.
        \item Documentation exists.
        \item Installation guide exists.
        \item Documentation is up-to-date.
        \item \newcontent{Documentation is comprehensive. An example of comprehensive documentation as a baseline is the documentation \textsc{JuBE}. It includes guides for many possible use cases, developer documentation, and also documented functions and its parameters in its source code.}
        \item Requires no superuser privilege.
        \item Requirements are documented.
        \item \newcontent{Simple installation steps. We decide if an installation is not simple if it requires more than two steps to install, no installation guide, or requires the user to have a pre-knowledge of specific OS features such as mounting a pseudo-filesystem.}
        \item \newcontent{Provides basic implementation for common benchmarking usage which include monitoring common resource usages and executing implementations to benchmark instances.}
        \item Ensure robust user experience through unit and integration testing.
    \end{enumerate}

    \item Accurate and Reliable
    \begin{enumerate}[noitemsep]
        \item Measure and limit resources accurately.
        \item Terminate processes reliably.
        \item Assign cores deliberately.
        \item Respect Nonuniform Memory Access.
        \item Avoid Swapping.
        \item Isolate Individual Runs.
    \end{enumerate}

    \item Reproducible
    \begin{enumerate}[noitemsep]
        \item System information is recorded.
        \item Results can be shared to other researchers \newcontent{in just one or two steps}.
        \item Benchmark setup can be shared to and reproduced by other researchers \newcontent{in just one or two steps}.
        \item Encourage the use of reproducible benchmark (\textbf{R2}) \newcontent{by allowing or recommending the use of reproducible data}.
        \item Encourage the use of reproducible tool implementation (\textbf{R1}) \newcontent{by allowing or recommending the use of reproducible implementation}.
    \end{enumerate}
\end{enumerate}

% Each of the evaluated tools are scored according to the degree of fulfillment for each requirements.
% That is, if $M_i$ is the scored degree of fulfillment of the $i$-th requirement, and $\mu_{i}$ is a vector of size $n$ such that
% \[
%     \mu_{ij} =
%     \begin{cases}
%         1 & \text{if the $j$-th key factor of $i$-th requirement is fulfilled} \\
%         0 & \text{otherwise}
%     \end{cases}
% \]
% then the degree of fulfillment is the average of $\mu_i$:
% \(
%     M_i = \frac{\sum\mu_{i}}{|\mu_i|}
% \)

% Furthermore, as an overall measure, \first~also denote $M_\sigma$ as the overall overage of all $\mu_{i}$ regardless of its requirement category.


\begin{table}
    \caption{Requirements score for various existing benchmarking tools}
    \label{tab:reqscoresummary}
    \begin{threeparttable}
        \begin{adjustbox}{max width=\textwidth}
            \begin{tabular}{l}
                \begin{tabular}{l | cccccccccc | cccccccccc}
                    \toprule
                    \multirow{2}{*}{Tool} &
                        \multicolumn{10}{c|}{Extensible/configurable} &
                        \multicolumn{10}{c}{Minimal effort to use}\\
                        & a & b & c & d & e & f & g & h & i & j &
                        a & b & c & d & e & f & g & h & i & j\\
                    \midrule
                    \textsc{benchmark-tool} &
                        & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & & \checkmark & \checkmark &
                        \checkmark & \checkmark & & & & \checkmark & & & &\\
                    \textsc{BenchExec} &
                        & & & \checkmark & \checkmark & & \checkmark & & \checkmark & &
                        \checkmark & \checkmark & \checkmark & \checkmark & & & & & \checkmark & \checkmark\\
                    \textsc{BenchKit} &
                        & \checkmark & & & \checkmark & & \checkmark & & \checkmark & &
                        & \checkmark & \checkmark & & & & & & \checkmark &\\
                    \textsc{JuBE} &
                        \checkmark & \checkmark & & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &
                        \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & &\\
                    \textsc{compbench} &
                        \checkmark & \checkmark & & \checkmark & \checkmark & & \checkmark & \checkmark & \checkmark & &
                        \checkmark & & & & & \checkmark & & \checkmark & \checkmark &\\
                    \textsc{StarExec} &
                        \checkmark & \checkmark & \checkmark & & \checkmark & & \checkmark & & \checkmark & \checkmark &
                        \checkmark & \checkmark & \checkmark & \checkmark & & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark\\
                    \bottomrule
                \end{tabular}\\
                \begin{tabular}{l | cccccc | ccccc}
                    \multirow{2}{*}{Tool} &
                        \multicolumn{6}{c|}{Accurate and Reliable} &
                        \multicolumn{4}{c}{Reproducible}\\
                        & a & b & c & d & e & f &
                        a & b & c & d & e\\
                    \midrule
                    \textsc{benchmark-tool} &
                        & & & \checkmark & &
                        & & \checkmark & & & \\
                    \textsc{BenchExec} &
                        \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &
                        \checkmark & \checkmark & \checkmark & & \checkmark
                        \\
                    \textsc{BenchKit} &
                        & \checkmark & & \checkmark & & \checkmark &
                        & & & & \checkmark  \\
                    \textsc{JuBE} &
                        \textasteriskcentered & \textasteriskcentered & \textasteriskcentered & \textasteriskcentered & \textasteriskcentered & \textasteriskcentered &
                        & \checkmark & \checkmark \\
                    \textsc{compbench} &
                        & & & & & &
                        & \checkmark & \checkmark & & \checkmark \\
                    \textsc{StarExec} &
                        \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &
                        \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
                    \bottomrule
                \end{tabular}
            \end{tabular}
        \end{adjustbox}
        \begin{tablenotes}
            \note (\textasteriskcentered) means the accuracy and reliability depends on the resource monitoring\\ tool used as the benchmarking tool is agnostic to them.
        \end{tablenotes}
    \end{threeparttable}
\end{table}


\Cref{tab:reqscoresummary} shows that none of the considered benchmarking tools fully fulfill the given requirements.
The closest is \textsc{StarExec} and \textsc{JuBE}.
Note that most of the features available in \textsc{StarExec}, such as managing post-processors and plots, are restricted to the community leaders.
The normal user is forced to use what was already available in the community space.
This contrasts with \textsc{JuBE} that approaches benchmarking in a very generic way.
While this approach allows the tool to be used in many possible use cases, it may also discourage new users to start using it since there is no sensible defaults and the effort to customize it might be a bit much.


Additionally, it can be seen from \Cref{tab:existing.overview} that most tools are implemented in Python.
This decision might be because Python offers an easy to understand language while also allowing low-level interfacing with C libraries.
For example, it has been shown that developers unfamiliar with both \textsc{BenchExec} and Python can write tools integration successfully for SV-COMP 2016 \citep{beyerReliableBenchmarkingRequirements2019}.

Note that this comparison is not complete and should only serves as a high level comparison between the tools.
\First~tried to make the comparison as objective as possible but some of the factors still subjects to non-objective evaluation.
There are also many other factors that are not taken into account.
This means advantages and disadvantages between the benchmarking tools is not fully captured from the evaluation.

Since none of the existing tools fully meet the criteria for the ideal benchmarking tool, \first~decided to develop a new benchmarking tool.
We introduce this new benchmarking tool in \Cref{ch:implementation}.

