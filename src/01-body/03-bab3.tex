\chapter{\babTiga}
\label{ch:priorWorks}

% intro

\section{Overview}

There are six benchmarking tool that \first~evaluate.
This is by no means an exhaustive collection but it should represent the current state of existing benchmarking tools.
Two of them---\textsc{Optil.io} and \textsc{StarExec}---took a web-based approach, where the user submit the configuration and the run happened in the host system.
\textsc{Optil.io} is a bit different since it specializes in competition-style benchmarking.
The rest of them---\textsc{benchmark-tool}, \textsc{BenchExec}, \textsc{BenchKit}, and \textsc{JuBE}---approaches benchmarking as task to be run in the local machine or submitted to a cluster system.


\section{Evaluation method}

The requirements from \hyperref[sec:idealBenchmarkingTool]{Section 2.3} are broken down to a few key factors.
These key factors, stated as questions, are used to measure how much of the requirements are fulfilled.

\newcounter{reqCount}
\newcounter{reqFactorCount}[reqCount]
\newcommand{\reqLabel}[1]{
	\setcounter{reqFactorCount}{0}
	\addtocounter{reqCount}{1}
	\arabic{reqCount}.
	#1
}
\newcommand{\reqFactor}[1]{
	\addtocounter{reqFactorCount}{1}
	(\alph{reqFactorCount}) #1
}

\begin{ThreePartTable}
	\begin{TableNotes}
		\footnotesize
		\item[$\alpha$] Subjective evaluation
	\end{TableNotes}
	\begin{longtable}{ll}

		\textbf{Requirements} & \textbf{Factors} \\
		\toprule
		\endhead

		% \cmidrule{2-2}
		\multicolumn{2}{r}{\textit{continued}}
		\endfoot

		\bottomrule
		\insertTableNotes\\
		\caption{Metrics for evaluating existing benchmarking tools}
		\endlastfoot

		\multirow{5}{*}{\reqLabel{Extensibility}}
			& \reqFactor{Flexible evaluation step} \\*
			& \reqFactor{Flexible analysis step} \\*
			& \reqFactor{Flexible benchmark instance source} \\*
			& \reqFactor{Does not enforce implementation type} \\*
			& \reqFactor{Can support arbitrary task scheduler} \\*
		\midrule

		\multirow{5}{*}{\reqLabel{Configurability}}
			& \reqFactor{Multiple runs} \\*
			& \reqFactor{Multiple tool configurations} \\*
			& \reqFactor{Support parameter space} \\*
			& \reqFactor{Benchmark instance selection} \\*
			& \reqFactor{Set resource limit} \\*
		\midrule

		\multirow{5}{*}{\reqLabel{Documentation}}
			& \reqFactor{Self-documenting configuration} \\*
			& \reqFactor{Installation guide} \\*
			& \reqFactor{Configuration guide} \\*
			& \reqFactor{Main workflow guide} \\*
			& \reqFactor{Comprehensive documentation\tnote{$\alpha$}} \\*
		\midrule

		\multirow{4}{*}{\reqLabel{Setup Effort}}
			& \reqFactor{No superuser privilege} \\*
			& \reqFactor{Installation guide} \\*
			& \reqFactor{Documented requirements} \\*
			& \reqFactor{No cumbersome dependencies\tnote{$\alpha$}} \\*
		\midrule

		\multirow{6}{*}{\reqLabel{Accuracy \& Reliability}}
			& \reqFactor{Measure and Limit Resources Accurately} \\*
			& \reqFactor{Terminate Processes Reliably} \\*
			& \reqFactor{Assign Cores Deliberately} \\*
			& \reqFactor{Respect Nonuniform Memory Access} \\*
			& \reqFactor{Avoid Swapping} \\*
			& \reqFactor{Isolate Individual Runs} \\*
		\midrule

		\multirow{5}{*}{\reqLabel{Reproducibility}}
			& \reqFactor{Stored system information} \\*
			& \reqFactor{Sharable results} \\*
			& \reqFactor{Sharable configuration} \\*
			& \reqFactor{Encourage sharable data\tnote{$\alpha$}} \\*
			& \reqFactor{Encourage sharable implementation\tnote{$\alpha$}} \\*
		\bottomrule
	\end{longtable}
\end{ThreePartTable}


\section{Evaluation}

Each of the evaluated tools are scored according to the degree of fulfillment for each requirements.
That is, if $M_i$ is the scored degree of fulfillment of the $i$-th requirement, and $\mu_{i}$ is a vector of size $n$ such that
\[
	\mu_{ij} =
	\begin{cases}
		1 & \text{if the $j$-th key factor of $i$-th requirement is fulfilled}\\
		0 & \text{otherwise}
	\end{cases}
\]
then
\[
	M_i = \frac{\sum\mu_{i}}{|\mu_i|}
\]

Furthermore, as an overall measure, \first~also denote $M$ as the weighted sum of all $M_i$:
\[
	M = \sum{\frac{M_i |\mu_i|}{|\mu_i|}}
\]

\subsection{\textsc{benchmark-tool}}
\subsection{\textsc{BenchExec}}
\subsection{\textsc{Benchkit}}
\subsection{\textsc{JuBE}}
\subsection{\textsc{Optil.io}}
\subsection{\textsc{StarExec}}
