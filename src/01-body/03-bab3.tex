\chapter{\babTiga}
\label{ch:priorWorks}

Repellendus officiis cumque incidunt vel placeat tempora totam. Provident impedit sint qui facilis omnis. Doloremque autem nam consequatur hic repellat aut reiciendis. Perferendis nisi quisquam est quas nulla.


\section{Overview}

There are five benchmarking tool that \first~evaluate.
This is by no means an exhaustive collection but it should represent the current state of existing benchmarking tools.
\textsc{StarExec} particularly took a web-based approach, where the user submit the configuration and the run happened in the host system.
The rest of them, \textsc{benchmark-tool}, \textsc{BenchExec}, \textsc{BenchKit}, and \textsc{JuBE}, approaches benchmarking as task to be run in the local machine or submitted to a cluster system.


\section{Evaluation method}

The requirements from Section \ref{sec:idealBenchmarkingTool} are broken down to a few key factors.
These key factors are used to measure how much of the requirements are fulfilled.

\newcounter{reqCount}
\newcounter{reqFactorCount}[reqCount]
\newcommand{\reqLabel}[1]{
	\setcounter{reqFactorCount}{0}
	\addtocounter{reqCount}{1}
	\arabic{reqCount}.
	#1
}
\newcommand{\reqFactor}[1]{
	\addtocounter{reqFactorCount}{1}
	(\alph{reqFactorCount}) #1
}

\begin{ThreePartTable}
	\begin{TableNotes}
		\footnotesize
		\item[$\alpha$] Subjective evaluation
	\end{TableNotes}
	\begin{longtable}{ll}

		\textbf{Requirements} & \textbf{Factors} \\
		\toprule
		\endhead

		% \cmidrule{2-2}
		\multicolumn{2}{r}{\textit{continued}}
		\endfoot

		\bottomrule
		\insertTableNotes\\
		\caption{Metrics for evaluating existing benchmarking tools}
		\endlastfoot

		\multirow{5}{*}{\reqLabel{Extensibility}}
			& \reqFactor{Flexible evaluation step} \\*
			& \reqFactor{Flexible analysis step} \\*
			& \reqFactor{Flexible benchmark instance source} \\*
			& \reqFactor{Does not enforce implementation type} \\*
			& \reqFactor{Can support arbitrary task scheduler} \\*
		\midrule

		\multirow{5}{*}{\reqLabel{Configurability}}
			& \reqFactor{Multiple runs} \\*
			& \reqFactor{Multiple tool configurations} \\*
			& \reqFactor{Support parameter space} \\*
			& \reqFactor{Benchmark instance selection} \\*
			& \reqFactor{Set resource limit} \\*
		\midrule

		\multirow{5}{*}{\reqLabel{Documentation}}
			& \reqFactor{Self-documenting configuration} \\*
			& \reqFactor{Installation guide} \\*
			& \reqFactor{Configuration guide} \\*
			& \reqFactor{Main workflow guide} \\*
			& \reqFactor{Comprehensive documentation\tnote{$\alpha$}} \\*
		\midrule

		\multirow{4}{*}{\reqLabel{Setup Effort}}
			& \reqFactor{No superuser privilege} \\*
			& \reqFactor{Installation guide} \\*
			& \reqFactor{Documented requirements} \\*
			& \reqFactor{No cumbersome dependencies\tnote{$\alpha$}} \\*
		\midrule

		\multirow{6}{*}{\reqLabel{Accuracy \& Reliability}}
			& \reqFactor{Measure and Limit Resources Accurately} \\*
			& \reqFactor{Terminate Processes Reliably} \\*
			& \reqFactor{Assign Cores Deliberately} \\*
			& \reqFactor{Respect Nonuniform Memory Access} \\*
			& \reqFactor{Avoid Swapping} \\*
			& \reqFactor{Isolate Individual Runs} \\*
		\midrule

		\multirow{5}{*}{\reqLabel{Reproducibility}}
			& \reqFactor{Stored system information} \\*
			& \reqFactor{Sharable results} \\*
			& \reqFactor{Sharable configuration} \\*
			& \reqFactor{Encourage sharable data\tnote{$\alpha$}} \\*
			& \reqFactor{Encourage sharable implementation\tnote{$\alpha$}} \\*
	\end{longtable}
\end{ThreePartTable}


\section{Evaluation}

Each of the evaluated tools are scored according to the degree of fulfillment for each requirements.
That is, if $M_i$ is the scored degree of fulfillment of the $i$-th requirement, and $\mu_{i}$ is a vector of size $n$ such that
\[
	\mu_{ij} =
	\begin{cases}
		1 & \text{if the $j$-th key factor of $i$-th requirement is fulfilled}\\
		0 & \text{otherwise}
	\end{cases}
\]
then the degree of fulfillment is the average of $\mu_i$:
\[
	M_i = \frac{\sum\mu_{i}}{|\mu_i|}
\]

Furthermore, as an overall measure, \first~also denote $M_\sigma$ as the overall overage of all $\mu_{i}$ regardless of its requirement category.


\begin{ThreePartTable}
	\begin{TableNotes}
		\footnotesize
		\item[*] Values are emptied if it's not applicable. The complete evaluation is given in the appendix.
	\end{TableNotes}
	\begin{longtable}{lddddddd}
		& \multicolumn{1}{c}{$M_1$} & \multicolumn{1}{c}{$M_2$} & \multicolumn{1}{c}{$M_3$} & \multicolumn{1}{c}{$M_4$} & \multicolumn{1}{c}{$M_5$} & \multicolumn{1}{c}{$M_6$} & \multicolumn{1}{c}{$M_\sigma$}\\
		\midrule
		\textsc{benchmark-tool} & .60 & .80 & .20 & .50 & .17 & .40 & .43 \\
		\textsc{BenchExec} & .40 & .60 & .60 & .50 & 1.00 & .80 & .66 \\
		\textsc{Benchkit} & .40 & .40 & .60 & .25 & .50 & .40 & .43 \\
		\textsc{JuBE} & .80 & 1.00 & 1.00 & 1.00 & & .40 & .83 \\
		% \textsc{Optil.io} & .40 & .80 & .60 & 1.00 & & .40 & .62 \\
		\textsc{StarExec} & .60 & .60 & .80 & 1.00 & 1.00 & 1.00 & .83 \\
		\bottomrule
		\insertTableNotes\\
		\caption{Requirements score for various existing benchmarking tools}
		\label{tab:reqscoresummary}
	\end{longtable}
\end{ThreePartTable}

Table \ref{tab:reqscoresummary} shows that none of the considered benchmarking tools fulfilled the requirements given.
The closest is \textsc{StarExec} and \textsc{JuBE}.
\textsc{StarExec} can even---arguably---achieves $\bm{R_1}$ by default.

But most of the features available in \textsc{StarExec}, such as managing post-processors and plots, are restricted to the community leaders.
The normal user is forced to use what was available in the community space.
On the other hand, \textsc{JuBE} is too generic and provides little support for reproducibility.

Further descriptions and evaluations of each benchmarking tools are given below.

\subsection{\textsc{benchmark-tool}}
\textsc{benchmark-tool}\footnote{\href{https://github.com/potassco/benchmark-tool}{https://github.com/potassco/benchmark-tool}} is originally a part of Potassco\footnote{\href{https://potassco.org/}{https://potassco.org/}}, the Postdam Answer Set Solving Collection, developed at the University of Potsdam.
It has been forked and used by some researchers in the computational logic field, such as its use in Technicsche Universität Dresden and Technicsche Universität Wien.

The general workflow is like this:
(a) create a \textit{runscript}, defining the benchmark run configuration;
(b) generate the file necessary for the benchmark by running the \code{bgen} script;
(c) run the benchmark by running the script generated in the output dir;
(d) evaluate the benchmark by running the \code{beval} script, outputting an xml file;
(e) generate a CSV or OpenOffice spreadsheet from the evaluated result by running the \code{bconv} script.

This benchmarking tool is not designed to be reusable.
Instead, the user has to clone the source code, then add their benchmarking config on top of the cloned code.
This makes it possible to achieve some degree of extensibility.
For example, it can be extended to submit the benchmarking job to any cluster job scheduler, like Slurm or HTCondor by writing a specific script generation used in the step (b).
On the other hand, this also makes it hard to share the configuration without actually sharing the whole cloned source code.
Some user even uses a centralized git repository and manages different benchmarks with git branches.

This tool is also lacking in documentation and accuracy.
The only documentation available is minimal and not has not been updated since 9 years ago.
Additionally, it uses \textsc{runsolver} to measure and limit the resources.
According to \citet{beyerReliableBenchmarkingRequirements2019}, \textsc{runsolver} is not accurate and reliable.

Setting up a benchmark with this tool takes a lot effort.
The requirements for the Python interpreter and external requirements is not made clear in the 9 years old documentation.
There is also certain rules for location and name of the tool that will be used.
Furthermore, writing the \textit{result parser} often involves a lot of copying since there is no certain definition of it and most of the time it is specific to each benchmark.

\First~tried to refactor this tool to some degree\footnote{\href{https://github.com/daajoe/benchmark-tool/tree/refactor}{https://github.com/daajoe/benchmark-tool/tree/refactor}}, but decided to create a new tool instead in the middle of refactoring.
This is because to create the ideal tool, we need significant architecture overhaul and decided it's better to start anew.

\subsection{\textsc{BenchExec}}

\textsc{BenchExec} \citep{philipp_wendler_2019_2561835} is developed by the Software and Computational Systems Lab of Ludwig-Maximilians-Universität München (LMU Munich)\footnote{\href{https://www.sosy-lab.org/}{https://www.sosy-lab.org/}} with the main goal of reliable measurement and limitation of resource usage.
It has been used successfully and proven its reliability and usefulness in the International Competition on Software Verification (SV-COMP) \citep{beyerReliableBenchmarkingRequirements2019}.
It is actively developed with the last release (as of 24th April 2019) in 11th February 2019 and licensed under the Apache License 2.0.

To achieve accuracy and reliability, they developed a new resource usage measurement and limiting tool, \textsc{RunExec}.
This tool uses linux-specific features, Linux Control Groups (CGroup) and Linux Containers optionally.
This allows the tool to contain the underlying process and its descendants reliably to then measure their accumulated resource usage accurately.
But in turn this also makes the installation a bit difficult and requires a minimum requirement for the kernel (full support for these features is available in Linux kernel 3.18 onwards).
Setting up CGroups in particular requires superuser access which often is not directly available in shared cluster systems.

The general workflow for benchmarking with this tool is as follows:
(a) defining an xml configuration file for the benchmark;
(b) defining a tasks set to be run (optionally in a yaml definition);
(c) reusing or defining a new \textit{tool-info module} for the tool to benchmark;
(d) running the \code{benchexec} program with the xml configuration;
(e) generating interactive table and plots with the \code{table-generator} program.

The documentation for this benchmarking framework, although somewhat sufficient, is not comprehensive and feels incomplete.
There is no tutorial or guides for getting started on benchmarking, instead the documentation revolves around the usage of each tools.
This makes following the documentation a bit difficult because the information is scattered across files.

The modular structure of this framework makes it extensible.
Tools are defined in self-contained \textit{tool-info modules} and can be reused.
They encourage user to submit a pull request for their \textit{tool-info modules} to the main repository so other user can reuse it, albeit this does not mean that the tool itself is made available to achieve $\bm{R_1}$.
There is also an executor module that can be written to enable running the benchmark in arbitrary execution environment, such as in cluster system.

\begin{listing}
	\begin{minted}{text}
		CHECK( init(main()), LTL(G valid-free) )
		CHECK( init(main()), LTL(G valid-deref) )
		CHECK( init(main()), LTL(G valid-memtrack) )
	\end{minted}
	\caption{An example property definition for \textsc{BenchExec}}
	\label{lst:benchexec.property}
\end{listing}

The downside of this framework is the evaluation is tightly coupled to the convention used in SV-COMP.
They use a property file like the one listed in Listing \ref{lst:benchexec.property}, specific to the competition.
Combined with the lack of documentation, this makes it impossible, for now, to evaluate and analyze a benchmark outside the scope of software verification with \textsc{BenchExec}.


\subsection{\textsc{Benchkit}}

\textsc{Benchkit} \citep{benchkit:2013} is a benchmarking tool actively used in the Model Checking Contest (MCC)\footnote{\href{https://mcc.lip6.fr/}{https://mcc.lip6.fr/}}.
The competition participants submit a virtual machine consisting of the (minimal) operating system (OS) to run the program, dependencies, and a small \textsc{BenchKit} head to interface with the \textsc{BenchKit} kernel.

The documentation for this tool\footnote{\href{http://cosyverif.org/wp-content/benchmarks/BenchKit.pdf}{http://cosyverif.org/wp-content/benchmarks/BenchKit.pdf}} has not been updated since version $\beta1$, released in February, 2013.
This documentation is plentiful and guides the user through the general workflow, but the workflow itself is confusing to follow.

This tool allows the user to run the benchmark across remote nodes, which in turn execute the run inside multiple virtual machines\footnote{This is not yet the case in the $\beta1$ version of the software described in the documentation}.
The result is then compiled manually or sent automatically through e-mails from the remote nodes and then manually analyzed from the generated CSV files.
It also forces certain requirements before running the benchmarks, such as the need for a specific folder structure, and the deployment of virtual machine images to the remote nodes in advance.

It uses \textsc{SysStat}\footnote{\href{https://github.com/sysstat/sysstat}{https://github.com/sysstat/sysstat}}, which measures the whole virtual machine resource usage.
This means that the resource usage measured also includes the OS.
Accordingly, \cite{beyerReliableBenchmarkingRequirements2019} assess this measurement tool as neither accurate nor reliable.

Although the necessity of providing virtual machine makes sure the program is $\bm{R_1}$ reproducible, \citet{kordonBenchKitToolMassive2014} reported an overhead of 40 seconds per run in their experiment due to the boot-up time of virtual machines.
\cite{beyerReliableBenchmarkingRequirements2019} remarks that this 40 seconds overhead would have taken an additional 190 CPU days if used in their competition, definitely a prohibitive overhead.


\subsection{\textsc{JuBE}}

\textsc{JuBE} \citep{frings2010flexible} is a benchmarking environment developed by Jülich Supercomputing Centre (JSC) of Forschungszentrum Jülich, Germany.
It is actively developed, albeit with no public version control repository, with the last release (as of 24th April 2019) in 4th February 2019.
It is licensed under the GNU General Public License version 3.

Compared to other benchmarking tool, it approaches the benchmarking task in a different, completely generic way.
There is no explicit measurement and limiter tool, no explicit task instances, and no specific evaluation or analysis step.
Instead, all the benchmarking task revolves around steps, parameters, and pattern-based analyzers.

The parameters can be defined as static, a parameter space, or even dynamic parameter space with the help scripting languages such as Python.
The steps are just a shell command, receiving several variables from the parameters and \textsc{JuBE} itself.
Finally, the analyzer just match patterns to the files generated from the benchmarking steps, then produce a CSV file.

The documentation\footnote{\href{https://apps.fz-juelich.de/jsc/jube/jube2/docu/index.html}{https://apps.fz-juelich.de/jsc/jube/jube2/docu/index.html}} is comprehensive and checks all the marks in \firstposs~$M_3$ evaluation.
There is a quick start guide and various advanced usage guides available.
Every command also has all its options documented.

Since it has no preference of resource measurement and limiter tool, \first~can't evaluate $M_5$.
But it definitely has the potential to use any kind of tool to measure and limit the resource usage.
This is also the case for the execution environment.
Submitting the benchmarking task to a job queue is just another benchmarking step.

The flexibility of this tool is surely its strong point.
But this flexibility also burdens the user to do all the repetitive task of benchmarking on their own, although it is possible to create some reusable wrappers for these repetitive tasks.
The analysis step is also restricting, since the user has to output the metrics to a file, then capture it with patterns.
Additionally, the tool itself does not provide much in terms of reproducibility.
It is up to the user how to make their benchmarking reproducible since shell commands is often not reproducible by itself.


\subsection{\textsc{StarExec}}

