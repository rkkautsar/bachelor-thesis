# Prior works

## Outline

- Overview

  - There are seven existing benchmarking tool that will be evaluated. Two of them are online-based while the other is offline-based. An online-based benchmarking tool allows the user to run benchmarks in the host cluster system.

  

- Evaluation metrics:

  - Extensibility
    - Flexible evaluation step
    - Flexible analysis step
    - Flexible benchmark instance source
    - Does not enforce implementation type
    - Can support arbitrary task scheduler
  - Configurability
    - Multiple runs
    - Multiple tool configurations
    - Parameter space
    - Benchmark instance selection
    - Set resource limit
  - Documentation
    - Self-documenting configuration
    - Installation guide
    - Configuration guide
    - Main workflow guide
    - Comprehensive documentation*
  - Setup effort
    - No superuser privilege
    - Installation guide
    - Documented requirements
    - No cumbersome dependencies*
  - Accuracy & Reliability
    - Measure and Limit Resources Accurately
    - Terminate Processes Reliably
    - Assign Cores Deliberately
    - Respect Nonuniform Memory Access
    - Avoid Swapping
    - Isolate Individual Runs
  - Reproducibility
    - Stored system information
    - Sharable results
    - Sharable configuration
    - Encourage sharable data*
    - Encourage sharable implementation*

  *) subjective evaluation

- Evaluation

  - benchmark-tool
  - benchexec
  - benchkit
  - compbench
  - jube
  - optil
  - starexec

- groups & matrix

1. benchmark-tool
2. benchexec
3. benchkit
4. compbench
5. jube
6. optil?
7. starexec?
8. ...
9. various other non-benchmarking tool (computational experiment helper)